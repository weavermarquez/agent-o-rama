= ACID Semantics in Rama

Rama integrates computation and storage, causing the ACID properties (Atomicity, Consistency, Isolation, Durability) to manifest differently than in traditional databases. Instead of requiring low-level configuration like isolation levels, Rama automatically provides the ideal semantics for each context.

== Internal and External PState Views

PStates are durable, replicated data stores that have two views:

* **Internal view:** In-progress, unreplicated changes.
* **External view:** Replicated, durable information.

Making changes "visible" is the process of durably replicating the internal view, making it the new external view.

A topology accessing its *own* PStates sees the internal view, allowing it to read its own writes. All other access, whether from other topologies or external clients, sees the external view, guaranteeing they only read durable, non-regressing data.

Topologies serve the same purpose as database transactions but are expressed via a general-purpose, Turing-complete API.

* **Stream topologies** act as transactions for a single partition.
* **Microbatch topologies** act as cross-partition transactions for all changes in the batch.

== Concurrency

Unlike databases that handle many concurrent write requests, Rama achieves parallelism by partitioning modules and their PStates across many single-threaded tasks. All actions on a given partition happen serially.

Efficiency comes from batching. Stream topologies batch writes for many events on a task. Microbatch topologies batch all PState changes for an entire microbatch. A new batch doesn't start until the previous one's changes are durable and visible.

== Atomicity

Atomicity ensures that a group of operations succeeds or fails together. It's impossible to see a partially-updated state.

=== Microbatch Topologies
Provide atomicity for the entire microbatch. All changes to all PStates across all partitions become visible together. This makes every microbatch a cross-partition transaction with exactly-once processing guarantees. If a microbatch fails, it is retried on the previous successful state, ensuring results are as if no failure occurred.

=== Stream Topologies
Provide atomicity at the event level. An event is the code executed between partitioner calls. All PState updates within a single event are atomic. For example, if multiple `.compoundAgg` calls occur before a `.hashPartition`, all those updates are applied atomically. This holds true even within complex logic like loops; updates between partitioner calls form a single atomic event.

== Consistency

Consistency ensures only valid data is written to a PState, preventing violations of defined constraints. Rama provides strong guarantees:

* **Schema validation** rejects any writes that violate a xref:pstates.adoc#_schema_validation[PState's schema].
* **Atomicity** ensures application-level constraints between PStates are maintained (cross-partition for microbatch topologies, co-located for stream topologies).
* **Replication** ensures that reads never regress to a prior state, even after failures.

== Isolation

Rama provides ideal isolation semantics automatically, eliminating the need for configurable isolation levels found in databases. This is possible because Rama's execution model, which processes data in batches on partitioned, single-threaded tasks, avoids the concurrency challenges of traditional databases.

The effective isolation level depends on the context of the read:

* **Reading own PStates (within a topology):** Akin to "read uncommitted". This is desirable as it allows a topology to read its own in-progress writes.
* **Reading other PStates (same module):** Akin to "serializable". This is due to the colocation of all computation and storage for a module, ensuring a consistent view within an event.
* **Reading other PStates (different module):** Akin to "read committed". This is because modules may run in different processes, so you only see data after it has been durably committed.

== Durability

Rama provides strong durability. Writes to PStates and depots are not made visible until they are durable on disk on both the leader and all in-sync replicas (ISRs). This guarantees that, in the event of a failure, reads will never regress to an earlier version of the data. See xref:replication.adoc[the page on replication] for more details.

== Summary

Rama offers strong, automatic ACID guarantees that rival or exceed traditional databases without requiring complex tuning. A "transaction" is an implicit concept: an event in a stream topology or an entire microbatch in a microbatch topology. These transactions are defined using a general-purpose, Turing-complete dataflow API, a significant departure from limited database DSLs.= Aggregator

Aggregators provide a high-level, and often more performant, alternative to xref:paths.adoc[paths] for updating PStates. They can offer more concise code for simple transformations and huge performance gains for complex ones, especially for global aggregations.

This page covers:
* The two types of aggregators: accumulators and combiners.
* The built-in aggregators and how to define your own.
* How combiners enable massive performance optimizations in batch blocks.
* Special features like `captureNewValInto`.
* The `topMonotonic` and `limitAgg` special aggregators.
* The "group by" operator.

== Using Aggregators

Aggregators are used with the `agg` and `compoundAgg` methods. `agg` updates a PState's top-level value, while `compoundAgg` targets and updates sub-values within a PState, much like a path.

[source, java]
----
public class AggregateModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*depot", Depot.random());

    StreamTopology s = topologies.stream("s");
    s.pstate("$$count", Long.class).initialValue(0L); // Top-level value
    s.pstate("$$countByKey", PState.mapSchema(String.class, Long.class)); // Nested values

    s.source("*depot").out("*k")
     .agg("$$count", Agg.count())
     .compoundAgg("$$countByKey", CompoundAgg.map("*k", Agg.count()));
  }

  // A main method would show this code produces:
  // Count: 4
  // Count by key: [["bette davis" 1] ["james cagney" 2] ["spencer tracy" 1]]
}
----

All built-in aggregators are available as static methods on the `Agg` class.

A key advantage of aggregators is that they automatically initialize nested values that don't exist. The equivalent `compoundAgg` using a path is more verbose:

[source, java]
----
// Aggregator version
.compoundAgg("$$countByKey", CompoundAgg.map("*k", Agg.count()));

// Path version
.localTransform("$$countByKey", Path.key("*k").nullToVal(0).term(Ops.PLUS, 1L));
----

NOTE: For top-level values like `$$count`, you are still responsible for setting an `initialValue` on the PState.

== Defining Aggregators: Accumulators vs. Combiners

Aggregators are defined by an `initVal` (or `zeroVal`) and a function to update a value. There are two types:

*   **Accumulators**: Less flexible. The update function takes the current value and new data. All aggregation must happen sequentially.
*   **Combiners**: More flexible. The update function combines two values of the same type. This property allows Rama to parallelize the aggregation, leading to significant performance gains.

=== Accumulators

An accumulator implements a `RamaAccumulatorAgg` interface, which has an `initVal()` method and an `accumulate` method.

// A simple accumulator to count inputs.
[source, java]
----
public class AccumCount implements RamaAccumulatorAgg0<Integer> {
  @Override
  public Integer initVal() { return 0; }

  @Override
  public Integer accumulate(Integer currVal) {
    return currVal + 1;
  }
}
// Usage: .agg("$$p", Agg.accumulator(new AccumCount()))
----

=== Combiners

A combiner implements the `RamaCombinerAgg` interface, which has `zeroVal()` and `combine` methods. The `combine` function's associative property is what enables parallelization. Rama can compute partial aggregates and then combine those partial aggregates into a final result.

// A combiner to sum integers.
[source, java]
----
public class CombinerSum implements RamaCombinerAgg<Integer> {
  @Override
  public Integer zeroVal() { return 0; }

  @Override
  public Integer combine(Integer val1, Integer val2) {
    return val1 + val2;
  }
}
// Usage: .agg("$$p", Agg.combiner(new CombinerSum(), "*v"))
----

An optional `isFlushRequired()` method can be overridden to return `true` if the combined value can grow unboundedly (e.g., aggregating into a map). This tells Rama to flush partial results more frequently to conserve memory.

== Built-in Aggregators

Rama provides many built-in aggregators on the `Agg` class.

*   `count()`: A combiner that increments by one.
*   `sum()`: A combiner that adds inputs.
*   `min()` / `max()`: Combiners to find the minimum/maximum value.
*   `and()` / `or()`: Combiners for boolean logic.
*   `last()`: An accumulator that stores the last seen value.
*   `voided()`: Removes an element from a collection, like `Path.termVoid()`.

See the link:https://redplanetlabs.com/javadoc/com/rpl/rama/Agg.html[Agg Javadoc] for a complete list.

== High-Performance Two-Phase Aggregation with Combiners

In batch blocks (used in microbatch and query topologies), Rama can perform a powerful optimization called **two-phase aggregation** if and only if *all* aggregators in the agg phase are combiners.

Consider a global aggregation:
[source, java]
----
// Inefficient: sends all data to one task before aggregating.
mb.source("*depot").out("*mb")
  .explodeMicrobatch("*mb").out("*v")
  .globalPartition()
  .agg("$$p", Agg.sum("*v"));

// Efficient: uses a batch block to enable two-phase aggregation.
mb.source("*depot").out("*mb")
  .batchBlock(
    Block.explodeMicrobatch("*mb").out("*v")
         .globalPartition()
         .agg("$$p", Agg.sum("*v")));
----

The second example is massively more scalable. Instead of sending every record across the network via `globalPartition`, Rama performs the optimization. Conceptually, it executes like this:

1.  **Phase 1 (Local Aggregation):** Each task partially aggregates its local data into a "combiner buffer". For `Agg.sum()`, each task sums its local numbers into a single partial sum.
2.  **Phase 2 (Global Aggregation):** Only the partial results are sent over the `globalPartition`. The final task then combines these few partial results into the final total.

This drastically reduces network traffic and the load on the final aggregation task. If any aggregator is an accumulator, this optimization is disabled. For global aggregations, always use combiners inside a `batchBlock`.

== Advanced Batch Aggregation Features

=== Capturing Newly Updated Values

In a batch block, an aggregator's `captureNewValInto` method lets you capture the final aggregated value for each updated key and use it in the post-agg phase. This is very useful for "top N" computations.

[source, java]
----
// This sub-batch counts words and emits each updated word and its new total count.
private SubBatch wordCounts(String microbatchVar) {
  Block b = Block.explodeMicrobatch(microbatchVar).out("*word")
                 .hashPartition("*word")
                 .compoundAgg("$$wordCounts",
                              CompoundAgg.map(
                                "*word",
                                Agg.count().captureNewValInto("*count"))); // <1>
  return new SubBatch(b, "*word", "*count");
}
----
<1> For every `*word` processed in the microbatch, `captureNewValInto` makes the final `*count` available in the post-agg phase. Even if "apple" appears 10 times, only one `["apple", <new-total-count>]` tuple is emitted from this subbatch.

*Restriction*: `captureNewValInto` cannot be used if the `compoundAgg` call branches into multiple maps, as Rama cannot determine a single set of output variables.

=== Aggregating to a Value or Temporary PState

In batch blocks, `agg` can also output a result to a new variable instead of updating a persistent PState. This is fundamental to query topologies.

[source, java]
----
// Query that sums a list of numbers. The result is emitted into *res.
topologies.query("q", "*nums").out("*res")
          .each(Ops.EXPLODE, "*nums").out("*num")
          .originPartition()
          .agg(Agg.sum("*num")).out("*res");
----

You can also aggregate into a temporary, in-memory PState that exists only for the duration of a microbatch. This allows you to share an intermediate result between sequential batch blocks.

[source, java]
----
.batchBlock(
  Block.explodeMicrobatch("*mb").out("*k")
       .hashPartition("*k")
       .compoundAgg(CompoundAgg.map("*k", Agg.count())).out("$$keyCounts")) // <1>
.batchBlock( // <2>
  Block.allPartition()
       .localSelect("$$keyCounts", Path.all()).out(...)
       ...)
----
<1> A temporary PState `$$keyCounts` is created and populated. It is not declared in `setup`.
<2> A subsequent `batchBlock` can read from `$$keyCounts`.

== Special Aggregators

=== `topMonotonic`

`Agg.topMonotonic` efficiently computes a top-N list. It's a combiner, making it highly performant for global top-N computations in batch blocks.

[source, java]
----
.each(Ops.TUPLE, "*word", "*count").out("*tuple")
.globalPartition()
.agg("$$topWords",
     Agg.topMonotonic(3, "*tuple")      // <1>
        .idFunction(Ops.FIRST)         // <2>
        .sortValFunction(Ops.LAST));   // <3>
----
<1> Keep the top 3 `*tuple` elements.
<2> The ID of each element is its first field (`*word`). Used to identify updates to existing elements.
<3> The sort value is the last field (`*count`).

The "monotonic" name implies a condition for correctness in incremental topologies: the sort values must be strictly increasing (or decreasing if `.ascending()` is used). If a word's count could decrease, `topMonotonic` might not produce the correct global top-N list. In query topologies, where all data is processed at once, this is not an issue.

`topMonotonic` uses an efficient amortized `O(log N)` algorithm. In non-batched contexts, the stored list might be in an intermediate state (e.g., up to 2*N elements), requiring the client to perform a final sort-and-trim.

=== `limitAgg`

`limitAgg` is a `Block` operator (not in `Agg`) that filters a batch down to a fixed number of elements. It can only be used in batch contexts.

[source, java]
----
.limitAgg(LimitAgg.create(3, "*v1", "*v3") // Keep at most 3 elements
                  .sort("*v2")             // Sort by *v2 before limiting
                  .reverse()               // Sort descending
                  .indexVar("*index"));    // Bind the 0-based index to *index
----
This example keeps the three elements with the highest `*v2` values and makes their `*v1` and `*v3` fields available in the post-agg phase, along with their rank `*index`. `limitAgg` uses a combiner, so it is efficient for global limits.

== "Group By" Operator

The `groupBy` operator partitions data by a set of keys and applies aggregations within each group, similar to SQL's `GROUP BY`.

[source, java]
----
.each(Ops.EXPLODE, "*tuplesVar").out("*tuple")
.each(Ops.EXPAND, "*tuple").out("*k", "*val")
.groupBy("*k",                               // <1>
  Block.agg(Agg.count()).out("*count")        // <2>
       .agg(Agg.sum("*val")).out("*sum"));    // <3>
----
<1> Groups data by `*k`. Automatically inserts a hash partition on `*k`.
<2> For each `*k`, `Agg.count()` is applied to produce `*count`.
<3> For each `*k`, `Agg.sum("*val")` is applied to produce `*sum`.

The post-agg phase emits tuples of `[*k, *count, *sum]` for each unique key. You can group by up to six variables.

== Summary

Aggregators are a powerful abstraction in Rama. While they offer conciseness for simple PState updates, their true strength lies in batched contexts. Combiners enable massive performance gains through two-phase aggregation, and specialized tools like `topMonotonic`, `limitAgg`, and `groupBy` allow for expressive and efficient data processing.= All Configs

This page lists static configurations for the xref:terminology.adoc#_conductor[Conductor], xref:terminology.adoc#_supervisor[Supervisors], xref:terminology.adoc#_worker[Workers], and xref:terminology.adoc#_foreign_context[foreign] clients.

Static configs are set on process launch and require a restart to change (e.g., process restart for Conductor/Supervisor, module update for Workers). They can be set in three ways:

* In the `rama.yaml` file.
* Via the `--configOverrides` flag for xref:operating-rama.adoc#_launching_modules[module launch] or xref:operating-rama.adoc#_updating_modules[module update].
* Programmatically when creating a link:https://redplanetlabs.com/javadoc/com/rpl/rama/RamaClusterManager.html[RamaClusterManager].

These are distinct from xref:operating-rama.adoc#_worker_configurations_and_dynamic_options[dynamic options], which are configurable at runtime via the xref:operating-rama.adoc#_cluster_ui[Cluster UI].

== Shared Conductor/Supervisor Configs

Set in the `rama.yaml` file for both components.

* `conductor.port`: Port for the Conductor. Defaults to `1973`.
* `local.dir`: Path for local state storage. Defaults to `local-rama-data`.
* `zookeeper.servers`: (Required) List of Zookeeper hostnames.
* `zookeeper.port`: Zookeeper port. Defaults to `2000`.
* `zookeeper.root`: Root Zookeeper node for Rama metadata. Defaults to `rama`.
* `zookeeper.connection.timeout.millis`: Zookeeper connection timeout. Defaults to `5000`.
* `zookeeper.session.timeout.millis`: Zookeeper session timeout. See the link:https://zookeeper.apache.org/doc/r3.3.3/zookeeperProgrammers.html[Zookeeper docs]. Defaults to `5000`.
* `zookeeper.retry.interval.millis`: Base wait time for Zookeeper request retries. Defaults to `1000`.
* `zookeeper.retry.times`: Number of Zookeeper request retries. Defaults to `2`.
* `zookeeper.retry.interval.millis.ceiling`: Max wait time for Zookeeper request retries. Defaults to `5000`.

== Conductor Configs

Set in the Conductor's `rama.yaml` file.

* `backup.provider`: External store configuration for xref:backups.adoc[module backups].
* `conductor.child.opts`: JVM options for the Conductor process. Defaults to `"-Xmx1024m"`.
* `conductor.assignment.mode`: Algorithm for assigning modules to nodes. See xref:operating-rama.adoc#_isolation_scheduler[isolation scheduler].
* `cluster.ui.port`: Port for the Cluster UI. Defaults to `8888`.

== Supervisor Configs

Set in the Supervisor's `rama.yaml` file.

* `conductor.host`: (Required) Hostname of the Conductor.
* `supervisor.host`: Hostname for this Supervisor, used for network communication. Defaults to `InetAddress.getLocalHost()`.
* `supervisor.child.opts`: JVM options for the Supervisor process. Defaults to `"-Xmx256m"`.
* `supervisor.port.range`: Port range for launched workers (e.g., `3000, 4000`). A large range is recommended.
* `supervisor.heartbeat.frequency.millis`: Frequency for heartbeating node state to Zookeeper. Defaults to `5000`.
* `supervisor.worker.heartbeat.timeout.millis`: If a worker heartbeat is not received within this time, the Supervisor will kill and restart it. Defaults to `10000`.
* `supervisor.worker.launch.timeout.millis`: If a launched worker's first heartbeat is not received within this time, the Supervisor will kill and restart it. Defaults to `90000`.
* `supervisor.labels`: Node labels for xref:heterogenous-clusters.adoc[heterogenous cluster assignments].

== Worker Configs

Set via the `--configOverrides` flag during xref:operating-rama.adoc#_launching_modules[module launch] or xref:operating-rama.adoc#_updating_modules[module update].

* `worker.child.opts`: JVM options for worker processes, often used for memory/GC tuning. Defaults to `"-Xmx4096m"`.
* `worker.max.direct.memory.size`: Amount of direct memory for each worker process. Defaults to `"500m"`.
* `worker.heartbeat.frequency.millis`: Frequency for heartbeating to its Supervisor. Defaults to `1000`.
* `worker.weft.client.max.threads`: Download threads for the xref:terminology.adoc#_weft[WEFT client]. Defaults to `10`.
* `worker.worp.server.threads`: Threads to process incoming xref:terminology.adoc#_worp[WORP] messages. Defaults to `10`.
* `replication.replog.recency.cache.size`: Number of xref:replication.adoc[replog entries] to cache per xref:terminology.adoc#_task_group_task_thread[task thread]. Defaults to `1000`.
* `pstate.maximal.schema.validations`: Disables expensive schema validation for nested data. Recommended to set to `false` in production.
* `pstate.validate.subindexed.structure.locations`: Disables validation that subindexed structures are not moved. Recommended to set to `false` in production.
* `pstate.rocksdb.options.builder`: For PStates with a top-level map schema, which use link:https://rocksdb.org/[RocksDB]. Specifies a class implementing link:https://redplanetlabs.com/javadoc/com/rpl/rama/RocksDBOptionsBuilder.html[RocksDBOptionsBuilder] to customize RocksDB instances.

== Foreign Configs

Set via a `rama.yaml` file on the client classpath or programmatically in the link:https://redplanetlabs.com/javadoc/com/rpl/rama/RamaClusterManager.html[RamaClusterManager] constructor.

* `conductor.host`: (Required) Hostname of the Conductor.
* `foreign.pstate.operation.timeout.millis`: Timeout for foreign PState queries.
* `foreign.depot.operation.timeout.millis`: Timeout for foreign depot appends and partition queries.
* `foreign.proxy.thread.pool.size`: Thread pool size for `ProxyState` callbacks. Defaults to `2`.
* `foreign.proxy.failure.window.seconds`: Time window to count proxy failures before forced termination.
* `foreign.proxy.failure.window.threshold`: Number of failures within the window to trigger termination.

== Shared Worker/Foreign Configs

Set via `--configOverrides` for Workers or `rama.yaml`/programmatically for foreign clients.

* `custom.serializations`: Registers xref:serialization.adoc[custom serializations].
* `worp.client.threads`: Number of xref:terminology.adoc#_worp[WORP] threads for sending outgoing messages. Defaults to `10`.
* `worp.max.send.buffer.size`: Max size of pending outgoing xref:terminology.adoc#_worp[WORP] messages. Defaults to `120000`.= Backups

Rama can back up all module state to a pluggable external storage service, like the provided open-source implementation for link:https://github.com/redplanetlabs/rama-s3-backup-provider[Amazon S3]. Restoring a module to a previous state is a simple one-line command.

This page covers:

* How backups work
* Configuration and usage
* Deletion policies
* Implementing custom backup providers
* Manual backups for the free license

== How backups work

When configured, Rama performs incremental backups of all module state, including:

* Depots
* PStates
* Topology progress
* Module JARs
* Config overrides
* Dynamic options

Backups are stored in an external service that implements a filesystem-like interface. Rama stores module data files along with manifests that map files to each task's state. Because backups are incremental, only new files (log segments, PState SSTs, WALs) that have accumulated since the last backup are uploaded.

Backups are designed to preserve Rama's atomicity guarantees, ensuring that all PState partitions are backed up at a transactionally consistent point in time. Backups also guarantee that depots contain all entries that could have affected the backed-up PState data.

== Configuring backups

. Place the backup provider JAR in the `lib/` directory on the Conductor and all Supervisors.
. Restart the Conductor and Supervisor processes. Workers will get the new JAR on the next module update.
. Add the `backup.provider` config to `rama.yaml` on the Conductor and restart it.

[source,yaml]
----
# Example for the S3 provider
backup.provider: "com.rpl.rama.backup.s3.S3BackupProvider my-s3-backup-bucket"
----

[IMPORTANT]
====
Each Rama cluster must use a unique backup provider configuration (e.g., a different S3 bucket or prefix). Sharing a configuration can lead to corrupted backups.
====

You can trigger backups manually or schedule them to run automatically.

*   **Manual Backup:**
+
[source,bash]
----
rama backup --action backup --module com.mycompany.MyModule
----

*   **Scheduled Backups:** Set the `backup.period.seconds` xref:operating-rama.adoc#_worker_configurations_and_dynamic_options[dynamic option] for the module.

== Viewing available backups

View backups in the Cluster UI or via the CLI. Each backup has a unique "backup ID" used for restores.

*   **List all backups for a module:**
+
[source,bash]
----
rama backup --action list --module com.mycompany.MyModule
----
+
The `list` command is paginated; use the `--pageToken` argument to fetch subsequent pages.

*   **Get detailed information about a specific backup:**
+
[source,bash]
----
rama backup --action info --backupId com.mycompany.MyModule-00000193E038B5F8-DE
----

== Restoring a backup

Use the `restore` command with a backup ID to restore a module.

*   **If the module is running:**
+
[source,bash]
----
rama backup --action restore --backupId com.mycompany.MyModule-00000193E038B5F8-DE
----

*   **If the module is not running, specify its configuration:**
+
[source,bash]
----
rama backup --action restore --backupId com.mycompany.MyModule-00000193E038B5F8-DE --threads 32 --workers 10 --replicationFactor 2
----
+
[NOTE]
====
A restore is non-reversible. To be safe, back up the current module state before restoring. A restore is only permitted if it does not break the dependencies of other running modules.
====

=== Effect on module dependencies

Rama automatically handles consistency when a module or its dependencies are restored. If a topology depends on a depot that is restored to an earlier state, Rama recalibrates the topology's processing offset to ensure no data is missed or reprocessed incorrectly. This recalibration typically occurs within seconds of the restore completing.

== Setting policies to delete old backups

To manage storage costs, you can configure policies to garbage collect (GC) old backups. The GC process deletes expired backup manifests and then removes any data files that are no longer referenced by any manifest.

Configure GC using these dynamic options:

*   `backup.gc.period.seconds`: How often to run the backup GC process.
*   `backup.max.age.hours`: The maximum age for a backup to be retained.
*   `backup.min.backups.to.keep`: The minimum number of backups to keep, regardless of age.

== Implementing a new backup provider

To support a different storage service, implement the link:https://redplanetlabs.com/javadoc/com/rpl/rama/backup/BackupProvider.html[BackupProvider] interface. This interface uses filesystem-like operations that return `CompletableFuture` objects for asynchronous execution.

These operations run on dedicated backup threads, but implementations should still be non-blocking to allow for high parallelism. The link:https://github.com/redplanetlabs/rama-s3-backup-provider[S3 backup provider] serves as a reference implementation.

== Backing up with the free license

The free license does not include the live, incremental backup feature. You can still perform a full manual backup, but this requires cluster downtime.

.   **To back up:**
..  Shut down the cluster using `rama shutdownCluster`.
..  Wait for the Conductor UI to show the state `[:cluster-shutdown-complete]`.
..  Stop the Conductor and Supervisor processes.
..  Create a link:https://zookeeper.apache.org/doc/r3.9.3/zookeeperSnapshotAndRestore.html[snapshot] of ZooKeeper.
..  Copy the contents of the Rama data directories (for the Conductor and all Supervisors) to your external storage.

.   **To restore:**
..  Restore the ZooKeeper snapshot.
..  Restore the backed-up data directories for the Conductor and Supervisors.
..  Start the Conductor and Supervisor processes. The cluster will restart all modules from the restored state.

[WARNING]
====
This manual process requires the restored cluster to be running the same Rama version as when the backup was taken.
===== Rama Dataflow Language: A Distilled Guide

Rama's dataflow API is a Turing-complete language for defining ETL and query topologies, enabling the expression of arbitrary distributed computation.

// To run the examples, use the rama-clojure-starter project:
// https://github.com/redplanetlabs/rama-clojure-starter
[source, clojure]
----
(use 'com.rpl.rama)
(use 'com.rpl.rama.path)
(require '[com.rpl.rama.ops :as ops])
(require '[com.rpl.rama.aggs :as aggs])
----

== Basic Structure

Rama is based on a "call and emit" paradigm, a generalization of the "call and response" model used by most languages. An operation can be called with arguments and emit zero, one, or many values to multiple independent "output streams".

[source,clojure]
----
(deframaop foo [*arg]
  (:> (inc *arg))  // Emit to the default stream
  (:> (dec *arg)))

(?<-
  (foo 5 :> *v)    // Capture emits from the default stream into *v
  (println "Emitted:" *v))
// Emitted: 6
// Emitted: 4
----

* `deframaop` defines a custom dataflow operation.
* `?<-` compiles and executes a dataflow block for testing and exploration.

Dataflow code consists of a sequence of *segments*. A segment includes an operation, input fields, and output declarations.

[source,clojure]
----
(operation input1 input2 :output-stream> <anchor> *var1 *var2)
----

Key components of a segment are:
* *Output Stream*:: A keyword ending in `>`. The default is `:>`.
* *Anchor*:: A symbol wrapped in `< >`, like `<my-anchor>`, used for branching and merging.
* *Variable*:: A symbol prefixed with `*` (value), `%` (anonymous operation), or `$$` (PState). Regular Clojure symbols are treated as constants.

Clojure macros and nested expressions are expanded and evaluated before being passed as arguments to Rama operations.

[source,clojure]
----
(?<-
  (println "Res:" (-> 10 (+ 3) (* 4))))
// Res: 52
----

== Branching and Unification

Dataflow code can form a graph. `anchor>` labels a point, and `hook>` attaches subsequent code to a labeled anchor, creating branches.

[source,clojure]
----
(deframaop multi-stream-op [*arg]
  (:> (inc *arg))
  (:other> (dec *arg)))

(?<-
  (multi-stream-op 5 :> <default> *v :other> <other> *v)
  (println "Default stream:" *v)
  (hook> <other>)
  (println "Other stream:" *v))
----

Branches can be merged using `unify>`. A variable is only in scope after `unify>` if it is bound on *all* incoming branches.

[source,clojure]
----
(deframaop multi-out [] (:a> 1) (:b> 2))

(?<-
  (multi-out :a> <a> *v :b> <b> *v)
  (unify> <a> <b>)
  (println "Val:" *v))
// Val: 1
// Val: 2
----
This creates an "abstract syntax graph" (ASG), which Rama reifies during compilation.

== Conditionals

Rama provides several ways to express conditional logic.

* `<<if`:: Similar to Clojure's `if`. The `then` and optional `else` branches are automatically unified.
+
[source,clojure]
----
(?<-
  (<<if (= 1 2)
    (println "True")
   (else>)
    (println "False")))
// False
----

* `if>`:: A primitive operation with `:then>` and `:else>` output streams. It is not a special form and can be passed as an argument.

* `ifexpr`:: A nested expression that returns a value, like Clojure's `if`.

* `<<cond`:: An analogue to Clojure's `cond`. All branches are unified. If a case emits multiple times, the continuation executes multiple times.
+
[source,clojure]
----
(?<-
  (<<cond
    (case> (> 2 1))
    (identity 2 :> *v)
    (default>)
    (identity -1 :> *v))
  (println "Val:" *v))
// Val: 2
----
+
Use `(default> :unify false)` to prevent unification on the default branch, which is useful when throwing exceptions.

== Loops

`loop<-` provides dataflow-aware looping. It emits values via `:>` and continues with `continue>`.

[source,clojure]
----
(?<-
  (loop<- [*i 0 :> *v]
    (:> *i) ; Emit the current value
    (<<if (< *i 2)
      (continue> (inc *i))))
  (println "Emit:" *v))
// Emit: 0
// Emit: 1
// Emit: 2
----
The loop's continuation runs immediately after an emit. `continue>` can be called multiple times in an iteration if the loop body is asynchronous (e.g., after a partitioner).

== Custom Operations

Decompose logic using several types of operations.

* *Clojure Functions*:: Standard functions whose return value is treated as a single emit to `:>`.
* *`defoperation`*:: Defines a `ramaop` in Clojure. Output streams are bound to symbols that are called to emit values.
* *`deframaop`*:: Defines a `ramaop` using dataflow code. Emits are made by calling output stream keywords as operations.
* *`deframafn`*:: Like `deframaop`, but its body must emit to `:>` exactly once as its final action. It can be called directly from Clojure.
* *`<<ramaop` / `<<ramafn`*:: Define anonymous operations, which are bound to `%` variables and capture their lexical closure.
+
[source,clojure]
----
(?<-
  (identity 10 :> *v)
  (<<ramafn %f [*a]
    (:> (+ *a *v)))
  (println "Res:" (%f 1) (%f 2)))
// Res: 11 12
----
*NOTE:* Do not store anonymous operations in depots or PStates, as their underlying class names are not stable across module updates.

== Key Differences from Clojure

* No direct Java interop (`.`). Use Clojure wrapper functions.
* No `fn`. Use `<<ramaop` or `<<ramafn`.
* No support for primitives.
* No special forms (`if`, `let`, etc.). Rama provides dataflow equivalents like `<<if` and `<<cond`.
* No var indirection for constants. Symbols used as arguments are embedded as constants at compile time. This affects `with-redefs`.
* Constants must be serializable for modules (e.g., basic types, records, data structures, `RamaSerializable` implementations).

== Distributed Execution

=== Partitioners

Partitioners are operations that route computation, potentially to a different task on a different machine. They have a `|` prefix.

* `|hash`:: Partitions to a single task based on the hash of a key.
* `|all`:: Broadcasts to all tasks.

Rama automatically analyzes variable scope to serialize only the necessary data across partitioner boundaries. The `$$` variants (`|hash$$`) partition based on the partitions of a depot or PState.

=== Asynchronous Operations

To avoid blocking task threads, use these operations for asynchronous work.

* `yield-if-overtime`:: Pauses execution and reschedules it as a new event if the current event exceeds a time limit (default 5ms).
* `completable-future>`:: Integrates a Java `CompletableFuture`. The dataflow continues when the future completes, and failures are handled by Rama's retry mechanism.

== Interacting with PStates

PStates are accessed using operations and Specter-style paths from `com.rpl.rama.path`.

* `local-select>`:: Reads from the PState partition on the current task. Emits once per navigated value.
* `select>`:: Partitions based on the path's key before reading.
* `local-transform>`:: Writes to the local PState partition. The path must end with `term`, `termval`, or `NONE>`.
* `local-clear>`:: Resets a top-level PState to its initial value.

[source,clojure]
----
; Read all values from a list at *user-id
(local-select> [(keypath *user-id) ALL] $$p :> *v)

; Set the value at *user-id to *profile
(local-transform> [(keypath *user-id) (termval *profile)] $$profiles)

; Add 10 to the value at :some-field
(local-transform> [(keypath *user-id) :some-field (term (fn [v] (+ v 10)))] $$p2)
----

== Aggregators

Aggregators provide a declarative way to update PStates and perform computations in batch blocks. They are prefixed with `+`.

* `+compound`:: Aggregates nested values, automatically handling initialization.
+
[source,clojure]
----
; For each *k, add *v to the value at :a and increment the count
(+compound $$p {*k {:a [(aggs/+sum *v) (aggs/+count)]}})
----
+
In batch blocks, it can capture updated values with the `:new-val>` stream.

* `+group-by`:: Groups data by a key and applies aggregators, similar to SQL's `GROUP BY`.

Custom aggregators can be defined with:
* `accumulator`:: Defines an aggregation like a `reduce` function.
* `combiner`:: Defines a parallelizable aggregation by merging two values. Rama automatically uses two-phase aggregation for combiners, enabling massive scalability.

== Batch Blocks

Batch blocks (`<<batch`) provide a partially declarative execution mode with SQL-like capabilities, including joins and scalable aggregation.

[source,clojure]
----
(?<-
  (<<batch
    ; Source 1
    (ops/explode [[:a 1] [:b 2]] :> [*k *v1])

    ; Source 2 (joined on *k)
    (gen>)
    (ops/explode [[:a 10] [:c 4]] :> [*k **v2]) ; **v2 for outer join

    ; Post-aggregation
    (println "Res:" *k *v1 **v2)))
// Res: :a 1 10
// Res: :b 2 nil
----
* `gen>` creates a new processing branch. Joins are inferred automatically based on variable usage.
* `**` variables indicate an outer join.
* Subbatches can be defined with `defgenerator` and `batch<-` for multi-stage aggregation.
* `materialize>` saves the results of a batch block to a temporary PState for reuse.

== Segmacros

Segmacros are Rama's macro system for generating dataflow segments. They operate after Clojure macros and return segments as vector data.

* `defbasicblocksegmacro`:: Defines a segmacro that expands to a block of segments.
* `defblock`:: A convenient way to define a segmacro that takes a block of dataflow code as an argument. These are typically prefixed with `<<`.

[source,clojure]
----
(defblock <<time [label block]
  [[`System/currentTimeMillis :> '*t1]
   [block> block]
   [`System/currentTimeMillis :> '*t2]
   [println label "elapsed:" (seg# - '*t2 '*t1) "ms"]])

(?<-
  (<<time "myblock"
    (identity 1)))
----

Notable built-in segmacros include `<<atomic`, `<<branch`, and `<<switch`.= Defining and Using Modules

This document is a reference for Rama's Clojure API, which mirrors the functionality of the Java API. For conceptual documentation, see the main xref:tutorial1.adoc[tutorial] (which uses Java) and the xref:terminology.adoc[Terminology page].

Useful resources:
* **Introductory Tutorial:** link:https://blog.redplanetlabs.com/2023/10/11/introducing-ramas-clojure-api/[Clojure API Blog Post]
* **API Documentation:** link:https://redplanetlabs.com/clojuredoc/index.html[ClojureDoc]
* **Examples:** link:https://github.com/redplanetlabs/rama-demo-gallery[rama-demo-gallery] project

== Defining Modules

Modules are defined with `defmodule`, which creates a Clojure function that receives `setup` and `topologies` arguments.

[source, clojure]
----
(defmodule MyModule [setup topologies]
  (declare-depot setup *my-depot :random))
----

* `setup`: Declares depots, PStates, and dependencies on other modules.
* `topologies`: Declares stream, microbatch, and query topologies.

A module's name is derived from its namespace and symbol (e.g., `"com.mycompany/MyModule"`). You can override the symbol part of the name:

[source, clojure]
----
(defmodule MyModule {:module-name "OtherName"} [setup topologies]
  ; Module name is now "com.mycompany/OtherName"
  )
----

Use `(get-module-name)` in tests to retrieve a module's name. For anonymous modules in tests, use `module`.

== Declaring Depots

xref:depots.adoc[Depots] are distributed, durable logs of data that act as sources for topologies. They are declared with `declare-depot`.

[source, clojure]
----
(defdepotpartitioner partition-by-value [data _]
  (mod (:some-value data) _))

(defmodule Foo [setup topologies]
  (declare-depot setup *depot1 :random)
  (declare-depot setup *depot2 (hash-by first))
  (declare-depot setup *depot3 :disallow)
  (declare-depot setup *depot4 partition-by-value)
  (declare-depot setup *depot5 :random {:global? true}))
----

The depot partitioner determines how client appends are distributed across partitions.
* `:random`: Appends go to a random partition.
* `(hash-by <fn>)`: Partitions based on the hash of a value extracted by `<fn>`.
* `:disallow`: Disables client appends; data can only be appended from within topologies.
* *Custom Partitioner*: A function defined with `defdepotpartitioner`.

The only option is `:global? true`, which creates a single-partition depot.

=== Depot Migrations

Depot records can be migrated during a xref:operating-rama.adoc#_updating_modules[module update]. A migration function transforms a record or removes it by returning `DEPOT-TOMBSTONE`. Migrations are applied on-read while data is updated in the background. See xref:depots.adoc#_migrations[the full documentation].

[source, clojure]
----
(declare-depot
  setup *depot :random
  {:migration (depot-migration
                "my-migration-id"
                (fn [record]
                  (if (= record 10) DEPOT-TOMBSTONE (str record))))})
----

The migration ID tracks progress. If the ID changes on a module update, the migration restarts.

== Declaring Tick Depots

Tick depots emit events at a specified frequency, allowing for time-based processing. The frequency is in milliseconds.

[source, clojure]
----
(defmodule Foo [setup topologies]
  (declare-tick-depot setup *my-tick 60000)) ; Emits once per minute
----

See xref:depots.adoc#_tick_depots[this section] for behavior differences in stream vs. microbatch topologies.

== Declaring Mirrors

Mirrors are references to depots, PStates, or query topologies in other modules. See xref:module-dependencies.adoc[Module dependencies] for more details.

[source, clojure]
----
(defmodule Foo [setup topologies]
  (mirror-depot setup *other-depot "com.mycompany.OtherModule" "*depot")
  (mirror-pstate setup $$p "com.mycompany.OtherModule" "$$p")
  (mirror-query setup *mirror-query "com.mycompany.OtherModule2" "some-query"))
----

Mirrored query topologies are invoked with `invoke-query`, just like local ones.

== Declaring Task Globals

link:https://redplanetlabs.com/docs/~/integrating.html#_task_globals[Task globals] provide global state to all tasks in a module, useful for large constants (like ML models) or external service clients. Declare them with `declare-object`.

[source, clojure]
----
(defmodule Foo [setup topologies]
  (declare-object setup *my-global "global-value"))
----

If the object implements `TaskGlobalObject`, it can be specialized for each task.

== Declaring ETL Topologies

ETL (Extract-Transform-Load) topologies process data from depots. They can be streaming (see xref:stream.adoc[Stream topologies]) or microbatching (see xref:microbatch.adoc[Microbatch topologies]).

*   `stream-topology`: Processes data as it arrives.
*   `microbatch-topology`: Processes data in batches.

[source, clojure]
----
;; Stream Topology Example
(defmodule StreamExample [setup topologies]
  (declare-depot setup *depot :random)
  (let [s (stream-topology topologies "counts")]
    (declare-pstate s $$counts {String Long})
    (<<sources s
      (source> *depot :> *data)
      (|hash *data)
      (+compound $$counts {*data (aggs/+count)}))))

;; Microbatch Topology Example
(defmodule MicrobatchExample [setup topologies]
  (declare-depot setup *depot :random)
  (let [mb (microbatch-topology topologies "counts")]
    (declare-pstate mb $$counts {String Long})
    (<<sources mb
      (source> *depot :> %microbatch)
      (%microbatch :> *data) ; Explode the batch into individual records
      (|hash *data)
      (+compound $$counts {*data (aggs/+count)}))))
----

`<<sources` defines the dataflow logic for processing depot data. In microbatching, `source>` emits a batch object, which you can then iterate over.

== Declaring PStates

xref:pstates.adoc[PStates] are partitioned, durable, and replicated indexes declared with `declare-pstate`. They are defined by a schema, which can include arbitrarily large nested data structures via xref:pstates.adoc#_subindexing["subindexing"].

[source, clojure]
----
(defmodule PStateExamples [setup topologies]
  (let [s (stream-topology topologies "s")]
    (declare-pstate s $$p1 {String Long})
    ; Subindexed set
    (declare-pstate s $$p2 {Long (set-schema String {:subindex? true})})
    ; Top-level value PState
    (declare-pstate s $$p3 Long)
    ; Nested subindexing with size tracking disabled
    (declare-pstate s $$p4 {Long (map-schema Long
                               (set-schema Long {:subindex-options {:track-size? false}})
                               {:subindex? true})})
    ))
----

PState options include:
* `:global? true`: Creates a single-partition PState.
* `:initial-value <val>`: Sets the initial value for each partition.
* `:private? true`: Restricts PState access to within the module.
* `:key-partitioner <fn>`: Customizes routing for foreign PState queries.

=== PState Migrations

PStates can be migrated to new schemas during a xref:operating-rama.adoc#_updating_modules[module update]. Migrations are defined with the `migrated` function and are applied on-read. See the xref:pstates.adoc#_migrations[full documentation] for details.

[source, clojure]
----
;; Change a value's schema from Long to String
(declare-pstate $$p {Long (migrated String "v1-to-v2" str)})

;; Update a fixed-keys map
(declare-pstate
  $$p
  {Long (migrated (fixed-keys-schema {:b String, :c Long})
                  "update-keys"
                  (fn [m] (-> m (dissoc :a) (assoc :c 10) (update :b str)))
                  [(fixed-key-additions #{:c})
                   (fixed-key-removals #{:a})])})
----

== Depot Subscriptions and Dataflow

The `<<sources` macro defines ETL logic using the `source>` function to subscribe to depots.

Subscription options for `source>`:
* `:start-from`: Determines where to start processing on a depot for the first time.
** `:end` (default), `:beginning`
** `(offset-ago <amt> <unit>)` (e.g., `(offset-ago 10 :records)`)
** `(offset-after-timestamp-millis <ts>)`
* `:retry-mode`: (Stream topologies only) Defines fault-tolerance behavior.
** `:individual` (default), `:all-after`, `:none`. See xref:stream.adoc#_fault_tolerance_and_retry_modes[docs] for semantics.

[source, clojure]
----
(<<sources s
  (source> *depot {:start-from :beginning, :retry-mode :all-after} :> *data)
  ;; ... dataflow logic ...
  )
----

Rama's dataflow API is based on "call and emit", where operations can emit zero, one, or many values to downstream code, including asynchronously to other partitions. This is detailed on the xref:clj-dataflow-lang.adoc[next page].

== Declaring Query Topologies

A xref:query.adoc[query topology] is an on-demand, realtime, distributed computation over PStates.

[source, clojure]
----
(defmodule MyQueries [setup topologies]
 (<<query-topology topologies "multiply" [*arg :> *res]
   (* 10 (inc *arg) :> *res)
   (|origin))) ; Partition back to the query's origin
----

A query definition includes a name, parameters, and dataflow code. It must end with the `|origin` partitioner. Query topologies can be invoked from other topologies using `invoke-query`.

== Foreign Module Clients

"Foreign" clients interact with a Rama cluster from the outside. First, get a cluster manager.

[source, clojure]
----
;; For a real cluster
(def manager (open-cluster-manager {"conductor.host" "1.2.3.4"}))

;; In tests, the InProcessCluster object is also a manager
(def ipc (create-ipc))
----

Then, fetch clients for depots, PStates, or query topologies.

[source, clojure]
----
(def depot (foreign-depot manager "com.mycompany/MyModule" "*depot"))
(def pstate (foreign-pstate manager "com.mycompany/MyModule" "$$p"))
(def query (foreign-query manager "com.mycompany/MyModule" "my-query"))
----

=== Foreign Depot Appends

Append data to a depot with `foreign-append!`.

[source, clojure]
----
(foreign-append! depot "some data") ; Uses default :ack level
(foreign-append! depot "other data" :append-ack)
----

*Ack Levels*:
* `nil`: Fire-and-forget. No guarantees.
* `:append-ack`: Blocks until data is persisted and replicated.
* `:ack` (default): Blocks until colocated stream topologies have also processed the record.

An async version, `foreign-append-async!`, returns a `CompletableFuture`.

=== Foreign Depot Queries

Read ranges of data directly from a depot partition.
* `(foreign-object-info depot)`: Returns info like `:num-partitions`.
* `(foreign-depot-partition-info depot <partition-idx>)`: Returns `:start-offset` and `:end-offset`.
* `(foreign-depot-read depot <partition-idx> <start> <end>)`: Reads records.

[source, clojure]
----
(foreign-depot-read depot 3 100 105)
;; => ["record100" "record101" "record102" "record103" "record104"]
----

Async versions `foreign-depot-partition-info-async` and `foreign-depot-read-async` are available.

=== Foreign PState Queries

PState queries use link:https://github.com/redplanetlabs/specter[Specter]-style paths via the `com.rpl.rama.path` namespace.

* `foreign-select`: Returns a sequence of results.
* `foreign-select-one`: Returns a single result.

[source, clojure]
----
(foreign-select [:a :b ALL even?] pstate)
(foreign-select-one [:k (nthpath 3)] pstate {:pkey "explicit-key"})
----

Use the `:pkey` option to specify an explicit partitioning key, overriding the default of using the first element in the path. Async versions `foreign-select-async` and `foreign-select-one-async` are available.

==== Range Queries

Rama provides special path navigators for efficient range queries on sorted structures (including all durable PState structures).

* `sorted-map-range`, `sorted-map-range-from`, `sorted-map-range-to`
* `sorted-set-range`, `sorted-set-range-from`, `sorted-set-range-to`

[source, clojure]
----
; Get a submap from key :a to :b
(foreign-select-one (sorted-map-range :a :b) pstate)

; Get up to 10 elements starting from key :k
(foreign-select-one (sorted-map-range-from :k 10) pstate)
----

==== Reactive Queries

Reactive queries provide fine-grained updates to clients. Use `foreign-proxy` to get a `ProxyState` object that represents the result of a query and is updated automatically.

[source, clojure]
----
(def proxy-val
  (foreign-proxy [:a :b] pstate
    {:callback-fn (fn [newval diff oldval]
                    (println "Value changed:" oldval "->" newval))}))

;; Get the current cached value without a remote call
(deref proxy-val)
----

The `ProxyState` receives minimal diffs from the server, which can be inspected in the `:callback-fn`. See xref:pstates.adoc#_reactive_queries[this section] for details. An async version, `foreign-proxy-async`, is also available.

=== Foreign Query Topology Invokes

Invoke a query topology like a regular function using `foreign-invoke-query`.

[source, clojure]
----
(def result (foreign-invoke-query my-query "arg1" :arg2 3))
----

The non-blocking `foreign-invoke-query-async` returns a `CompletableFuture`.

== Summary

This page covered how to define modules and their components (depots, PStates, topologies) and how to interact with them from external clients. The xref:clj-dataflow-lang.adoc[next page] provides a detailed guide to Rama's dataflow API.= Custom serialization =

To use custom object types in Rama, you must provide a way for them to be serialized (converted to bytes) and deserialized. Rama requires this for storing data to disk and transferring it between processes.

Rama has built-in support for primitive types, `java.util` and Clojure data structures, and types defined with `defrecord`. For all other types, you must register a custom serializer.

== Methods for Custom Serialization

Rama uses link:https://github.com/taoensso/nippy[Nippy] for serialization. You have two main options for adding custom types:

*   **Implement `RamaCustomSerialization` (Recommended):** This is the preferred method. See the main xref:serialization-adoc[serialization page] and the link:https://redplanetlabs.com/javadoc/com/rpl/rama/RamaCustomSerialization.html[`RamaCustomSerialization`] javadoc for details.

*   **Extend Nippy's Protocols:** As an alternative, you can register serializers by extending Nippy's protocols directly.

[IMPORTANT]
====
When choosing a method, be aware of how type IDs are handled:

*   **Direct Nippy Extension** uses a 2-byte hash for type IDs. Due to the link:https://en.wikipedia.org/wiki/Birthday_problem[birthday problem], this creates a >1% chance of a hash collision after registering only 50 custom types.

*   **`RamaCustomSerialization`** avoids this risk by using a robust 8-byte hash for type IDs, making collisions effectively impossible.
===== Testing

Test Rama modules with link:https://redplanetlabs.com/javadoc/com/rpl/rama/test/InProcessCluster.html[InProcessCluster]. The link:https://redplanetlabs.com/clojuredoc/com.rpl.rama.test.html[com.rpl.rama.test] namespace provides a Clojure API for this and other testing utilities. For complete details and examples, see the xref:testing.adoc[main testing documentation], the link:https://blog.redplanetlabs.com/2023/10/11/introducing-ramas-clojure-api/[intro blog post], and the link:https://github.com/redplanetlabs/rama-demo-gallery[rama-demo-gallery].

To unit test a `deframafn` or `deframaop` that operates on a PState, use the `link:https://redplanetlabs.com/clojuredoc/com.rpl.rama.test.html#var-create-test-pstate[create-test-pstate]` function.

.Example
[source, clojure]
----
(use 'com.rpl.rama)
(use 'com.rpl.rama.path)
(require '[com.rpl.rama.test :as rtest])

(deframafn foo-op [$$p]
  (local-transform> [:a "b" (term inc)] $$p)
  (:>))

(with-open [tp (rtest/create-test-pstate
                 {clojure.lang.Keyword (map-schema String
                                                   Long
                                                   {:subindex? true})})]
  (rtest/test-pstate-transform [:a "b" (termval 10)] tp)
  (println "Initial:" (rtest/test-pstate-select-one [:a "b"] tp))
  (foo-op tp)
  (println "After one call:" (rtest/test-pstate-select-one [:a "b"] tp))
  (foo-op tp)
  (println "After two calls:" (rtest/test-pstate-select-one [:a "b"] tp)))
----

.Output
[source, text]
----
Initial: 10
After one call: 11
After two calls: 12
----= Depots =

Depots are distributed, partitioned logs of data. All new data enters Rama through depots, which serve as the source for topologies.

All examples can be found in the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== Declaring Depots ==

Depots are declared in a module using `setup.declareDepot()`. A depot's _partitioning scheme_ determines which partition receives an appended record.

[source, java]
----
public class BasicDepotExamplesModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*depot1", Depot.random());
    setup.declareDepot("*depot2", Depot.hashBy(Ops.FIRST));
    setup.declareDepot("*depot3", Depot.disallow());
  }
}
----

=== Built-in Partitioning Schemes ===

* `Depot.random()`: Appends to a random partition for even distribution. Ordering is not guaranteed.
* `Depot.hashBy(function)`: Appends to a partition based on the hash of a value extracted by the provided function. This ensures data with the same extracted value is processed by the same partition, maintaining local ordering.
* `Depot.disallow()`: Prohibits client appends. Used for depots that are only appended to from within a topology.

You can also provide a class that implements `Depot.Partitioning` for custom partitioning logic.

=== Why Partitioning Matters ===

1.  *Local Ordering*: To process related events in the order they occurred (e.g., a user's `Follow` event before their `Unfollow` event), they must be sent to the same partition. `Depot.hashBy` is ideal for this.
2.  *Performance*: If a depot is partitioned by the same key as a PState, a topology can write to the PState without an extra network hop. This is known as _co-location_.

.Good: Co-located depot and PState partitioning avoids a network hop.
[source, java]
----
// Depot partitioned by user ID
setup.declareDepot("*profileFieldsDepot", Depot.hashBy(Ops.FIRST));

// Topology can write directly to the PState
profiles.source("*profileFieldsDepot").out("*tuple")
        .each(Ops.EXPAND, "*tuple").out("*userId", "*field", "*value")
        .localTransform("$$profiles", Path.key("*userId", "*field").termVal("*value"));
----

.Bad: Mismatched partitioning requires an extra `hashPartition` step.
[source, java]
----
// Depot is randomly partitioned
setup.declareDepot("*profileFieldsDepot", Depot.random());

// Topology must re-partition the data, causing a network hop
profiles.source("*profileFieldsDepot").out("*tuple")
        .each(Ops.EXPAND, "*tuple").out("*userId", "*field", "*value")
        .hashPartition("*userId") // <-- Extra network hop
        .localTransform("$$profiles", Path.key("*userId", "*field").termVal("*value"));
----

=== Depot Organization Guidelines ===

*   **Same Depot**: Group related data that requires local ordering or affects the same conceptual entities (e.g., `Follow` and `Unfollow` events). Use `subSource` to process different data types from the same depot stream.
*   **Separate Depots**: Keep unrelated data in separate depots to avoid unnecessary filtering in topologies (e.g., "pageviews" vs. "profile updates").

=== Depot Options ===

A depot can be declared as `global()`, creating a single-partition depot. This is not scalable and should not be used for high-throughput data.

[source, java]
----
setup.declareDepot("*myGlobalDepot", Depot.random()).global();
----

== Tick Depots ==

Tick depots emit a constant value at a configured frequency, useful for time-based logic in topologies. They cannot be appended to.

[source, java]
----
public class TickDepotModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    // Emits a tick every 3000 milliseconds
    setup.declareTickDepot("*ticks", 3000);

    StreamTopology s = topologies.stream("s");
    s.source("*ticks").each(Ops.PRINTLN, "Tick");
  }
}
----

Ticks are emitted on task 0. Use `allPartition` to broadcast the tick to all tasks.

*   *Stream Topologies*: Process ticks at approximately the configured frequency (push-based).
*   *Microbatch Topologies*: Emit at most one tick per microbatch execution, only if enough time has passed since the last tick (pull-based). The frequency is limited by the microbatch duration.

== Depot Client API ==

Depot clients are used to append data and query depot partitions.

=== Appends and Ack Levels ===

The `append()` method sends data to the correct partition automatically. You can specify an `AckLevel` to control when the call returns success.

[source, java]
----
depot.append("some data", AckLevel.ACK);          // Default
depot.append("some data", AckLevel.APPEND_ACK);
depot.append("some data", AckLevel.NONE);
----

*   `AckLevel.NONE`: Returns immediately (fire-and-forget).
*   `AckLevel.APPEND_ACK`: Returns after the data is successfully written and replicated in the depot.
*   `AckLevel.ACK` (Default): Returns after the conditions for `APPEND_ACK` are met, *plus* all co-located stream topologies have processed the data.

An async version, `appendAsync()`, returns a `CompletableFuture`.

=== Querying Depot Partitions ===

You can read a range of records from a specific depot partition. A record is identified by its `partition index` and `offset` (starting from 0).

1.  `getObjectInfo()`: Get the number of partitions for the depot.
2.  `getPartitionInfo(partitionIndex)`: Get the start and end offsets for a partition.
3.  `read(partitionIndex, startOffset, endOffset)`: Fetch a list of records in the given range.

To avoid long-running remote calls, fetch data in smaller batches (e.g., under 50kb).

== Streaming Ack Returns ==

When using `AckLevel.ACK`, co-located stream topologies can return arbitrary information to the client. The `append()` call returns a `Map<String, Object>` mapping topology names to their return values. This is useful for returning generated IDs or success messages from the topology itself.

== Appending from Topologies ==

Topologies can append to depots using `depotPartitionAppend()`.

Unlike client appends, `depotPartitionAppend` writes to the partition of the current task. Therefore, you must use a partitioner (`hashPartition`, etc.) to direct the data to the correct destination partition.

[source, java]
----
public class DepotPartitionAppendModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*incomingDepot", Depot.hashBy(Ops.FIRST));
    setup.declareDepot("*outgoingDepot", Depot.disallow());

    StreamTopology s = topologies.stream("s");
    s.source("*incomingDepot").out("*tuple")
     .each(Ops.EXPAND, "*tuple").out("*k", "*k2", "*v")
     .hashPartition("*k2") // Direct data to the correct partition
     .each(Ops.TUPLE, "*k2", new Expr(Ops.INC, "*v")).out("*newTuple")
     .depotPartitionAppend("*outgoingDepot", "*newTuple");
  }
}
----

== Migrations ==

Depot migrations transform or remove existing records during a module update. A migration is a function that takes a record and returns a new value, or `Depot.TOMBSTONE` to delete it.

[source, java]
----
setup.declareDepot("*depot", Depot.random())
     .migration("myMigrationId", (Integer num) -> {
       if(num.equals(10)) return Depot.TOMBSTONE;
       else return "" + num; // Convert Integer to String
     });
----

Migrations take effect instantly. They work by creating a new, migrated log in the background and atomically swapping it upon completion. Disk usage will temporarily increase during this process. The migration function is only applied to data that existed before the update; clients must begin appending data in the new format immediately after the update.

== Depot Trimming ==

Depot trimming automatically deletes old records to manage disk space, configured via dynamic options.

*   `depot.max.entries.per.partitition`: Sets the number of recent entries to keep per partition.
*   `depot.excess.proportion`: A buffer of extra entries kept to prevent race conditions with new topologies starting from the beginning.
*   `depot.trim.coordinate.local.topologies`: (Default: `true`) If true, prevents trimming data that is still needed by co-located topologies.
*   `depot.trim.coordinate.remote.topologies`: (Default: `true`) If true, prevents trimming data still needed by topologies in other modules.

== Tuning Options ==

=== Dynamic Options ===
*   `replication.depot.append.timeout.millis`: Timeout for replicating a depot append.
*   `depot.ack.failure.on.any.streaming.failure`: (Default: `true`) If `false`, an `AckLevel.ACK` append will wait for a stream topology to succeed on a retry instead of failing immediately.

=== Foreign Depot Client Configs ===
*   `foreign.depot.flush.delay.millis`: (Default: `0`) Adds a delay before flushing appends to increase batching. Optimal values are often 0-50ms.
*   `foreign.depot.operation.timeout.millis`: Timeout for foreign depot operations.

== Summary ==

Depots are the entry point for all data into Rama. Effective use of depots hinges on three key design decisions:
1.  How many depots to create.
2.  Which data belongs in each depot.
3.  How each depot should be partitioned.

Mastering these concepts is central to building efficient and well-structured Rama applications.= Downloads, Maven, and Local Development

== Downloads

Download Rama from the link:https://redplanetlabs.com/download[official website].

Explore our open-source projects on link:https://github.com/redplanetlabs[GitHub]:

* link:https://github.com/redplanetlabs/rama-demo-gallery[rama-demo-gallery]:: Commented demos for various use cases. A great resource for learning Rama basics (Java/Maven and Clojure/Leiningen).
* link:https://github.com/redplanetlabs/rama-helpers[rama-helpers]:: Utilities for implementing modules. Also available as a xref:#_maven_dependencies[Maven dependency].
* link:https://github.com/redplanetlabs/rama-examples[rama-examples]:: Source code for all examples in this documentation.
* link:https://github.com/redplanetlabs/rama-kafka[rama-kafka]:: Integrate with Kafka as a data source or for publishing records. Also available as a xref:#_maven_dependencies[Maven dependency].
* link:https://github.com/redplanetlabs/twitter-scale-mastodon[twitter-scale-mastodon]:: A Twitter-scale Mastodon implementation in 10k lines of code.

== Maven Configuration

Add the Red Planet Labs and Clojars repositories to your `pom.xml` to access Rama libraries.

[source, xml]
----
<repositories>
  <repository>
    <id>nexus-releases</id>
    <url>https://nexus.redplanetlabs.com/repository/maven-public-releases</url>
  </repository>
  <repository>
    <id>clojars</id>
    <url>https://repo.clojars.org/</url>
  </repository>
</repositories>
----

[#_maven_dependencies]
=== Dependencies

Add the required dependencies to your `pom.xml`.

*rama*
[source, xml]
----
<dependency>
    <groupId>com.rpl</groupId>
    <artifactId>rama</artifactId>
    <version>1.1.0</version>
    <scope>provided</scope>
</dependency>
----

*rama-helpers*
[source, xml]
----
<dependency>
    <groupId>com.rpl</groupId>
    <artifactId>rama-helpers</artifactId>
    <version>0.10.0</version>
</dependency>
----

*rama-kafka*
[source, xml]
----
<dependency>
    <groupId>com.rpl</groupId>
    <artifactId>rama-kafka</artifactId>
    <version>0.14.0</version>
</dependency>
----

== Local Development

Develop modules with your preferred JVM build tools. The typical workflow is to test locally with xref:testing.adoc[InProcessCluster] and deploy to a cluster using the xref:_using_the_rama_cli[Rama CLI].

For local testing with `InProcessCluster`, add a `log4j2.properties` file to your classpath to enable logging. Here is a minimal configuration:

[source, properties]
----
appender.console.name = console
appender.console.type = Console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{HH:mm:ss.SSS} %-5p [%t] %.20c - %m%n%throwable
appender.console.immediateFlush=true

rootLogger.level=ERROR
rootLogger.appenderRef.console.ref = console

logger.rama.name=rpl.rama
logger.rama.level=WARN
----

The Rama CLI is included in the main Rama download. Unpack the release and configure it to point to your cluster. See xref:operating-rama.adoc[Operating Rama] for details.= Heterogenous Clusters

Rama uses a label system to efficiently manage clusters with diverse hardware. You can assign labels to supervisor nodes (e.g., `gpu`, `xlarge-cpu`) and then schedule modules to run only on nodes with a specific label.

== Labeling a Supervisor

Add labels to a supervisor's `rama.yaml` using the `supervisor.labels` key.

[source, yaml]
----
zookeeper.servers: ["1.2.3.4", "5.6.7.8"]
conductor.host: "9.10.11.12"
local.dir: "/data/rama"
supervisor.port.range: [2500, 3500]
supervisor.labels:
  - "gpu"
  - "xlarge-cpu"
----

== Targeting Labels

How a module targets a label depends on the Conductor's scheduling mode.

=== Dev Scheduler (Default)

In the default dev scheduling mode, use the optional `--targetLabel` flag during deployment.

[source, text]
----
rama deploy \
--action launch \
--jar target/my-application.jar \
--module com.mycompany.MyModule \
--tasks 64 \
--threads 16 \
--workers 8 \
--replicationFactor 3 \
--targetLabel xlarge-cpu
----

The deployment will fail if there are not enough nodes with the target label to satisfy the assignment.

The `--targetLabel` flag behaves like config overrides during the module lifecycle:
*   *update*: The `--targetLabel` must be re-specified. Omitting it unsets the label for the module.
*   *scale*: The `--targetLabel` is not required.

=== Isolation Scheduler

With the xref:operating-rama.adoc#_isolation_scheduler[isolation scheduler], you pre-configure module assignments, including optional target labels, in the Conductor's `rama.yaml`.

[source, yaml]
----
conductor.assignment.mode:
  type: isolation
  modules:
    com.mycompany.Module1:
      numSupervisors: 4
      targetLabel: xlarge-cpu
    com.mycompany.AnotherModule: 8
    com.mycompany.Module2:
      numSupervisors: 12
      targetLabel: gpu
    com.mycompany.Module3:
      numSupervisors: 10
      targetLabel: xlarge-cpu
----

*   `numSupervisors` is required for each module.
*   `targetLabel` is optional.
*   A number by itself is shorthand for the `numSupervisors` value.

The isolation scheduler makes a best-effort attempt to assign nodes, considering all configured modules (even those not yet deployed) to ensure they can all be scheduled successfully.= Introduction

Rama is a distributed-first programming platform that simplifies application development. Its model combines the capabilities of databases and stream processing systems, but with greatly enhanced features.

Rama's core benefits include:
* Durable, replicated storage
* Inherently fault-tolerant computation
* Horizontal scalability
* Integrated deployment, monitoring, and updating

By handling the complexities of distributed systems, Rama lets you focus on your application's logic. Applications are scalable by default. Rama's dataflow-oriented paradigm allows you to declaratively specify _what_ to do, rather than _how_ to do it.

== Getting Started with Code

The best way to start is with our open-source examples.

* link:https://github.com/redplanetlabs/rama-demo-gallery[rama-demo-gallery]::
A repository of short, self-contained, and commented examples. They cover use cases like user profiles, time-series analytics, financial transfers, and REST API integration. Each example includes unit tests.

* link:https://github.com/redplanetlabs/twitter-scale-mastodon[twitter-scale-mastodon]::
A complete, from-scratch backend implementation of Mastodon that scales to Twitter's level. It uses over 40% less code than Mastodon's implementation. For a deep dive, see our link:https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/[accompanying blog post].

== How this guide is structured

This guide is organized to help you learn Rama efficiently. We recommend starting with the tutorial and referring to other sections as needed.

xref:tutorial1.adoc[Tutorial]::
Introduces core concepts: clusters, modules, tasks, depots, PStates, and topologies. The tutorial culminates in building the complete backend for a scalable social network in about 180 lines of code.

xref:terminology.adoc[Terminology]::
A quick reference for the main terms used in Rama.

Deep Dive Pages::
Most other pages expand on concepts from the tutorial, covering advanced features and use cases.

xref:operating-rama.adoc[Operating Rama clusters]::
Explains how to set up clusters and manage the lifecycle of your Rama applications.

xref:integrating.adoc[Integrating with other tools]::
Details how to integrate Rama with external systems like queues and databases.= Integrating with other tools

Rama integrates with external systems like queues and databases via its Java API. This guide covers key concepts for building these integrations, such as managing client connections, performing asynchronous operations, and sourcing data from external systems. The open-source link:https://github.com/redplanetlabs/rama-kafka[rama-kafka] library is a practical example of these principles.

== Task globals

Task globals manage resources, like client connections, for each task. They solve the problem of needing per-task, per-thread, or per-process resources and handling their lifecycle (creation/closing) during testing and deployment.

A task global is declared with `setup.declareObject()` and provides a copy of an object to every task.

[source, java]
----
public class BasicTaskGlobalModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    // Declares a global var *globalValue with the value 7
    setup.declareObject("*globalValue", 7);
    setup.declareDepot("*depot", Depot.random());

    StreamTopology s = topologies.stream("s");
    s.source("*depot")
     // *globalValue is accessible on any task
     .each(Ops.PRINTLN, "Task", new Expr(Ops.CURRENT_TASK_ID), "->", "*globalValue")
     .shufflePartition()
     .each(Ops.PRINTLN, "Task", new Expr(Ops.CURRENT_TASK_ID), "->", "*globalValue");
  }
}
----

To create specialized resources for each task (e.g., a unique database connection), a task global can implement the link:https://redplanetlabs.com/javadoc/com/rpl/rama/integration/TaskGlobalObject.html[TaskGlobalObject] interface.

*`prepareForTask(int taskId, TaskGlobalContext context)`*: Initializes the object on each task. It's called after the object is deserialized on the worker, allowing for task-specific setup.
*`close()`*: Cleans up resources (e.g., closes connections) when a worker shuts down. This is critical for preventing resource leaks in tests using xref:testing.adoc[InProcessCluster].

[source, java]
----
public class MyTaskGlobal implements TaskGlobalObject {
  int _v;
  public int special;

  public MyTaskGlobal(int v) { _v = v; }

  @Override
  public void prepareForTask(int taskId, TaskGlobalContext context) {
    // Specialize the object's state based on the task ID
    this.special = taskId * _v;
  }

  @Override
  public void close() { /* Cleanup logic here */ }
}

// In a module:
setup.declareObject("*tg", new MyTaskGlobal(10));
----

=== Ticks on task global objects

To execute code periodically (e.g., flush monitoring data), a task global can implement `TaskGlobalObjectWithTick`. This extends `TaskGlobalObject` with two methods:

*`getFrequencyMillis()`*: Returns the tick interval in milliseconds.
*`tick()`*: Contains the code to execute on each tick.

[source, java]
----
public class TickedTaskGlobalExample implements TaskGlobalObjectWithTick {
  @Override
  public long getFrequencyMillis() { return 30000; }

  @Override
  public void tick() {
    System.out.println("Ticking every 30 seconds");
  }
  // ... other required methods
}
----

=== Sharing resources between task global instances

To share a resource (e.g., a single thread-safe client) across multiple tasks within a thread or process, use a static map keyed by a unique resource identifier. The first task to access the resource creates it, stores it in the map, and becomes the "owner" responsible for its cleanup in the `close()` method.

The `TaskGlobalContext` passed to `prepareForTask` provides IDs (module instance, task thread) to create a unique key, ensuring isolation even in test environments.

[source, java]
----
public class TaskThreadSharedResourceTaskGlobal implements TaskGlobalObject {
  // Static map to hold shared resources
  static ConcurrentHashMap<List, Closeable> sharedResources = new ConcurrentHashMap<>();

  List _resourceId;
  Closeable _resource;
  boolean _owner = false; // Tracks if this instance created the resource

  // ... constructor

  @Override
  public void prepareForTask(int taskId, TaskGlobalContext context) {
    int taskThreadId = Collections.min(context.getTaskGroup());
    // Unique ID for the resource
    _resourceId = Arrays.asList(context.getModuleInstanceInfo().getModuleInstanceId(),
                                taskThreadId);

    // Atomically create the resource if it doesn't exist
    _resource = sharedResources.computeIfAbsent(_resourceId, k -> {
      _owner = true; // This instance is the owner
      return makeResource(); // Your resource creation logic
    });
  }

  @Override
  public void close() throws IOException {
    if(_owner) {
      _resource.close();
      sharedResources.remove(_resourceId);
    }
  }
}
----
NOTE: For per-process resources, additional synchronization (like a `lock`) is needed in `prepareForTask` to prevent race conditions between different task threads.

== Issuing remote calls within a topology with eachAsync

Never block a task thread. For non-blocking remote calls or other asynchronous computations, use link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#eachAsync-com.rpl.rama.ops.RamaFunction0-[eachAsync].

It takes a function that returns a link:https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html[CompletableFuture]. When the future completes, its result is emitted to the output variable. Rama automatically manages failures and retries based on the future's outcome.

[source, java]
----
s.source("*depot")
 .eachAsync(() -> {
   // Returns a CompletableFuture that will be completed elsewhere
   return someAsyncHttpClient.get("http://api.example.com/data");
 }).out("*v")
 .each(Ops.PRINTLN, "Result:", "*v");
----

If an external library only has a blocking API, use a background thread pool (managed by a task global) to execute the call and complete the future.

== External depots

The link:https://redplanetlabs.com/javadoc/com/rpl/rama/integration/ExternalDepot.html[ExternalDepot] interface allows external systems to be used as data sources for topologies, behaving like native Rama depots. They are declared as task globals and used in `source()` calls. For stream topologies, Rama provides backpressure, pausing fetches if consumers are behind.

Core concepts:

*   *Partition Index*: A zero-based integer identifying a partition in the external source.
*   *Offset*: A monotonically increasing `long` identifying a record within a partition.

All `ExternalDepot` methods must be non-blocking and return a `CompletableFuture`.

[source, java]
----
public interface ExternalDepot extends TaskGlobalObject {
  // Returns the total number of partitions.
  CompletableFuture<Integer> getNumPartitions();

  // Returns the first available offset for a partition.
  CompletableFuture<Long> startOffset(int partitionIndex);

  // Returns the final offset that will be appended.
  CompletableFuture<Long> endOffset(int partitionIndex);

  // (Optional) Returns the first offset after a given timestamp.
  CompletableFuture<Long> offsetAfterTimestampMillis(int partitionIndex, long millis);

  // Fetches an *exact* range of records [startOffset, endOffset). Used for microbatches.
  CompletableFuture<List> fetchFrom(int partitionIndex, long startOffset, long endOffset);

  // Fetches a "reasonable" number of records. Used for streaming with backpressure.
  CompletableFuture<List> fetchFrom(int partitionIndex, long startOffset);
}
----

For a complete implementation, see the link:https://github.com/redplanetlabs/rama-kafka[rama-kafka] library.

== Achieving exactly-once update semantics on external systems

xref:microbatch.adoc[Microbatch topologies] provide exactly-once semantics for PStates using an incrementing "microbatch ID". You can extend this guarantee to external systems by versioning your external data with this ID.

Use link:https://redplanetlabs.com/javadoc/com/rpl/rama/ops/Ops.html#CURRENT_MICROBATCH_ID[Ops.CURRENT_MICROBATCH_ID] within a microbatch topology to get the current ID.

The pattern for updating an external K/V store is:
1.  Store three pieces of information with each value: the current value, the previous value, and the last microbatch ID that updated it.
2.  In your topology, fetch the external record.
3.  Check if the record's `microbatchId` matches the current one.
** If it matches (a retry):** Base the new calculation on the *previous* value stored in the record.
** If it doesn't match (a new microbatch):** Use the *current* value.
4.  Write the new value, the old current value (as the new previous value), and the current microbatch ID back to the external system.

This ensures that retries do not result in duplicate updates.

[source, java]
----
// Pseudocode for an exactly-once update
.each(Ops.CURRENT_MICROBATCH_ID).out("*microbatchId")
.each((MyRecord record, Long microbatchId, Integer toAdd) -> {
  int baseVal;
  if(microbatchId.equals(record.microbatchId)) {
    // This is a retry, use the previous value to avoid double-counting.
    baseVal = record.prevVal;
  } else {
    // New microbatch, use the current value.
    baseVal = record.currVal;
  }
  // Create a new record with updated values and the current microbatch ID.
  return new MyRecord(baseVal, baseVal + toAdd, microbatchId);
}, "*record", "*microbatchId", "*toAdd").out("*newRecord")
// Asynchronously write the new record back to the external system.
.eachAsync((MyExternalSystem s, String key, MyRecord record) ->
  s.putAsync(key, record), "*mySystem", "*key", "*newRecord");
----= Intermediate dataflow programming

This page covers advanced dataflow programming in Rama, building on the xref:tutorial4.adoc[basic tutorial]. You will learn about:

*   Commonly used dataflow utilities.
*   Batch blocks for declarative, SQL-like data processing.
*   Macros for creating reusable chunks of dataflow code.

== Commonly used utilities

This section covers helpful, built-in operations from the `com.rpl.rama.ops.Ops` class and other common utilities.

=== Ops.EXPLODE, Ops.TUPLE, and Ops.EXPAND

`Ops.EXPLODE`:: Emits each element of a list as a separate branch of computation.
+
[source,java]
----
List data = Arrays.asList(1, 2, 3);
Block.each(Ops.EXPLODE, data).out("*v")
     .each(Ops.PRINTLN, "Elem:", "*v")
     .execute();
// Prints "Elem: 1", "Elem: 2", and "Elem: 3" on separate lines.
----

`Ops.TUPLE`:: Packages multiple inputs into a single list.
+
[source,java]
----
Block.each(Ops.TUPLE, 1, "a", true).out("*tuple")
     .each(Ops.PRINTLN, "Tuple:", "*tuple")
     .execute();
// Prints "Tuple: [1 a true]"
----

`Ops.EXPAND`:: The inverse of `Ops.TUPLE`. It binds each element of a list to a separate output variable. Use it only when the list size is known.
+
[source,java]
----
List tuple = Arrays.asList(1, 2, 3);
Block.each(Ops.EXPAND, tuple).out("*a", "*b", "*c")
     .each(Ops.PRINTLN, "Elements:", "*a", "*b", "*c")
     .execute();
// Prints "Elements: 1 2 3"
----

=== `subSource`

The `subSource` method provides a clean way to dispatch dataflow logic based on an object's type, avoiding deeply nested `if` blocks. It executes the first `SubSource` block whose type matches the dispatch object.

.Before: Nested `if` blocks
[source,java]
----
List data = Arrays.asList("a", 1, 3L);
Block.each(Ops.EXPLODE, data).out("*v")
     .ifTrue(new Expr(Ops.IS_INSTANCE_OF, Integer.class, "*v"),
       Block.each(Ops.PRINTLN, "Integer case", "*v"),
       Block.ifTrue(new Expr(Ops.IS_INSTANCE_OF, String.class, "*v"),
         Block.each(Ops.PRINTLN, "String case", "*v"),
         Block.ifTrue(new Expr(Ops.IS_INSTANCE_OF, Long.class, "*v"),
           Block.each(Ops.PRINTLN, "Long case", "*v"))))
     .execute();
----

.After: Using `subSource`
[source,java]
----
List data = Arrays.asList("a", 1, 3L);
Block.each(Ops.EXPLODE, data).out("*v")
     .subSource("*v",
       SubSource.create(Integer.class)
                .each(Ops.PRINTLN, "Integer case", "*v"),
       SubSource.create(String.class)
                .each(Ops.PRINTLN, "String case", "*v"),
       SubSource.create(Long.class)
                .each(Ops.PRINTLN, "Long case", "*v"))
     .execute();
----

=== `yieldIfOvertime`

Rama colocates computation and data, giving your code exclusive access to a task thread. However, long-running events can block the thread, increasing latency for other operations. `yieldIfOvertime` prevents this by suspending and later resuming an event if it exceeds a time limit (default 5ms).

An "event" is all the code that runs between "async boundaries" like partitioners or remote `localSelect` calls.

.Problem: A long loop blocks the task thread
[source,java]
----
// This loop runs 100 PState queries in a single, potentially long-running event.
.loopWithVars(LoopVars.var("*i", 0),
  Block.ifTrue(new Expr(Ops.LESS_THAN, "*i", 100),
    Block.localSelect("$$p2", Path.key("*k", "*i").sortedMapRangeFrom(0, 1000)).out("*m")
         .emitLoop("*m")
         .continueLoop(new Expr(Ops.INC, "*i"))))
----

.Solution: Add `yieldIfOvertime`
[source,java]
----
.loopWithVars(LoopVars.var("*i", 0),
  Block.ifTrue(new Expr(Ops.LESS_THAN, "*i", 100),
    Block.yieldIfOvertime() // <1>
         .localSelect("$$p2", Path.key("*k", "*i").sortedMapRangeFrom(0, 1000)).out("*m")
         .emitLoop("*m")
         .continueLoop(new Expr(Ops.INC, "*i"))))
----
<1> Checks if the event has run too long and yields if necessary.

Use Rama's xref:operating-rama.adoc#_activating_self_monitoring[self-monitoring] to detect if your events are blocking task threads.

=== `keepTrue`

`keepTrue` acts as a filter, continuing a branch of computation only if its argument is `true`.

[source,java]
----
List data = Arrays.asList(1, 2, 3, 4);
Block.each(Ops.EXPLODE, data).out("*v")
     .keepTrue(new Expr(Ops.IS_EVEN, "*v"))
     .each(Ops.PRINTLN, "Val:", "*v")
     .execute();
----

This prints:
[source,text]
----
Val: 2
Val: 4
----

=== `atomicBlock`

`atomicBlock` executes a body of code and waits for all its *synchronous* work to complete before continuing. The code after `atomicBlock` runs exactly once, after the body finishes.

This is useful for coordinating a dynamic amount of work. However, be aware that it only waits for synchronous completion. Asynchronous operations like partitioners will be initiated, but the `atomicBlock` will complete before those async events execute.

[source,java]
----
s.source("*depot").out("*list")
 .atomicBlock(
   Block.each(Ops.EXPLODE, "*list").out("*v")
        .each(Ops.PRINTLN, "A:", "*v")
        .shufflePartition() // <1>
        .each(Ops.PRINTLN, "B:", "*v"))
 .each(Ops.PRINTLN, "After atomicBlock"); // <2>
----
<1> This is an async boundary.
<2> This line executes after all "A" lines are printed but before any "B" lines.

The output demonstrates this behavior:
[source,text]
----
A: 1
A: 2
A: 3
After atomicBlock
B: 1
B: 2
B: 3
----

=== `branch`

`branch` is a helper for creating branching dataflow graphs, which can improve performance by running independent logic in parallel. It attaches a sub-block to a specified anchor.

.Serial Execution
[source,java]
----
.each(Ops.EXPAND, "*tuple").out("*k1", "*k2")
.hashPartition("*k1")
.compoundAgg("$$p", CompoundAgg.map("*k1", Agg.sum(1)))
.hashPartition("*k2") // <1>
.compoundAgg("$$p", CompoundAgg.map("*k2", Agg.sum(-1)));
----
<1> The second partition happens only after the first aggregation is sent.

.Parallel Execution with `branch`
[source,java]
----
.each(Ops.EXPAND, "*tuple").out("*k1", "*k2")
.anchor("root")
.hashPartition("*k1")
.compoundAgg("$$p", CompoundAgg.map("*k1", Agg.sum(1)))
.branch("root", // <1>
  Block.hashPartition("*k2")
       .compoundAgg("$$p", CompoundAgg.map("*k2", Agg.sum(-1))));
----
<1> Both branches start from the `root` anchor, allowing the partitions for `*k1` and `*k2` to happen in parallel, reducing latency.

== Batch blocks

Batch blocks are a partially declarative execution mode for dataflow, with semantics similar to relational languages like SQL. They are available in microbatch and query topologies and enable powerful features like joins and multi-phase aggregation.

A batch block runs in three phases, in order:
1.  **Pre-agg phase:** Reads and combines data sources.
2.  **Agg phase:** Performs aggregations on the data from the pre-agg phase.
3.  **Post-agg phase:** Processes the results of the aggregations.

.Example: Batch Block Phases
[source,java]
----
Block.batchBlock(
   Block
        // pre-agg phase
        .each(Ops.EXPLODE, data).out("*v")
        .each(Ops.PRINTLN, "Data:", "*v")

        // agg phase (defined by aggregator calls)
        .agg(Agg.count()).out("*count")
        .agg(Agg.sum("*v")).out("*sum")

        // post-agg phase
        .each(Ops.PRINTLN, "Count:", "*count", "Sum:", "*sum"));
----

=== Pre-agg phase

The pre-agg phase reads one or more "batch sources" and combines them into a single branch through merges or joins.

*   `freshBatchSource()` starts a new, independent source of data.
*   `unify()` merges multiple branches into one, just like in regular dataflow.
*   **Joins** are performed implicitly when subsequent code requires variables from different batch sources. The join occurs on all commonly named variables.

.Example: Implicit Inner Join
[source,java]
----
List source1 = Arrays.asList(Arrays.asList("a", 1), Arrays.asList("b", 2));
List source2 = Arrays.asList(Arrays.asList("a", 10), Arrays.asList("c", 30));

Block.batchBlock(
  Block.each(Ops.EXPLODE, source1).out("*tuple1").each(Ops.EXPAND, "*tuple1").out("*k", "*v1")
       .freshBatchSource()
       .each(Ops.EXPLODE, source2).out("*tuple2").each(Ops.EXPAND, "*tuple2").out("*k", "*v2")

       // This line requires *v1 and *v2, triggering a join on the common var *k.
       .each(Ops.PRINTLN, "Joined:", "*k", "*v1", "*v2"))
     .execute();
----
This prints `Joined: a 1 10`.

=== Agg and Post-agg phases

The **agg phase** computes aggregations over the data emitted by the pre-agg phase. For combiner aggregators, Rama automatically performs a two-phase aggregation for scalability (e.g., computing partial sums on each task before a final aggregation).

The **post-agg phase** runs after aggregation is complete.
*   It runs on the task(s) determined by the *final partitioner* in the pre-agg phase.
*   If the pre-agg phase has aggregators, it *must* end with a partitioner.
*   Only variables output by aggregators are in scope.
*   Partitioners are not allowed in the post-agg phase.

=== Subbatches

Subbatches are analogous to subqueries in SQL, allowing you to nest batch blocks and aggregate your aggregates. A subbatch is a self-contained batch block whose output can be consumed as a source by another batch block.

[source,java]
----
// Subbatch to count words
private static SubBatch wordCount(List source) {
  Block b = Block.each(Ops.EXPLODE, source).out("*k")
                 .compoundAgg(CompoundAgg.map("*k", Agg.count())).out("*m")
                 .each(Ops.EXPLODE_MAP, "*m").out("*k", "*count");
  return new SubBatch(b, "*k", "*count");
}

// Main batch block consuming the subbatch
Block.batchBlock(
  Block.subBatch(wordCount(source)).out("*k", "*c") // <1>
       .agg(Agg.max("*c")).out("*maxCount")
       .each(Ops.PRINTLN, "Max count:", "*maxCount"))
     .execute();
----
<1> The `subBatch` call executes the `wordCount` logic and emits its results into the parent batch block.

=== Invalid batch blocks

A batch block is invalid if:
1.  **A variable is shadowed** in the pre-agg phase.
2.  The pre-agg phase has **multiple, disconnected branches** at the end. All sources must combine into a single branch via joins or merges.
3.  Multiple batch sources exist that **cannot be joined** (i.e., they have no common variables).

=== Outer joins

Outer joins are specified by making a batch source "unground" (infinite). This is done using special variable names.

`**var` (unground var):: Marks a batch source as unground. This source is now conceptually infinite, containing a row for every possible key, with non-key values as `null`.
`*___var` (delayed unground var):: An intermediate variable that is treated as ground until an unground var is introduced, at which point it also becomes unground.

A join between a ground source and an unground source results in an outer join.

.Example: Left Outer Join
[source,java]
----
// source1 is ground, source2 is unground
Block.batchBlock(
  Block.each(Ops.EXPLODE, source1).out("*tuple1")
       .each(Ops.EXPAND, "*tuple1").out("*k", "*v1")

       .freshBatchSource()
       .each(Ops.EXPLODE, source2).out("*___tuple2") // delayed unground
       .each(Ops.EXPAND, "*___tuple2").out("*k", "**v2") // unground

       .each(Ops.PRINTLN, "Joined:", "*k", "*v1", "**v2"))
     .execute();
----
If a `*k` from `source1` is not in `source2`, `**v2` will be `null` in the output.

=== `materialize`

Inside a microbatch topology, you can use `materialize` to save the output of a batch block to a temporary, in-memory PState. This PState is cleared between microbatch attempts and can be consumed in subsequent batch blocks using `explodeMaterialized`.

[source,java]
----
.batchBlock(
  Block.explodeMicrobatch("*microbatch").out("*v")
       .each(Ops.INC, "*v").out("*v2")
       .materialize("*v", "*v2").out("$$nums")) // <1>
.batchBlock(
  Block.explodeMaterialized("$$nums").out("*v1", "*v2") // <2>
       .globalPartition()
       .agg("$$p1", Agg.sum("*v1")));
----
<1> Creates a temporary PState named `$$nums`.
<2> Consumes the materialized data in a subsequent step.

== Macros

Macros let you define reusable, dynamic chunks of dataflow code that compose elegantly with Rama's builder-style API. A macro is a static method that returns a `Block`.

.Problem: Reusable code can break the builder pattern
[source,java]
----
// Awkward usage: reassigning the block variable
Block.Impl b = s.source("*depot").out("*data");
b = extractJavaFieldsNonMacro(b, "*data", "*id", "*name");
b.localTransform(...);
----

.Solution: Use `macro()` for elegant composition
[source,java]
----
s.source("*depot").out("*data")
 .macro(extractJavaFieldsMacro("*data", "*id", "*name")) // <1>
 .localTransform(...);
----
<1> The `macro` method expands the `Block` returned by `extractJavaFieldsMacro` in place.

A critical rule for writing robust, composable macros is to **use `Helpers.genVar` for all intermediate variables**. This prevents accidental variable name collisions when the macro is expanded.

.Example: A macro with a generated intermediate variable
[source,java]
----
public static Block genIdMacro(String pstateName, String outVar) {
  String counterVar = Helpers.genVar("counter"); // <1>
  String taskIdVar = Helpers.genVar("taskId");

  return Block.create()
         .localSelect(pstateName, Path.stay()).out(counterVar)
         .localTransform(pstateName, Path.term(Ops.PLUS_LONG, 1))
         .each(Ops.CURRENT_TASK_ID).out(taskIdVar)
         .each(MyClass::generateId, counterVar, taskIdVar).out(outVar);
}
----
<1> `Helpers.genVar` creates a unique variable name (e.g., `*___counter_1234`), preventing conflicts with variables like `*counter` that might exist where the macro is used.

== Summary

You have now learned about Rama's advanced dataflow features, including powerful utilities, declarative batch blocks for complex queries, and macros for creating clean, reusable abstractions. By leveraging these tools within a general-purpose language like Java, you can build sophisticated, scalable, and maintainable applications.

To see these concepts applied in a large-scale, real-world application, we highly recommend studying our xref:downloads-maven-local-dev.adoc[Twitter-scale Mastodon implementation].= Microbatch topologies

Microbatch topologies share similarities with stream topologies but have important differences. They offer greater computational power, different performance characteristics, and provide simple, exactly-once fault-tolerance for PState updates.

This page covers:

*   Advanced computational capabilities of microbatch topologies.
*   The operational phases of a microbatch.
*   How exactly-once semantics are achieved despite failures.
*   Available tuning options.

All examples are in the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== Usage

Microbatch topologies process accumulated depot data in coordinated batches, unlike stream topologies which process records individually as they arrive. A single microbatch can process thousands of records from each depot partition simultaneously.

[source, java]
----
public class ExampleMicrobatchTopologyModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*keyPairsDepot", Depot.hashBy(Ops.FIRST));
    setup.declareDepot("*numbersDepot", Depot.random());

    MicrobatchTopology mb = topologies.microbatch("mb");
    mb.pstate(
       "$$keyPairCounts",
       PState.mapSchema(
         String.class,
         PState.mapSchema(String.class, Long.class).subindexed()));
    mb.pstate("$$globalSum", Long.class).global().initialValue(0L);

    mb.source("*keyPairsDepot").out("*microbatch")
      .explodeMicrobatch("*microbatch").out("*tuple")
      .each(Ops.EXPAND, "*tuple").out("*k", "*k2")
      .compoundAgg("$$keyPairCounts", CompoundAgg.map("*k", CompoundAgg.map("*k2", Agg.count())));

    mb.source("*numbersDepot").out("*microbatch")
      .batchBlock(
        Block.explodeMicrobatch("*microbatch").out("*v")
             .globalPartition()
             .agg("$$globalSum", Agg.sum("*v")));
  }
  // main method...
}
----

A microbatch `source` emits a single object representing the entire batch of data. This object must be passed to `explodeMicrobatch`, which then emits each individual record from the batch for processing.

The full dataflow API is available, including powerful batch computation capabilities via `batchBlock`. Batch blocks enable two-phase aggregation, joins, and temporary PStates. For details, see xref:intermediate-dataflow.adoc#_batch_blocks[batch blocks].

All dataflow constructs are composable. The only restriction is that `batchBlock` must be initiated from task 0, which is always the case at the start of a microbatch source block.

In tests, use `waitForMicrobatchProcessedCount` to wait for a microbatch to process a specific number of appended records, as processing is asynchronous to appends.

== Operation and fault-tolerance

Microbatch topologies achieve high throughput and exactly-once fault-tolerance through a phased execution model controlled by a "runner" on the task 0 leader. This runner operates as a series of asynchronous events.

A single execution cycle is a "microbatch attempt", identified by a `microbatch ID` and a `version`. If an attempt fails (due to an exception, leadership change, or timeout), it is retried with the same `microbatch ID` and an incremented `version`.

Crucially, each attempt for a given `microbatch ID`:

1.  Processes the *exact same set of depot data*.
2.  Starts with PStates in the *exact same state* as they were after the previous successful microbatch.

This guarantees exactly-once semantics for PState updates.

A microbatch executes in three sequential phases coordinated by the runner:

image::./diagrams/microbatch/microbatch-loop.png[]

.   *Priming Phase:* Prepares all tasks for the microbatch. This involves clearing internal buffers and resetting the _internal view_ of user PStates to their state after the last successful microbatch ID. Topology code always operates on this internal view.
.   *Processing Phase:* Executes the user-defined topology code. The range of depot data to be processed is fixed for the current microbatch ID.
.   *Commit Phase:* After processing completes, all PState updates are checkpointed and xref:replication.adoc[replicated]. Once a task's PStates are checkpointed and replicated, their _external view_ is updated, making the changes visible to clients and other topologies. All PState changes on a given task become visible atomically.

A microbatch topology's output is not strictly deterministic if the code uses non-deterministic operations like `shufflePartition` or random numbers. However, the resulting PStates still correctly represent the processing of each depot record exactly once.

[NOTE]
====
Depot appends performed within a microbatch topology do not currently have exactly-once semantics. This is on the roadmap.
====

== "Start from" options

You can configure a microbatch topology to start processing from a specific point in a depot's history. These options apply only the first time a topology encounters a depot.

[source, java]
----
mb.source("*depot", MicrobatchSourceOptions.startFromBeginning());
mb.source("*depot2", MicrobatchSourceOptions.startFromOffsetAfterTimestamp(107740800000));
mb.source("*depot3", MicrobatchSourceOptions.startFromOffsetAgo(10000, OffsetAgo.RECORDS));
mb.source("*depot4", MicrobatchSourceOptions.startFromOffsetAgo(15, OffsetAgo.DAYS));
----

For descriptions, see xref:stream.adoc#_start_from_options[this section]. Unlike stream topologies, no additional fault-tolerance options are needed, as microbatch topologies are always exactly-once.

== Tuning options

Microbatch topologies have several xref:operating-rama.adoc#_worker_configurations_and_dynamic_options[dynamic options] that can be changed live from the Cluster UI.

*   `depot.microbatch.max.records`: Max records to read per depot partition in a microbatch. Higher values increase batching but also memory usage.
*   `depot.max.fetch`: Max depot records to fetch from a partition in a single network request. Must be <= `depot.microbatch.max.records`.
*   `topology.microbatch.phase.timeout.seconds`: Timeout for each microbatch phase. A timeout triggers a retry.
*   `topology.microbatch.empty.sleep.time.millis`: Time the runner sleeps if a microbatch processes zero records, preventing CPU spinning on idle depots.
*   `topology.microbatch.pstate.flush.path.count`: Rate at which PState writes are flushed to disk. Higher values improve write batching but can increase task thread utilization.
*   `worker.combined.transfer.limit`: Batches outgoing events into a single network message. Higher values improve throughput but can cause network-level delays.
*   `topology.combiner.limit`: Flush frequency for combiners that require it (most do not).
*   `topology.microbatch.ack.branching.factor`: Branching factor for the hierarchical acking tree used to detect processing phase completion.
*   `topology.microbatch.ack.delay.base.millis`: Base wait time for new completion info in the acking algorithm.
*   `topology.microbatch.ack.delay.step.millis`: Additional wait time per level in the acking tree, increasing towards the root (task 0).

== Summary

Unless you require millisecond-level update latency for PStates, you should prefer microbatch topologies over stream topologies. They offer higher throughput, simpler fault-tolerance semantics, and significantly more powerful computational capabilities through batch blocks.= Dependencies Between Modules

Modules can use depots, PStates, and query topologies from other modules. This is useful for decomposing large applications, enabling independent team workflows, and allowing for fine-grained application updates.

An object (depot, PState, or query topology) used by another module is called a "mirror". Mirrors do not store data locally; operations on them are proxied to the original object.

== Declaring and Using Mirrors

Declare mirrors in your module definition using `clusterDepot`, `clusterPState`, and `clusterQuery`.

[source, java]
----
public class MirrorDeclarationsModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    // setup.cluster<Type>(varForThisModule, owningModuleName, objectNameInOwner);
    setup.clusterPState("$$mirror", "com.mycompany.FooModule", "$$p");
    setup.clusterDepot("*mirrorDepot", "com.mycompany.FooModule", "*depot");
    setup.clusterQuery("*mirrorQuery", "com.mycompany.BarModule", "someQueryTopologyName");
  }
}
----

The first argument is the var used within the current module, the second is the name of the owning module, and the third is the name of the object in the owning module.

Mirrors are used in topologies just like colocated objects.

[source, java]
----
public class UsingMirrorsExample {
  public static class Module1 implements RamaModule {
    @Override
    public void define(Setup setup, Topologies topologies) {
      setup.declareDepot("*depot1", Depot.hashBy(Ops.IDENTITY));
      setup.declarePState("$$p", PState.mapSchema(String.class, Long.class));
      topologies.query("qq", "*v1", "*v2").out("*res")
                .each(Ops.TIMES, new Expr(Ops.INC, "*v1"), "*v2").out("*res")
                .originPartition();

      StreamTopology s = topologies.stream("s");
      s.source("*depot1").out("*k")
       .compoundAgg("$$p", CompoundAgg.map("*k", Agg.count()));
    }
  }

  public static class Module2 implements RamaModule {
    @Override
    public void define(Setup setup, Topologies topologies) {
      // Declare mirrors of objects in Module1
      setup.clusterPState("$$mirror", Module1.class.getName(), "$$p");
      setup.clusterQuery("*mirrorQuery", Module1.class.getName(), "qq");

      StreamTopology s = topologies.stream("s");
      s.source(Module1.class.getName(), "*depot1").out("*k") // Source a mirror depot
       .select("$$mirror", Path.key("*k")).out("*count") // Query a mirror PState
       .invokeQuery("*mirrorQuery", 3, 7).out("*queryResult") // Invoke a mirror query
       .each(Ops.PRINTLN, "Results:", "*count", "*queryResult");
    }
  }
}
----

A `select` on a mirror PState works by partitioning to the correct task and then using a "mirror partition index" to query the correct partition on the remote module. Alternatively, you can explicitly partition first:

[source, java]
----
.hashPartition("$$mirror", "*k")
.localSelect("$$mirror", Path.key("*k")).out("*count")
----

All object partitioners set the mirror partition index. This allows you to partition once and then use `localSelect` on multiple mirror PStates from the same module if they share the same partitioning scheme. Similarly, `depotPartitionAppend` on a mirror depot requires an object partitioner.

== Tradeoffs of Using Mirrors

While seamless, using mirrors has tradeoffs compared to using colocated objects.

*   **Loss of Colocation:** Operations on mirrors become network calls instead of local operations. While Rama batches these calls, the network overhead reduces performance.
*   **No Stream Coordination:** Appends to a depot do not coordinate with stream topologies that mirror that depot. An append can be acknowledged before a remote stream has finished processing it.
*   **Async Boundaries:** A `localSelect` on a mirror is an "async boundary," meaning it executes in a separate event. This can affect data synchronization when querying multiple PStates. For example:
+
[source, java]
----
.localSelect("$$p1", Path.key("*k")).out("*v1")
.localSelect("$$mirror", Path.key("*k")).out("*v2") // This is an async boundary
.localSelect("$$p2", Path.key("*k")).out("*v1")
----
+
Here, the reads from `$$p1` and `$$p2` occur in separate events and are not synchronized to the same moment in time. If `$$mirror` were colocated, all three `localSelect` calls would happen in the same event.

== Dependency Enforcement

Rama enforces module dependencies during lifecycle operations:

*   A `launch` or `update` will fail if a declared mirror dependency does not exist.
*   A `destroy` will fail if other modules depend on it. You must first remove the dependent modules or update them to remove the dependency.
*   An `update` will fail if it removes an object that another module depends on.

== Circular Dependencies

Circular dependencies between modules are possible. They cannot be created at launch time but can be established via an update.

The process is as follows:
1.  Deploy Module A.
2.  Deploy Module B with a dependency on an object in Module A.
3.  Update Module A to add a dependency on an object in Module B.

Here is an example demonstrating this flow:
[source, java]
----
public class CircularDependenciesExample {
  // Version 1 of ModuleA has no dependencies
  public static class ModuleA_v1 implements RamaModule {
    @Override
    public void define(Setup setup, Topologies topologies) {
      setup.declareDepot("*depot", Depot.random());
    }
    @Override
    public String getModuleName() { return "ModuleA"; }
  }

  // ModuleB depends on ModuleA
  public static class ModuleB implements RamaModule {
    @Override
    public void define(Setup setup, Topologies topologies) {
      setup.clusterDepot("*depot", "ModuleA", "*depot");
      StreamTopology s = topologies.stream("s");
      s.pstate("$$p", PState.mapSchema(String.class, Long.class));
      s.source("*depot").out("*k").compoundAgg("$$p", CompoundAgg.map("*k", Agg.count()));
    }
  }

  // Version 2 of ModuleA adds a dependency on ModuleB
  public static class ModuleA_v2 implements RamaModule {
    @Override
    public void define(Setup setup, Topologies topologies) {
      setup.declareDepot("*depot", Depot.random()); // Original depot
      setup.clusterPState("$$mirror", ModuleB.class.getName(), "$$p"); // New dependency
      // ... topology uses $$mirror
    }
    @Override
    public String getModuleName() { return "ModuleA"; }
  }

  public static void main(String[] args) throws Exception {
    try(InProcessCluster cluster = InProcessCluster.create()) {
      // 1. Deploy ModuleA v1
      cluster.launchModule(new ModuleA_v1(), new LaunchConfig(2, 2));

      // 2. Deploy ModuleB (depends on ModuleA)
      cluster.launchModule(new ModuleB(), new LaunchConfig(4, 4));

      // 3. Update ModuleA to v2 (adds dependency on ModuleB)
      cluster.updateModule(new ModuleA_v2());
    }
  }
}
----
The `getModuleName` method is used here to give both versions of the module the same name, allowing the update.= Operating Rama Clusters

This page covers setting up Rama clusters and managing module deployments. For development, you can use `xref:testing.adoc[InProcessCluster]` without needing this information.

Key topics covered:

* Cluster setup and configuration
* Using the Rama CLI for management
* Launching, updating, and scaling modules
* Schedulers, monitoring, and the Cluster UI
* Node decommissioning and cluster upgrades

== Setting up a Rama cluster

A Rama cluster consists of a link:https://zookeeper.apache.org/[Zookeeper cluster], one Conductor node, and multiple Worker nodes.

. *Set up Zookeeper:* Rama uses Zookeeper to store cluster metadata. Follow the official link:https://zookeeper.apache.org/doc/current/zookeeperAdmin.html[Zookeeper deployment instructions].
. *link:https://redplanetlabs.com/download[Download] a Rama release:* Unpack it on all cluster nodes and client machines. The client and cluster must use the same release version. A release contains:
+
[cols="1,3"]
|===
| `rama`, `rama.jar`, `lib/` | The CLI, Rama implementation, and dependencies.
| `rama.yaml` | Configuration file for Rama daemons.
| `log4j2.properties` | Logging configuration.
| `logs/` | Default directory for log files.
|===
+
*System Requirements:* UNIX-based OS (Linux, macOS), Java (8, 11, 17, or 21), and Python 3.

. *Set up the Conductor Node:* The Conductor manages module lifecycles and serves the Cluster UI.
.. Unpack the Rama release.
.. Edit `rama.yaml` with the required configurations.
.. Launch with `./rama conductor`. For production, use a process supervisor like `systemd` or `monit`.
+
.Conductor `rama.yaml` Example
[source,yaml]
----
# Required
zookeeper.servers:
  - "1.2.3.4"
  - "5.6.7.8"
# Recommended
local.dir: "/data/rama"
# Optional (with defaults)
zookeeper.port: 2000
conductor.port: 1973
cluster.ui.port: 8888
conductor.child.opts: "-Xmx1024m"
----

. *Set up Worker Nodes:* Worker nodes run Supervisor daemons, which launch and manage module worker processes.
.. Unpack the same Rama release.
.. Edit `rama.yaml` with the required configurations.
.. Launch with `./rama supervisor`. Use a process supervisor and configure it *not* to kill subprocesses if the Supervisor dies, as running workers are unaffected.
.. It's recommended to configure worker nodes with swap space to handle temporary memory needs during module updates.
+
.Supervisor `rama.yaml` Example
[source,yaml]
----
# Required
zookeeper.servers:
  - "1.2.3.4"
  - "5.6.7.8"
conductor.host: "9.10.11.12"
# Recommended
local.dir: "/data/rama"
# Optional (with defaults)
supervisor.port.range: [3000, 4000]
supervisor.child.opts: "-Xmx256m"
supervisor.labels: ["gpu-enabled"]
----

Other configurations are available on the xref:all-configs.adoc[All Configs] page.

=== Adding a license

The embedded free license supports up to two Supervisor nodes. A paid license is required for larger clusters. Licenses are managed on the Conductor. If a license expires, the cluster shuts down gracefully without data loss; it will resume after a valid license is added and the Conductor is restarted.

* *Add a license:*
+
[source,text]
----
./rama upsertLicense --licensePath /path/to/license.edn
----

* *Remove expired licenses:*
+
[source,text]
----
./rama cleanupLicenses
----

=== Internal/external hostname configuration

For environments like AWS with distinct internal and external hostnames, you can configure `conductor.host` and `zookeeper.servers` with a map structure. Daemons use internal hostnames, while clients can choose.

[source,yaml]
----
conductor.host:
  internal: "10.0.1.5"
  external: "ec2-instance.public.com"
----

`RamaClusterManager.open()` uses external hostnames; `RamaClusterManager.openInternal()` uses internal ones.

== Using the Rama CLI

The `rama` script in the unpacked release is used to manage the cluster. Configure `conductor.host` in a local `rama.yaml` to connect.

Common commands include:
* `rama deploy`: Launches or updates a module.
* `rama scaleExecutors`: Changes a module's workers, threads, or replication factor.
* `rama destroy <moduleName>`: Destroys a module.
* `rama moduleStatus <moduleName>`: Prints a module's current status.
* `rama shutdownCluster`: Gracefully shuts down the cluster for an upgrade.
* `rama disallowLeaders <supervisorId>`: Moves leaders off a node for a patch upgrade.
* `rama allowLeaders <supervisorId>`: Allows leaders back on a node.
* `rama upsertLicense` / `cleanupLicenses`: Manages licenses.
* `rama help`: Displays all available commands.

The optional `--useInternalHostnames` flag on some commands forces the use of internal hostnames.

== Accessing modules from remote Java clients

Use `link:https://redplanetlabs.com/javadoc/com/rpl/rama/RamaClusterManager.html[RamaClusterManager]` to get clients for depots, PStates, and query topologies.

[source,java]
----
// Reads connection info from rama.yaml on the classpath
RamaClusterManager manager = RamaClusterManager.open();

// Or, provide config manually
Map config = new HashMap();
config.put("conductor.host", "1.2.3.4");
RamaClusterManager managerWithConfig = RamaClusterManager.open(config);
----

== Launching modules

Use `rama deploy` to launch a new module.

[source,text]
----
rama deploy \
--action launch \
--jar target/my-application.jar \
--module com.mycompany.MyModule \
--tasks 64 \
--threads 16 \
--workers 8 \
--replicationFactor 3
----

* `--tasks`: Number of tasks (must be a power of two).
* `--threads`: Number of task threads.
* `--workers`: Number of worker processes.
* `--replicationFactor`: Fault-tolerance level. Defaults to 1; 3 is recommended for production.
* `--jar`: Path to the module's uberjar.
* `--module`: The fully qualified module class or Clojure var.

*Building an uberjar:* A module must be packaged as an uberjar (jar-with-dependencies). When using Maven, declare the `rama` dependency with `<scope>provided</scope>` to avoid including it in the uberjar.

=== Worker configurations and dynamic options

* *Configurations* are set on launch/update and apply to a specific module instance. Use the `--configOverrides` flag with a path to a YAML file.
+
[source,yaml]
----
# overrides.yaml
worker.child.opts: "-Xmx8192m"
custom.serializations:
  - "com.rpl.myapp.serialization.MySerialization"
----

* *Dynamic Options* can be changed at any time and persist across updates. They can be set on launch via flags (`--moduleOptions`, `--topologyOptions`, etc.) or modified later via the CLI or Cluster UI. They follow a hierarchy (Module > Topology > Depot/PState), with more specific settings overriding general ones.

=== Isolation scheduler vs. dev scheduler

* *Dev Scheduler (default):* Spreads workers from all modules evenly across the cluster. Convenient for development but can lead to resource contention.
* *Isolation Scheduler (recommended for production):* Dedicates a fixed number of nodes to each module, preventing resource competition.

To use the isolation scheduler, configure it in the Conductor's `rama.yaml` and restart the Conductor.

[source,yaml]
----
conductor.assignment.mode:
  type: isolation
  modules:
    com.mycompany.Module1: 4
    com.mycompany.AnotherModule: 8
----

Existing modules can be transitioned to isolated nodes by running a no-op scaling command (`rama scaleExecutors` with no changes).

== Updating modules

Update a running module to a new code version using `rama deploy`.

[source,text]
----
rama deploy \
  --action update \
  --jar myapplication-1.2.1.jar \
  --module 'com.mycompany.MyModule' \
  --configOverrides overrides.yaml \
  --objectsToDelete '$$pStateToDelete,*depotToDelete'
----

* Parallelism settings (`--tasks`, etc.) are inherited from the previous version.
* *Instance configurations (`--configOverrides`) are NOT inherited and must be respecified.*
* Deleting a PState or depot is destructive and requires explicit confirmation with `--objectsToDelete`.

Module updates are highly coordinated to minimize downtime. PState queries have zero downtime. Depot appends and topology processing may have a brief pause.

== Scaling modules

Change a module's workers, threads, or replication factor with `rama scaleExecutors`.

[source,text]
----
rama scaleExecutors \
--module com.mycompany.MyModule \
--threads 90 \
--workers 30
----

* Unspecified flags retain their current values.
* Module instance configurations are automatically carried over to the new instance.
* Scaling can take longer than an update because data may need to be copied between nodes.

*Scaling Tips:* Monitor the "Task groups load" metric in the Cluster UI. Keep it below 70% to handle load spikes. Also, watch for diverging "progress" and "size" lines on microbatch topology charts.

*Task Scaling:* The number of tasks cannot be changed after launch. Plan for future growth when choosing the initial task count. A manual, complex process is required to change it later.

== Activating self-monitoring

Deploy the built-in monitoring module to collect detailed telemetry for the Cluster UI.

[source,text]
----
rama deploy \
  --action launch \
  --systemModule monitoring \
  --tasks 32 \
  --threads 8 \
  --workers 4
----

This module is operated like any other. A good rule of thumb is one monitoring worker per 70 workers from other modules.

* *Telemetry Retention:* Use `rama monitoringConfig` to manage disk usage by setting retention windows for telemetry data.

== Cluster UI

The Cluster UI (default port 8888 on the Conductor) provides a detailed view of the cluster, modules, and performance telemetry.

* *Main Page:* Lists all modules and their operational state (e.g., `[:running]`, `[:updating-next-assigned]`).
* *Module/Worker Pages:* Offer granular telemetry, including:
** *Task groups load:* The percentage of time task threads are busy. A key indicator for scaling.
** *System event throughput/duration:* Fine-grained metrics on internal and module-specific events.
* *Depot/PState/Topology Pages:* Show telemetry specific to each component, like append rates, query latencies, and processing progress.

== Decommissioning a node

To move workers off a failed or decommissioned node, run a module update with the `--reassignSupervisors` flag.

[source,text]
----
rama deploy \
   --action update \
   --reassignSupervisors <supervisorIdToDecommission> \
   --jar ...
----

* This command must be run for each module with workers on the target node.
* The operation assumes the target node is inaccessible.
* If a task group's only in-sync replica is on the failed node, data will be lost. The `--forceReassign` flag is required to proceed in this rare scenario.

== Upgrading a Rama cluster

There are two upgrade procedures, depending on the version change.

=== Patch version upgrade (e.g., 0.13.0 to 0.13.1)

This is a rolling upgrade with no cluster downtime if modules have a replication factor >= 2.

. For each Supervisor node:
.. Run `./rama disallowLeaders <supervisorId>`.
.. Stop the Supervisor and worker processes.
.. Replace the Rama release with the new version, using the same `local.dir`.
.. Restart the Supervisor.
.. Run `./rama allowLeaders <supervisorId>`.
.. Wait a minute for leaders to rebalance.
. Upgrade the Conductor node similarly (stop, replace release, restart).
. Restart all clients with the new Rama version.

=== Atomic version upgrade (major/minor version change)

This process requires cluster downtime (typically 5-10 minutes with preparation).

. Shut down the cluster gracefully with `rama shutdownCluster`. Wait for completion.
. Stop all Conductor and Supervisor processes.
. On all nodes, unpack the new Rama release, ensuring `rama.yaml` points to the same `local.dir`.
. Start the new Conductor and Supervisor processes.
. For each module (including `monitoring`), submit an upgrade command using `rama deploy --action upgrade`. The module structure must be identical to the previous version, and instance configs must be respecified.
. Once all modules are upgraded, Rama finalizes the process and resumes normal operation.
. Upgrade all clients to the new Rama version.= Partitioners

Partitioners control where computation runs in a Rama ETL topology, moving dataflow between tasks as needed. This allows you to fetch and update distributed data efficiently.

All examples on this page can be found in the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== How Partitioners Work

A Rama module is deployed across multiple tasks, each running the same code but holding different partitions of PStates and depots. A partitioner directs the flow of computation to a specific target task (or tasks), ensuring that subsequent operations run in the right place.

.hashPartition Example
This example uses `hashPartition`, which routes data to a task based on the hash of an input variable. The same input will always be routed to the same task.

[source,java]
----
// Module launched with 8 tasks
s.source("*depot", Depot.random()).out("*k")
 .each(Ops.PRINTLN, "Start task", "*k", new Expr(Ops.CURRENT_TASK_ID))
 .hashPartition("*k")
 .each(Ops.PRINTLN, "End task", "*k", new Expr(Ops.CURRENT_TASK_ID));

// Appending "cagney" twice...
----

.Output
[source,text]
----
Start task cagney 3
End task cagney 4
Start task cagney 1
End task cagney 4
----
Even though the data starts on random tasks (1 and 3), `hashPartition("*k")` ensures both records for `"cagney"` are processed on the same deterministic end task (4).

.allPartition Example
The `allPartition` operation broadcasts the computation to _every_ task in the module.

[source,java]
----
// Module launched with 4 tasks
s.source("*depot", Depot.random()).out("*k")
 .each(Ops.PRINTLN, "Start task", "*k", new Expr(Ops.CURRENT_TASK_ID))
 .allPartition()
 .each(Ops.PRINTLN, "End task", "*k", new Expr(Ops.CURRENT_TASK_ID));

// Appending "groucho"...
----

.Output
[source,text]
----
Start task groucho 2
End task groucho 3
End task groucho 1
End task groucho 0
End task groucho 2
----
The processing for `"groucho"` starts on a single task but is then replicated to run on all four tasks (0, 1, 2, 3). Use `allPartition` with care, as its cost scales with the number of tasks.

=== Guarantees and Optimizations

*Local Ordering*:: Partitioners guarantee that if task A sends events 1, 2, and 3 to task B, task B will receive and process them in that exact order.

*Selective Variable Transfer*:: Rama analyzes the topology and only transfers the variables needed after the partitioner, minimizing network traffic and serialization overhead.

*Automatic Batching*:: Events are automatically batched before being transferred between tasks to maximize throughput. Microbatch topologies typically achieve better batching than stream topologies due to less stringent latency requirements.

== Built-in Partitioners

* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#hashPartition-java.lang.Object-[hashPartition]`:: Partitions based on the hash of the input. Deterministic.
* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#allPartition--[allPartition]`:: Broadcasts to every task in the module.
* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#globalPartition--[globalPartition]`:: Sends computation to task 0.
* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#shufflePartition--[shufflePartition]`:: Distributes work evenly using a random round-robin algorithm.
* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#directPartition-java.lang.Object-[directPartition]`:: Sends computation to a specific task ID.
* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#pathPartition-java.lang.String-com.rpl.rama.Path-[pathPartition]`:: Partitions according to a PState's configured key partitioner. Used implicitly by `select`.
* `link:https://redplanetlabs.com/javadoc/com/rpl/rama/Block.html#originPartition--[originPartition]`:: In query topologies, returns computation to the task where the query started.

Many partitioners have variants that accept a PState or depot. These restrict partitioning to only the tasks hosting that object, which is essential when working with objects from other modules.

== Custom Partitioners

You can implement custom partitioning logic using `customPartition` and a `RamaFunction`. The function's first argument is always the number of available partitions, followed by any variables you pass. It must return a target partition number.

[source,java]
----
// Partitions to task 0 if n2 > n1, otherwise the last task.
public static class MyPartitioner implements RamaFunction3<Integer, Integer, Integer, Integer> {
  @Override
  public Integer invoke(Integer numPartitions, Integer n1, Integer n2) {
    if(n2 > n1) return 0;
    else return numPartitions - 1;
  }
}

// ... in topology ...
s.source("*depot").out("*tuple")
 .each(Ops.EXPAND, "*tuple").out("*n1", "*n2")
 .customPartition(new MyPartitioner(), "*n1", "*n2")
 .each(Ops.PRINTLN, "Final task", "*tuple", new Expr(Ops.CURRENT_TASK_ID));
----

When partitioning for a PState or depot, `numPartitions` will be the number of partitions for that object. Otherwise, it is the total number of tasks in the module.

[#mirror-partitioners]
== Partitioning for Foreign PStates and Depots

When accessing a PState or depot from another module (a "mirror" object), its partitions may not align one-to-one with the current module's tasks. To interact with a specific partition of a mirror object, you must first use a partitioner.

This moves computation to the correct task in the current module *and* specifies which foreign partition to target for subsequent `localSelect` or `depotPartitionAppend` calls.

[source,java]
----
public class Module2 implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    // Module1 has 8 tasks, Module2 has 4 tasks.
    setup.clusterPState("$$other", Module1.class.getName(), "$$p");

    // ...
    s.source("*depot").out("*tuple")
     .each(Ops.EXPAND, "*tuple").out("*k", "*v")
     // Partition to the task in *this* module that handles the
     // partition for *k in the *other* module's PState.
     .hashPartition("$$other", "*k")
     // Now localSelect knows which partition of "$$other" to read.
     .localSelect("$$other", Path.key("*k").nullToVal(0L)).out("*n")
     // ...
  }
}
----

A full discussion is on the xref:module-dependencies.adoc[Module dependencies page]. However, using `select` is often a more concise way to achieve this.

== Implicit Partitioner in `select`

The `select` operation is a convenient shortcut that combines partitioning and reading from a PState. It is equivalent to using `pathPartition` followed by `localSelect`.

`select` automatically uses the PState's configured xref:pstates.adoc#_pstate_options[key partitioner] to determine the target task. If the PState is global (has only one partition), no partitioning is needed.

Using `select` is the preferred and most concise way to query both colocated and mirror PStates.

== Summary

Partitioners are a core feature for controlling dataflow in Rama. They seamlessly move computation across a cluster, with automatic optimizations like batching and selective variable transfer. While many simple topologies can rely on initial depot partitioning, complex topologies that manage multiple, differently-partitioned PStates use partitioners extensively to direct computation precisely where it's needed.= Paths

Paths are the core mechanism in Rama for reading and writing PStates. They offer a flexible, concise, and composable way to specify fine-grained behavior on complex data structures.

== Navigation Model

A path is a sequence of _navigators_ that defines a traversal through a data structure. A path can navigate to zero, one, or many locations.

Consider this data structure:
[source, java]
----
Map data = new HashMap() {{
  put("a0", new HashMap() {{
    put("a1", Arrays.asList(9, 3, 6));
    put("b1", Arrays.asList(0, 8));
  }});
  put("b0", new HashMap() {{
    put("c1", Arrays.asList("x", "y"));
  }});
}};
----

Paths navigate this structure by "hopping" between locations:

*   **Single Navigation:** `Path.key("a0", "a1").nth(1)` navigates to the value `3`.
*   **Multi-Navigation:** `Path.key("a0").mapVals().all()` navigates to each of the five numbers individually: `9, 3, 6, 0, 8`.
*   **Filtered Navigation:** `Path.key("a0").mapVals().all().filterLessThan(7)` navigates to `3, 6, 0`.

The behavior of `select` depends on the context. In the dataflow API, it emits a separate record for each navigated value. In the PState client API, it returns a single list of all navigated values.

== Value Navigators

Value navigators navigate to zero or more _existing_ subvalues within a data structure.

*   `key(k)`: Navigates to the value for key `k` in a map.
*   `nth(i)`: Navigates to the value at index `i` in a list.
*   `mapVals()`: Navigates to each value in a map.
*   `all()`: Navigates to every element in a list, set, or map entry.
*   `first()`: Navigates to the first element of a list.
*   `mapKey(k)`: Navigates to the _key_ `k` itself, not its value. This is primarily useful in transforms for renaming a map key.

== Virtual Value Navigators

Virtual value navigators navigate to locations that do not yet exist, enabling insertion during transforms. They have no effect in `select` operations.

*   `voidSetElem()`: Navigates to a "void" location in a set. Transforming this location adds a new element.
*   `afterElem()`, `beforeElem()`: Navigate to void locations at the end or beginning of a list for appending/prepending.
*   `beforeIndex(i)`: Navigates to a void location before index `i` for insertion.

== Filter Navigators

Filter navigators conditionally continue navigation from the current location. If the condition fails, the path stops for that branch.

*   `filterPred(predicate)`: Continues if the `predicate` function returns true for the current value.
*   `filterSelected(path)`: Continues if the given sub-`path` navigates to at least one value from the current location. It does not change the current location.

== Substructure Navigators

Substructure navigators navigate to a smaller version of a data structure (e.g., a sublist or submap). Transforms on the substructure affect the original data structure.

*   `sublist(start, end)`: Navigates to a contiguous sublist.
*   `filteredList(path)`: Navigates to a sublist containing only elements that match the filter `path`. The original positions are maintained for transforms.
*   `sortedMapRange(start, end)`, `sortedMapRangeFrom(start, limit)`: Efficiently navigate ranges in sorted maps (e.g., in subindexed PStates). Essential for range queries.

== View Navigators

View navigators navigate to a transformation of the currently navigated value.

*   `view(function)`: Navigates to the result of applying the `function` to the current value.
*   `transformed(path)`: Navigates to a new value by applying a transform `path` to the current value.
*   `nullToVal(defaultValue)`: If the current value is `null`, navigates to `defaultValue`. Otherwise, stays at the current value.

[NOTE]
====
When used in PState queries from a client, functions passed to `view` must exist on the module's classpath. Lambdas are not permitted.
====

== Control Navigators

Control navigators manipulate the flow of navigation, similar to control flow statements in programming.

*   `ifPath(conditionPath, thenPath, [elsePath])`: If `conditionPath` selects any values, continue with `thenPath`, otherwise use `elsePath` (or `stop()` if not provided).
*   `multiPath(path1, path2, ...)`: Navigates down each provided path sequentially from the current location. Like an `OR` in a filter condition.
*   `subselect(path)`: Executes the sub-`path` from the current location and navigates to a single list containing all the results. In transforms, it retains the original locations of the selected items, allowing for complex, cross-substructure manipulations.
*   `stay()`, `stop()`: Continues navigation at the current location, or stops it entirely.

== Transform Paths

Paths used in transforms must end with a "term" navigator. The navigation itself works identically to `select`.

Term navigators:
*   `termVal(value)`: Replaces the navigated location with a static `value`.
*   `term(function, [args...])`: Replaces the navigated location with the result of applying the `function` to it.
*   `termVoid()`: Removes the element at the navigated location from its parent collection.

[NOTE]
====
Transform paths operate on PStates and Clojure-style immutable data structures, not standard mutable Java collections like `HashMap` or `ArrayList`.
====

== Custom Navigators

For advanced use cases, such as handling custom data types, you can implement the `Navigator` interface.

[source, java]
----
public interface Navigator<T> extends RamaSerializable {
  interface Next { Object invokeNext(Object obj); }
  Object select(T obj, Next next);
  T transform(T obj, Next next);
}
----

*   `select(obj, next)`: Implements read-only navigation. Calls `next.invokeNext(subvalue)` for each item to navigate to.
*   `transform(obj, next)`: Implements transform logic. Calls `next.invokeNext(subvalue)` to get the replacement value and updates `obj`.

Use `Path.customNav(new MyNavigator())` to add a custom navigator to a path.

== Value Collection

A feature for collecting values into a side-list during navigation. If any values are collected, a `select` operation returns a list containing the collected values, with the final navigated value appended.

Key methods include `collectOne(path)`, `collect(path)`, and `putCollected(value)`.

== Reactivity

Reactive queries (`proxy`, `proxyAsync`) are implemented at the path level. Fine-grained reactivity is supported for built-in navigators. Custom navigators will result in coarse-grained diffs.

== Summary

Paths are a powerful, composable abstraction for querying and manipulating PStates. While there are many navigators, a small subset provides significant utility. Mastery comes from understanding the core navigation model and composing navigators to achieve the desired behavior.= PStates

Partitioned states (PStates) are durable, replicated data structures that index data for application queries. Rama programming centers on defining PState structures and how they are updated. Based on composable data structures, PStates offer far more flexibility than traditional databases.

PStates also introduce "fine-grained reactive queries", a novel feature that allows applications to be reactive by receiving minimal "diffs" when data changes, rather than the entire new value.

== PStates versus Databases

Databases offer "fixed indexing models" (key/value, document, relational), which are essentially specific combinations of data structures. If a model doesn't fit a new use case, you often need another database.

Rama provides a "flexible indexing model" where each PState can be any combination of data structures you need. This aligns backend development with standard programming practices of choosing the right data structures for the job.

Key differences from traditional databases:

*   **Update Model**: A database is global mutable state. A PState is only updated by its owning ETL topology, which consumes data from an immutable depot log. This centralizes write logic, simplifies data flow, and provides a replayable history of all changes.
*   **Reactivity**: Databases offer, at best, "coarse-grained reactivity" (e.g., triggers) that only notify you that a value changed. PStates provide "fine-grained reactivity", telling you precisely *how* a value changed, no matter how deeply nested it is.

== Declaring PStates

PStates are declared in stream or microbatch topologies with a var (e.g., `$$p`) and a schema.

[source, java]
----
public class BasicPStateDeclarationsModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    StreamTopology s = topologies.stream("s");
    s.pstate("$$p1", Long.class);
    s.pstate("$$p2", PState.mapSchema(String.class, Integer.class));
    s.pstate(
      "$$p3",
      PState.mapSchema(String.class,
        PState.fixedKeysSchema(
          "count", Integer.class,
          "someList", PState.listSchema(String.class))));
  }
}
----

Available schema types:

*   `mapSchema`: A map with a specified key class and value schema.
*   `setSchema`: A set. Cannot contain other schemas.
*   `listSchema`: A list.
*   `fixedKeysSchema`: A map with a fixed set of keys, each with its own value schema.

Top-level PState schemas can only be a `mapSchema`, `fixedKeysSchema`, or a direct class reference.

=== Subindexing

For performance, Rama can index elements of a collection individually. This is called "subindexing". It is the default for top-level maps and can be enabled for nested structures.

[source, java]
----
s.pstate("$$p",
  PState.mapSchema(String.class,
    PState.setSchema(Long.class).subindexed()));
----

Without subindexing, reading a single element from a large collection requires reading the entire collection from disk. With subindexing, Rama can access individual elements efficiently. This is critical for performance when dealing with collections containing thousands or millions of elements.

Subindexed maps and sets are sorted, enabling efficient range queries using navigators like `sortedMapRange` and `sortedSetRange`.

==== Size Tracking

By default, subindexed structures track their size, making size queries an O(1) operation (e.g., `Path.key("a").view(Ops.SIZE)`). This adds a small overhead to writes. It can be disabled for maximum write performance if size queries are not needed.

[source, java]
----
PState.setSchema(Long.class).subindexed(SubindexOptions.withoutSizeTracking());
----

==== Deleting Subindexed Structures

Subindexed structures can be removed like any other value (e.g., `.localTransform("$$p", Path.key("a").termVoid())`).

*Caution*: Deleting a subindexed structure directly cleans up its elements from disk. Deleting its *parent* container will only remove the reference, orphaning the elements on disk. Always delete nested subindexed structures explicitly before deleting their parent.

=== PState Options

*   `.global()`: Creates a single-partition PState on task 0. Useful for global aggregates like "top 10" lists or global counts.
*   `.initialValue(val)`: Sets an initial value for each partition on module launch. Only for PStates with a class reference schema.
*   `.makePrivate()`: Makes the PState readable only by its owning topology.
*   `.keyPartitioner(fn)`: Specifies a custom function to route client queries to partitions, overriding the default hash-based partitioning.

== Using PStates in Topologies

PStates are queried and updated within topologies using the dataflow API.

=== Basic Querying

*   `localSelect`: Queries the PState partition on the current task. It is synchronous.
*   `select`: Partitions the computation based on a key in the path, then queries the target task.

[source, java]
----
// Queries the collection at "*k" on the local partition
.localSelect("$$p", Path.key("*k").all()).out("*v")

// Partitions by "*k", then queries the collection on the target partition
.select("$$p", Path.key("*k").all()).out("*v")
----

=== Yielding Select

For queries over large amounts of data, use `SelectOptions.allowYield()`. This prevents blocking the task thread by executing the query over multiple events, operating on a stable snapshot of the PState.

[source, java]
----
.localSelect("$$p", Path.key("cagney").all(), SelectOptions.allowYield()).out("*v")
----

=== Transforming

PStates are updated by their owning topology using:

*   `localTransform`: Uses xref:paths.adoc[Paths] to specify fine-grained updates.
*   `agg` / `compoundAgg`: Higher-level abstractions for aggregation.

== Using PState Clients

PState clients, obtained from a `RamaClusterManager`, perform ad-hoc point queries from outside a Rama module. They use yielding selects, so queries can be of any size.

The non-reactive API includes `select` (returns a list of results) and `selectOne` (returns a single result).

[source, java]
----
// In a client application
PState p = cluster.clusterPState(moduleName, "$$p");

// Returns a list with one element: [#{12 14 10}]
List results = p.select(Path.key("davis"));

// Returns the set itself: #{12 14 10}
Set result = p.selectOne(Path.key("davis"));
----

*Note*: Lambdas cannot be used in PState client paths, as the function object's class must exist on both the client and module classpaths.

=== How Client Queries Are Routed

For partitioned PStates, queries must either begin with `Path.key(someKey)` to use the configured key partitioner, or you must provide an explicit partitioning key as an argument to `select`/`selectOne`. The latter is useful when a PState is partitioned by data not present in its own structure.

=== Non-blocking API

Asynchronous versions like `selectAsync` and `selectOneAsync` are available, returning `CompletableFuture` objects.

== Reactive Queries

Reactive queries are created with `proxy`, which works like `selectOne` but returns a stateful `ProxyState` object. This object's value is kept in sync with the PState on the cluster by receiving fine-grained "diffs" in the background.

[source, java]
----
// Get a live, self-updating view of the map at key "a"
ProxyState<Map> proxy = p.proxy(Path.key("a"));

System.out.println("Initial value: " + proxy.get()); // e.g., {"b": 1, "c": 1}

// After an update on the server...
Thread.sleep(50);
System.out.println("New value: " + proxy.get()); // e.g., {"b": 1, "c": 2}
----

A callback can be registered to inspect the diffs as they arrive.

[source, java]
----
p.proxy(Path.key("a"), new ProxyState.Callback<Map>() {
  @Override
  public void change(Map newVal, Diff diff, Map oldVal) {
    System.out.println("Received diff: " + diff);
  }
});
----
// Example output: Received diff: KeyDiff[c | NewValueDiff[2]]

This is far more efficient than re-sending the entire map on every change.

=== Diffs

The type of diff generated by a PState update is determined by the transform path used. For example, `Path.key("k").term(...)` generates a `KeyDiff`. These diffs are only computed if there are active subscribers. Subscribers only receive the portion of a diff relevant to their proxied path.

=== Processing Diffs

Diffs can be processed using a "double dispatch" API. You create a processor that implements interfaces for the specific diff types you want to handle (e.g., `KeyDiff.Processor`). Rama will automatically convert or expand diffs to match what your processor can handle.

It is critical to handle `ResyncDiff`, which is sent on initial subscription or after a fault-recovery event. This indicates your client should re-evaluate the entire new value.

=== Fault Handling and Cleanup

`ProxyState` is robust, using CRCs and heartbeats to detect and recover from errors, automatically resyncing if needed.

An active `ProxyState` consumes resources on both the client and server. Call `proxy.close()` when it is no longer needed to release these resources promptly. The server will also garbage collect stale subscriptions after a timeout.

== Schema Validation

By default, Rama performs a full validation of data against the PState schema on every write. For maximum performance in production, two potentially expensive checks can be disabled via configuration:
*   `pstate.validate.subindexed.structure.locations`: Prevents moving a subindexed structure.
*   `pstate.maximal.schema.validations`: Prevents deep iteration of large non-subindexed structures.

== Migrations

PStates can be migrated to a new schema during a module update. The migration is specified with a transformation function. Rama applies the migration on-read immediately after the update, while migrating the data on disk in the background. This means a module update is not delayed by the migration.

[source, java]
----
// Original Schema: PState.mapSchema(Long.class, Long.class)

// Migrated Schema:
s.pstate("$$p",
  PState.mapSchema(Long.class,
    PState.migrated(
      String.class,
      "myMigrationId",
      (Object o) -> { // Migration function must be idempotent
        if(o instanceof String) return o;
        else return o.toString() + "!";
      })));
----

Migrations can also convert structures to subindexed, or add/remove keys from a `fixedKeysSchema`. The migration function must not change data structure types in a way that breaks existing topology code (e.g., keep `clojure.lang.IPersistentMap` if paths operate on it).

*   **Valid Locations**: Migrations can only be declared on "full values" (values of a top-level map or a subindexed structure), not on parts of a larger, non-subindexed value.
*   **Updates**: If you update a module mid-migration, the migration continues if the migration ID is unchanged. Changing the ID restarts the migration.
*   **Telemetry**: The Cluster UI provides detailed telemetry on migration progress.
*   **Completion**: Once the UI shows a migration is complete, the `PState.migrated` wrapper can be removed in the next module update.
*   **Rate Control**: Migration speed is controllable via dynamic options to manage task thread load.
*   **Implicit Migrations**: Simple schema changes, like generalizing `Integer.class` to `Object.class`, do not require a formal migration.

== Tuning Options

=== Configurations (Fixed on Deploy)

*   `pstate.rocksdb.options.builder`: Customize RocksDB settings for map-based PStates.
*   `foreign.pstate.operation.timeout.millis`: Timeout for queries to PStates in other modules.
*   `foreign.proxy.*`: Configure `ProxyState` thread pools and failure thresholds.
*   `pstate.validate.*` / `pstate.maximal.*`: Disable expensive schema validation checks.

=== Dynamic Options (Editable in UI)

*   `pstate.excessive.write.time.warning.millis`: Log `localTransform` calls that exceed a time threshold.
*   `pstate.reactivity.queue.limit`: Limit for queued-up reactive subscribers.
*   `pstate.yielding.select.page.size`: Default page size for yielding selects.= Query Topologies

Query topologies are a powerful Rama feature for executing real-time, on-demand, distributed queries across any PState or task. They are defined using the same expressive Java dataflow API as ETLs, allowing them to handle complex logic that goes far beyond simple point queries.

While PState clients are excellent for fetching data from a single partition, query topologies excel at querying multiple PStates simultaneously or aggregating data across the entire cluster.

All examples can be found in the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== Structure of Query Topologies

Query topologies behave like distributed functions: they accept input arguments and return a single output object. They are defined by calling `topologies.query()` within a `RamaModule`.

[source, java]
----
// A simple query topology that computes (*a + *b + 1)
topologies.query("q", "*a", "*b").out("*res")
          .each(Ops.PLUS, "*a", "*b", 1).out("*res")
          .originPartition();
----

Key characteristics:

*   **Definition:** `topologies.query("name", "*input1", ...).out("*output")` defines the signature.
*   **Implicit Batch Block:** All query topologies are implicitly batch blocks, enabling powerful aggregation patterns.
*   **`originPartition()`:** The computation must end with this special partitioner, which routes the final result back to the query's origin.
*   **Single Emission:** The output variable must be emitted exactly once. Emitting it zero or multiple times will cause an error.

For example, this topology is problematic because `Ops.EXPLODE` can emit the output var multiple times:
[source, java]
----
topologies.query("q", "*list").out("*res")
          .originPartition()
          .each(Ops.EXPLODE, "*list").out("*res");
----

=== Example: URL Reach

A more complex example is calculating "URL reach"the number of unique users exposed to a URL. This involves fetching all users who shared the URL, fetching all of their followers, and then computing a distinct count of the followers.

This can be implemented efficiently with a query topology that parallelizes the work.

[source, java]
----
public class ReachModule implements RamaModule {
  // Sub-batch to compute partial distinct counts in parallel
  private SubBatch partialReachCounts(String urlVar) {
    return new SubBatch(
      Block.hashPartition(urlVar)
           .localSelect("$$urlToUsers", Path.key(urlVar).all()).out("*userId")
           .select("$$followers", Path.key("*userId").all()).out("*reachedUserId")
           .hashPartition("*reachedUserId") // <-- Key to parallel distinct count
           .agg(Agg.set("*reachedUserId")).out("*partialReachedSet")
           .each(Ops.SIZE, "*partialReachedSet").out("*count"),
      "*count");
  }

  @Override
  public void define(Setup setup, Topologies topologies) {
    // ... PState and ETL definitions for $$urlToUsers and $$followers ...

    topologies.query("reach", "*url").out("*numUniqueUsers")
              .subBatch(partialReachCounts("*url")).out("*partialCount")
              .originPartition()
              .agg(Agg.sum("*partialCount")).out("*numUniqueUsers");
  }

  public static void main(String[] args) throws Exception {
    try(InProcessCluster cluster = InProcessCluster.create()) {
      // ... Cluster setup, data loading, and waiting for ETLs ...

      QueryTopologyClient<Integer> reach = cluster.clusterQuery(module.getClass().getName(), "reach");
      System.out.println("grapefruit.com reach: " + reach.invoke("grapefruit.com"));
      // Prints: grapefruit.com reach: 6
    }
  }
}
----

The query distributes the computation by hashing on `*reachedUserId`. This sends all instances of the same follower to the same task, where a set aggregation handles the distinct counting. The final result is a sum of the sizes of these partial sets.

image::./diagrams/query-diagrams/reach.png[]

This approach is highly efficient because the `select` and `localSelect` calls read from PStates that are colocated on the same task, minimizing network overhead.

== Query Topology Client

You get a `QueryTopologyClient` from a `RamaClusterManager` (or `InProcessCluster` for tests).

[source, java]
----
QueryTopologyClient<Integer> q = cluster.clusterQuery(moduleName, "q");
----
Clients have two ways to execute a query:

*   **`invoke(...)`**: A blocking call that returns the result directly or throws an exception on failure.
+
[source, java]
----
Integer result = q.invoke(1, 2);
----

*   **`invokeAsync(...)`**: A non-blocking call that immediately returns a `CompletableFuture`. The future is completed with the result or an exception.
+
[source, java]
----
CompletableFuture<Integer> futureResult = q.invokeAsync(1, 2);
----

== Leading Partitioner Optimization

To improve performance, a query can start execution directly on the task holding the initial data it needs. This "leading partitioner optimization" saves network hops.

The optimization is triggered if the *first operation* in a query topology is a suitable partitioner (e.g., `hashPartition`).

Restrictions for a partitioner to be "leading":
*   Must be a built-in partitioner that targets a single task.
*   Its inputs must be query topology input variables.
*   Cannot be a `select` call, as its implicit partitioner does not count. This is why the "reach" example uses `.hashPartition` followed by `.localSelect`.

If no leading partitioner is present, the query starts on a random task.

== Invoking Colocated Query Topologies

Query topologies can be invoked from other topologies in the same module using `.invokeQuery()`. This is useful for decomposing complex logic.

[source, java]
----
// "q2" invokes "q1"
topologies.query("q1", "*a").out("*res")
          .each(Ops.TIMES, "*a", 3).out("*res")
          .originPartition();

topologies.query("q2", "*a").out("*res")
          .invokeQuery("q1", "*a").out("*v")
          .each(Ops.PLUS, "*v", 1).out("*res")
          .originPartition();
----

== Recursive Query Topologies

Query topologies can call themselves, enabling recursion. Rama protects against infinite recursion by enforcing the query's timeout.

[source, java]
----
// Recursive Fibonacci implementation
topologies.query("fib", "*n").out("*res")
          .ifTrue(new Expr(Ops.OR, new Expr(Ops.EQUAL, "*n", 0),
                                   new Expr(Ops.EQUAL, "*n", 1)),
            Block.each(Ops.IDENTITY, 1).out("*res"),
            Block.invokeQuery("fib", new Expr(Ops.DEC, "*n")).out("*a")
                 .invokeQuery("fib", new Expr(Ops.MINUS, "*n", 2)).out("*b")
                 .each(Ops.PLUS, "*a", "*b").out("*res"))
          .originPartition();
----

== Temporary In-Memory State

For complex queries needing temporary state, Rama provides an implicit, per-invocation PState.

*   **Name:** `$$<queryName>$$` (e.g., `$$findPath$$` for a query named `"findPath"`).
*   **Scope:** The state is unique to each invocation and is cleared afterward. Each partition starts as `null`.

[source, java]
----
topologies.query("foo", "*v", "*v2").out("*res")
          .hashPartition("*v")
          // Store *v2 in the temporary PState on the current partition
          .localTransform("$$foo$$", Path.termVal("*v2"))
          .shufflePartition()
          .hashPartition("*v")
          // Read the stored value back from the temporary PState
          .localSelect("$$foo$$", Path.stay()).out("*v3")
          .originPartition()
          .each(Ops.PLUS, "*v", "*v3").out("*res");
----

== Tuning Options

Query topologies have one dynamic option, editable in the Cluster UI:

*   `topology.query.timeout.millis`: The execution timeout for the query.

== Summary

Designing a Rama application involves balancing precomputation (ETLs and PStates) with on-demand computation. Query topologies provide a flexible and powerful tool for the on-demand side. By using the same dataflow API as ETLs, they make it easy to implement even sophisticated, distributed query logic with high efficiency.= Replication
All data in Ramaincluding depots, PStates, and internal stateis replicated across a configurable number of nodes for fault tolerance. Replication is automatic and managed behind the scenes; as a user, you only need to configure the desired level of redundancy. While not strictly required, replication is highly recommended for any production system.

This document provides a high-level overview of Rama's replication mechanism, covering:
* The core replication process using a replication log (replog).
* The roles of leaders and followers.
* The concepts of In-Sync Replicas (ISR) and Out-of-Sync Replicas (OSR).
* How to configure replication settings.
* How lagging replicas catch up to the leader.

== How Data is Replicated
Replication is configured with a `replicationFactor`, which determines how many copies of your data are stored. The unit of replication is the xref:terminology.adoc#_task_group_task_thread[task group]. Each replica of a task group is assigned to a xref:terminology.adoc#_worker[worker] on a different physical node to protect against machine failure.

image::./diagrams/replication-diagrams/assignment.png[]

For each task group, one replica is designated the *leader* and the others are *followers*.

*   *Leader*: Executes all work for its tasks, including ETL code, PState updates, depot appends, and serving client queries.
*   *Follower*: Passively receives updates from the leader and maintains a copy of the state.

If a leader fails, an in-sync follower is promoted to become the new leader.

The core of this process is the *replog* (replication log), an on-disk, sequential log of every change within a task group. Each change is a "replog entry" with a unique offset. Examples of replog entries include depot appends, PState updates, and internal progress updates.

The replication workflow is as follows:
1.  The leader creates a new replog entry.
2.  It forwards the entry to its followers.
3.  Once a sufficient number of followers confirm they have received the entry, the leader advances its *apply offset*.
4.  Advancing the apply offset allows the leader (and subsequently, its followers) to *apply* the changes from the entry, making them visible to clients.

This process guarantees that once data is visible, it will persist even if the leader fails, as any new leader will have that data.

== Leadership Balancing
The leader task thread performs significantly more work than a follower. To ensure efficient resource usage, Rama actively balances the distribution of leaders across the cluster. An unbalanced distribution can lead to performance bottlenecks, where some nodes are overloaded while others are idle.

image::./diagrams/replication-diagrams/balanced-leadership.png[]

The Conductor runs a "leadership balancing" routine that identifies a more optimal distribution of leaders and coordinates the handoff from existing leaders to better-located followers. You may see modules briefly enter a `[:leadership-balancing]` state in the Cluster UI during this process.

== ISR and OSR
The concepts of "sufficient followers" and "in-sync followers" are defined by the ISR and OSR.

*   *ISR (In-Sync Replicas)*: The set of replicas (including the leader) that have all the data necessary to become a leader. Only members of the ISR are eligible for leader promotion.
*   *OSR (Out-of-Sync Replicas)*: Replicas that are lagging and cannot become the leader until they catch up.

A critical configuration is the *min-ISR*, which specifies the minimum number of ISR members that must acknowledge a replog entry before it can be applied.

If a leader attempts to apply an entry but an ISR member is unresponsive, a stall timer begins.
*   *If the timer expires and removing the unresponsive follower still leaves the ISR size >= min-ISR:* The follower is moved to the OSR, and the leader applies the entry. This prevents the lagging follower from being promoted and causing data inconsistency.
*   *If removing the follower would violate min-ISR:* The leader stalls. It cannot apply the entry, and all operations for that task group (depot appends, PState updates) will fail until the follower recovers or is replaced.

A stall is a critical issue that can cascade and halt all microbatch processing in a module. The `replicationFactor` and `min-ISR` settings represent a trade-off: higher values provide better data integrity but increase the risk of stalls. A common and recommended configuration is a replication factor of 3 and a min-ISR of 2.

== Configuring Replication
The `replicationFactor` is set at launch or when scaling a module via the CLI.

.Launching a module with a replication factor of 3
[source, text]
----
rama deploy --action launch --module com.mycompany.MyModule --workers 8 --replicationFactor 3
----

.Scaling a module
[source,text]
----
rama scaleExecutors --module com.mycompany.MyModule --workers 30 --replicationFactor 3
----

The `min-ISR` is a xref:operating-rama.adoc#_worker_configurations_and_dynamic_options[dynamic option] named `replication.min.isr` that can be changed at any time, typically in the Cluster UI. If unset, it defaults to `replicationFactor - 1`.

== Leader Epochs
A leader epoch is a version number that increments each time a new leader is elected for a task group. It's an internal mechanism that ensures a safe leadership transition.

When a new leader takes over, the replogs of the remaining ISR members may be slightly inconsistent. The new leader uses its epoch and replog as the authoritative source of truth. It instructs followers to discard any unapplied replog entries that are inconsistent with its own log. This is safe because those entries were never made visible. The new leader then ensures all its own unapplied entries are fully replicated before it begins processing new work.

== Near-Horizon and Far-Horizon Catchup
A follower in the OSR continuously works to catch up and rejoin the ISR. The method it uses depends on how far behind it is.

*   *Near-Horizon Catchup*: Used when a follower is only slightly behind. It receives replog entries from the leader, just like normal replication.
*   *Far-Horizon Catchup*: Used when a follower is too far behind to efficiently replay the replog. This mechanism transfers the underlying data files for PStates and depots directly from the leader. This occurs over several "transfer rounds" until the follower is close enough to finish with near-horizon catchup.

== How Replication Uses the Metastore
Rama's xref:terminology.adoc#_metastore[Metastore] (implemented with ZooKeeper) stores cluster metadata and is used for coordination. Importantly, *no application data or replog entries ever pass through the Metastore*. Its use in replication is limited to:

*   Leadership election.
*   Tracking the ISR, OSR, and leader epoch for each task group.
*   Coordinating leader resignation.

== Replication Telemetry in the Cluster UI
The Cluster UI provides several metrics to monitor the health of replication, available at both the module and worker levels.

*   *replog entries*: The average and maximum size of replogs or the number of unapplied entries.
*   *replicate! duration*: The time from creating a replog entry until it is applied by the leader.
*   *replicate! throughput*: The rate of replication operations.
*   *replicate! failures*: A time-series of replication failures. A healthy cluster should show zero.
*   *apply duration*: The time it takes to apply a replog entry's changes after it's committed.
*   *ISR count*: The size of the smallest ISR set across all relevant task groups, compared to the configured `min-ISR`.= REST API =

Rama provides a built-in REST API for depot appends, PState queries, and query topology invokes using HTTP POST requests. The API uses JSON for request bodies and responses, enabling interaction from any programming language or tools like `curl`.

.Example `curl` requests
[source, text]
----
# Append to a depot
>> curl -X POST -H "Content-Type: text/plain" 'http://1.2.3.4:2000/rest/com.mycompany.MyModule/depot/*registerDepot/append' -d '{"data": {"userid":"alice"}}'
{"profileTopology":"success"}

# Query a PState
>> curl -X POST -H "Content-Type: text/plain" 'http://1.2.3.4:2000/rest/com.mycompany.MyModule/pstate/$$profiles/selectOne' -d '["alice", "location"]'
"New York City, NY"

# Invoke a query topology
>> curl -X POST -H "Content-Type: text/plain" 'http://1.2.3.4:2000/rest/com.mycompany.MyModule/query/usersWithAge/invoke' -d '[31]'
["cagney", "davis", "tracy", "lemmon"]
----

Requests are handled by xref:terminology.adoc#_supervisor[Supervisor] daemons. The xref:terminology.adoc#_conductor[Conductor] (default port 8888) also exposes the API but redirects requests to the appropriate Supervisor. Use the Conductor for initial service discovery, not for high-throughput traffic.

.Example `curl` request to the Conductor
[source, text]
----
>> curl -L -X POST -H "Content-Type: text/plain" 'http://1.1.1.1:8888/rest/com.mycompany.MyModule/pstate/$$profiles/selectOne' -d '["alice", "location"]'
"New York City, NY"
----
The `-L` flag tells `curl` to follow redirects.

== REST API Structure ==

A REST request for a module is handled only by a Supervisor running a worker for that module. Requests to the Conductor or an incorrect Supervisor will receive a 308 redirect.

A 308 redirect includes these headers:
[source, text]
----
"Location": "http://1.2.3.4:2000/rest/com.mycompany.MyModule/depot/*registerDepot/append"
"Supervisor-Locations": '["1.2.3.4:2000", "1.2.3.5:2000", "1.2.3.6:3000"]'
----

Programmatic clients should use the custom `Supervisor-Locations` header to get a JSON-encoded list of all active Supervisors for a module. Cache this list and distribute subsequent requests randomly across these Supervisors to balance the load.

If a request fails (e.g., 500 status or another redirect), try another Supervisor from the cached list or query the Conductor again to get an updated list.

NOTE: URL-encode any `/` characters in module names as `%2F`.

== JSON-Encoding Arguments ==

Since JSON has limited native types, Rama uses a special string format for others like longs, floats, and function references: `"#__<type character><value>"`. This encoding is parsed automatically, even in nested data structures.

By default, numbers are parsed as 32-bit integers, and floating-point values as 64-bit doubles.

.Example of encoding a list with a long, a character, and a byte
[source, text]
----
'["#__L123456789", "#__Ca", "#__B19"]'
----

.Special Type Encodings
[cols="1,2,2"]
|====
| Type Char | Parsed Type | Example
| B | Byte (8-bit) | `"#__B19"`
| S | Short (16-bit) | `"#__S5190"`
| L | Long (64-bit) | `"#__L123456789"`
| F | Float (32-bit) | `"#__F123.456"`
| C | Character | `"#__CQ"`
| K | Clojure keyword | `"#__Ka"`
| f | Function | `"#__fcom.mycompany.ops.MyRamaFunction"`
|====

Function references can point to a `RamaFunction` implementation, a fully-qualified Clojure function, or a built-in function from the `Ops` class (e.g., `"#__fOps.IS_EVEN"`).

NOTE: This special encoding is used only for requests. Responses are standard JSON.

== Depot Appends ==

Post to `\http://<host>:<port>/rest/<module>/depot/<depot>/append`.

*Request Body:* A JSON map with a required `"data"` key and an optional `"ackLevel"` key (`"ack"`, `"appendAck"`, or `"none"`). Defaults to `"ack"`.
*Response:* JSON-encoded xref:depots.adoc#_streaming_ack_returns[streaming ack returns].

.Example depot appends
[source, text]
----
>> curl -X POST -H "Content-Type: text/plain" 'http://1.2.3.4:2000/rest/com.mycompany.MyModule/depot/*depot/append' -d '{"data": {"userid":"alice"}}'
{"profileTopology":"success"}

>> curl -X POST -H "Content-Type: text/plain" 'http://1.2.3.4:2000/rest/com.mycompany.MyModule/depot/*depot/append' -d '{"data": {"userid":"alice"}, "ackLevel": "none"}'
{}
----

== PState Queries ==

Post to `\http://<host>:<port>/rest/<module>/pstate/<pstate>/<method>`, where `<method>` is one of:
* `select`: Returns a list of navigated values.
* `selectOne`: Returns a single navigated value. Errors if zero or more than one value is selected.

Paths are represented as a JSON list of navigators. A navigator can be _implicit_ or _explicit_.

*Implicit Navigators*:: Strings, keywords, and functions. Strings/keywords become `key` navigators; functions become `filterPred` navigators.
+
[source, text]
----
["a", "b", "#__fOps.IS_EVEN"] // Path.key("a").key("b").filterPred(Ops.IS_EVEN)
----

*Explicit Navigators*:: A list of `[<operation string>, <args>...]`. The operation determines how arguments are interpreted (e.g., as values or as nested paths).
+
[source, text]
----
["must", "a", "b"]
["filterSelected", "a", ["all"], "#__fOps.IS_EVEN"]
----

To route a query to a specific partition, start the path with `[["pkey", "<partitioning-key>"], ...]`.

=== Navigator Reference ===
The following tables list all available navigators. Both Java and Clojure API names are accepted. The "Arguments" column describes how arguments are interpreted (`Value`, `Path`, etc.).

==== Map Navigators
[cols="1,2,2,2"]
|====
| Names | Arguments | Example | Note
| `all` / `ALL` | None | `["all"]` |
| `key` / `keypath` | Value... | `["key", "a", "b"]` |
| `mapKey` / `map-key` | Value | `["mapKey", "k1"]` |
| `mapKeys` / `MAP-KEYS` | None | `["mapKeys"]` |
| `mapVals` / `MAP-VALS` | None | `["mapVals"]` |
| `must` | Value... | `["must", "k1", "k2"]` |
| `sortedMapRange` / `sorted-map-range` | Value, Value, ?Value | `["sortedMapRange", 1, 9, {"inclusive-start?": false}]` | Optional map for `inclusive-start?` / `inclusive-end?`
| `sortedMapRangeFrom` / `sorted-map-range-from` | Value, ?Value | `["sortedMapRangeFrom", "k", 100]` | Optional arg is max amount or options map
| `sortedMapRangeFromStart` / `sorted-map-range-from-start` | Value | `["sortedMapRangeFromStart", 100]` |
| `sortedMapRangeTo` / `sorted-map-range-to` | Value, ?Value | `["sortedMapRangeTo", "s", {"max-amt": 10}]` | Optional arg is max amount or options map
| `subMap` / `submap` | Value... | `["submap", "k1", "k2"]` |
|====

==== List Navigators
[cols="1,2,2"]
|====
| Names | Arguments | Example
| `all` / `ALL` | None | `["all"]`
| `afterElem` / `AFTER-ELEM` | None | `["afterElem"]`
| `beforeElem` / `BEFORE-ELEM` | None | `["beforeElem"]`
| `beforeIndex` / `before-index` | Value | `["beforeIndex", 3]`
| `beginning` / `BEGINNING` | None | `["beginning"]`
| `end` / `END` |  None | `["end"]`
| `filteredList` / `filterer` | Path... | `["filteredList", ["must", "a"]]`
| `first` / `FIRST` | None | `["first"]`
| `index` / `index-nav` | Value | `["index", 7]`
| `indexedVals` / `INDEXED-VALS` | None | `["indexedVals"]`
| `last` / `LAST` | None | `["last"]`
| `nth` / `nthpath` | Value... | `["nth", 4, 0, 2]`
| `sublist` / `srange` | Value, Value | `["sublist", 1, 8]`
| `sublistDynamic` / `srange-dynamic` | Value, Value | `["sublistDynamic", "#__fFn1", "#__fFn2"]`
|====

==== Set Navigators
[cols="1,2,2,2"]
|====
| Names | Arguments | Example | Note
| `all` / `ALL` | None | `["all"]` |
| `setElem` / `set-elem` | Value | `["setElem", "e1"]` |
| `sortedSetRange` / `sorted-set-range` | Value, Value, ?Value | `["sortedSetRange", "a", "z", {"inclusive-end?": true}]` | Optional map for `inclusive-start?` / `inclusive-end?`
| `sortedSetRangeFrom` / `sorted-set-range-from` | Value, ?Value | `["sortedSetRangeFrom", "e", 10]` | Optional arg is max amount or options map
| `sortedSetRangeFromStart` / `sorted-set-range-from-start` | Value | `["sortedSetRangeFromStart", 10]` |
| `sortedSetRangeTo` / `sorted-set-range-to` | Value, ?Value | `["sortedSetRangeTo", "q", {"max-amt": 10}]` | Optional arg is max amount or options map
| `subset` | Value... | `["subset", "e1", "e3"]` |
| `voidSetElem` / `NONE-ELEM` | None | `["voidSetElem"]` |
|====

==== Filter Navigators
[cols="1,2,2"]
|====
| Names | Arguments | Example
| `filterEqual` / `pred=` | Value | `["filterEqual", 9]`
| `filterGreaterThan` / `pred>` | Value | `["filterGreaterThan", 9]`
| `filterGreaterThanOrEqual` / `pred>=` | Value | `["filterGreaterThanOrEqual", 9]`
| `filterLessThan` / `pred<` | Value | `["filterLessThan", 9]`
| `filterLessThanOrEqual` / `pred<=` | Value | `["filterLessThanOrEqual", 9]`
| `filterNotEqual` / `prednot=` | Value | `["filterNotEqual", 9]`
| `filterPred` / `pred` | Value | `["filterPred", "#__fOps.IS_ODD"]`
| `filterSelected` / `selected?` | Path... | `["filterSelected", ["all"], ["filterEqual", 2]]`
| `filterNotSelected` / `not-selected?` | Path... | `["filterNotSelected", ["all"], ["filterEqual", 2]]`
|====

==== View Navigators
[cols="1,2,2"]
|====
| Names | Arguments | Example
| `nullToList` / `NIL->VECTOR` | None | `["nullToList"]`
| `nullToSet` / `NIL->SET` | None | `["nullToSet"]`
| `nullToVal` / `nil->val` | Value | `["nullToVal", 0]`
| `transformed` / `multi-transformed` | Path... | `["transformed", ["all"], ["termVal", "v"]]`
| `view` | Value... | `["view", "#__fOps.PLUS", 2]`
|====

These navigators are used in paths passed to `transformed`:

[cols="1,2,2"]
|====
| Names | Arguments | Example
| `term` | Value | `["term", "#__fOps.INC"]`
| `termVal` / `termval` | Value | `["termVal", 100]`
| `termVoid` / `NONE>` | None | `["termVoid"]`
|====

==== Control Navigators
[cols="1,2,2,2"]
|====
| Names | Arguments | Example | Note
| `ifPath` / `if-path` | Path, Path, ?Path | `["ifPath", ["a"], "b", "c"]` | Last argument is optional "else" path
| `multiPath` / `multi-path` | Paths... | `["multiPath", ["a"], ["b"]]` |
| `stay` / `STAY` | None | `["stay"]` |
| `stop` / `STOP` | None | `["stop"]` |
| `subselect` | Path... | `["subselect", ["ALL"], "a"]` |
| `withPageSize` / `with-page-size` | Value, Path | `["withPageSize", 3, [["mapVals"]]]` |
|====

==== Value Collection Navigators
[cols="1,2,2"]
|====
| Names | Arguments | Example
| `collect` | Path... | `["collect", ["ALL"], "a"]`
| `collectOne` / `collect-one` | Path... | `["collectOne", "k"]`
| `dispenseCollected` / `DISPENSE` | None | `["dispenseCollected"]`
| `isCollected` | Value | `["isCollected", "#__fMyPredicate"]`
| `putCollected` / `putVal` | Value | `["putCollected", "foo"]`
|====

== Query Topology Invokes ==

Post to `\http://<host>:<port>/rest/<module>/query/<topology>/invoke`.

*Request Body:* A JSON-encoded list of arguments for the query topology.
*Response:* The JSON-encoded result of the topology invoke.

.Example query topology invoke
[source, text]
----
>> curl -X POST -H "Content-Type: text/plain" 'http://1.2.3.4:2000/rest/com.mycompany.MyModule/query/usersWithAge/invoke' -d '[31]'
["cagney", "davis", "tracy", "lemmon"]
----

== Limitations of REST API ==

The REST API has a few limitations compared to the native Java and Clojure clients:
* Can only use types representable in Rama's JSON format.
* Custom PState navigators cannot be referenced.
* PState reactivity is not exposed.= Custom Serialization in Rama

Rama requires serialization for custom object types used in depots, ETLs, or PStates. This is necessary for disk storage and network transfer.

Rama has built-in support for:
* Java primitive types (`int`, `long`, `String`, etc.)
* Common `java.util` classes
* link:https://clojure.org/reference/data_structures[Clojure's immutable data structures]

For custom types, you have two options: the `RamaSerializable` interface for simplicity, and the `RamaCustomSerialization` interface for production use.

== RamaSerializable Interface (For Prototypes & Tests)

The `RamaSerializable` interface is a convenient way to make a Java type usable in Rama. Simply implement the interface, and Rama will handle serialization.

[source,java]
----
import com.rpl.rama.RamaSerializable;

public class LocationUpdate implements RamaSerializable {
  private long userId;
  private String location;
  // Constructors, getters...
}
----

This approach uses standard link:https://docs.oracle.com/javase/8/docs/technotes/guides/serialization/index.html[Java serialization], which has significant drawbacks:

* *Poor Performance:* Can produce a large number of bytes compared to other frameworks.
* *Difficult Schema Evolution:* Adding or removing fields can break deserialization of old data stored in PStates.

Due to these limitations, `RamaSerializable` is recommended only for testing and experimentation.

== RamaCustomSerialization Interface (For Production)

For production systems, the link:https://redplanetlabs.com/javadoc/com/rpl/rama/RamaCustomSerialization.html[`RamaCustomSerialization`] interface is the recommended approach. It decouples the serialization logic from your data types, allowing you to integrate high-performance frameworks like link:https://thrift.apache.org/[Apache Thrift] or link:https://developers.google.com/protocol-buffers[Protocol Buffers].

An implementation of `RamaCustomSerialization` provides three methods:
* `serialize(T obj, DataOutput out)`: Writes an object to a byte stream.
* `deserialize(DataInput in)`: Reads an object from a byte stream.
* `targetType()`: Returns the base class the serializer handles (e.g., `TBase.class` for Thrift).

=== Example: Thrift Serialization

This example shows a generic serializer for any Thrift object. It uses a common pattern of writing a unique *type ID* before the object's data, so the deserializer knows which class to instantiate.

[source,java]
----
// Abstract base class for Thrift serialization
public abstract class ThriftSerialization implements RamaCustomSerialization<TBase> {
  private final Map<Short, Class> _idToType = new HashMap<>();
  private final Map<Class, Short> _typeToId = new HashMap<>();

  // Populates maps from a user-defined typeId map
  protected ThriftSerialization() { /* ... */ }

  @Override
  public void serialize(TBase obj, DataOutput out) throws Exception {
    Short id = _typeToId.get(obj.getClass());
    // ... error handling ...
    out.writeShort(id);
    byte[] serialized = new TSerializer(/* ... */).serialize(obj);
    out.writeInt(serialized.length);
    out.write(serialized);
  }

  @Override
  public TBase deserialize(DataInput in) throws Exception {
    TBase obj = (TBase) _idToType.get(in.readShort()).newInstance();
    byte[] arr = new byte[in.readInt()];
    in.readFully(arr);
    new TDeserializer(/* ... */).deserialize(obj, arr);
    return obj;
  }

  @Override
  public Class targetType() { return TBase.class; }

  // Must be implemented by a concrete class
  protected abstract Map<Integer, Class> typeIds();
}
----

An application would extend this class to provide its specific Thrift types and their IDs.

[source,java]
----
public class MyApplicationThriftSerialization extends ThriftSerialization {
  @Override
  protected Map<Integer, Class> typeIds() {
    Map<Integer, Class> ret = new HashMap<>();
    ret.put(0, ProfileEdit.class);
    ret.put(1, PageView.class);
    ret.put(2, Post.class);
    return ret;
  }
}
----

=== Registering Custom Serializers

You must register your `RamaCustomSerialization` implementations with all modules and clients that use the custom types.

.In Production (Module Deployment & Clients)
Register serializers via the `"custom.serializations"` configuration key, which takes a list of fully-qualified class names. The classes must have a zero-argument constructor.

.YAML Config (`overrides.yaml`)
[source,text]
----
custom.serializations:
  - "com.rpl.myapp.serialization.MyApplicationThriftSerialization"
----

.Client-side `RamaClusterManager`
[source,java]
----
Map<String, Object> config = new HashMap<>();
config.put("custom.serializations",
  Arrays.asList("com.rpl.myapp.serialization.MyApplicationThriftSerialization"));
RamaClusterManager manager = RamaClusterManager.open(config);
----

.In Test Environments (`InProcessCluster`)
For testing, pass the serialization classes directly to the `InProcessCluster` constructor.

[source,java]
----
try(InProcessCluster ipc = InProcessCluster.create(
      Arrays.asList(MyApplicationThriftSerialization.class))) {
  // ... launch modules
}
----

=== Implementation Requirements

* *Deterministic:* Equal objects must always serialize to the same bytes.
* *Thread-Safe:* A single instance may be used by multiple threads.
* *Non-Overlapping:* Do not register multiple serializers that can handle the same type or its parent.

== Requirements for Map Keys and Set Values

When using custom types as keys in maps or values in sets within PStates, two additional rules apply because Rama uses the serialized form for on-disk lookups and sorting.

. *Consistent Serialization:* Equal objects *must* serialize to the exact same byte sequence. For types with undefined internal order (like a `HashMap`), you must enforce a consistent order during serialization (e.g., by sorting elements).
. *Order Preservation:* For range queries to work correctly, the link:https://en.wikipedia.org/wiki/Lexicographic_order[lexicographical order] of the serialized bytes must match the natural order of the objects.

Rama's built-in serializers handle these properties correctly for types like numbers, strings, and collections.= Stream Topologies =

Stream topologies reactively process data from depots with millisecond-level latency. They are ideal for use cases requiring fast updates to PStates and for coordinating work with front-end clients.

All examples can be found in the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== Usage ==

Stream topologies consume data from any number of depots to update any number of PStates. They have access to the full dataflow API, except for batch blocks. This allows for complex logic using conditionals, loops, and branching.

[source,java]
----
public class BasicStreamTopologyModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*depot", Depot.hashBy(Ops.FIRST));
    setup.declareDepot("*depot2", Depot.random());

    StreamTopology s = topologies.stream("s");
    s.pstate("$$p1", PState.mapSchema(String.class, Long.class));
    s.pstate(
      "$$p2",
      PState.mapSchema(
        String.class,
        PState.mapSchema(String.class, Long.class).subindexed()));

    s.source("*depot").out("*tuple")
     .each(Ops.EXPAND, "*tuple").out("*k1", "*k2")
     .compoundAgg("$$p1", CompoundAgg.map("*k1", Agg.count()))
     .compoundAgg("$$p2", CompoundAgg.map("*k1", CompoundAgg.map("*k2", Agg.count())))
     .ifTrue(new Expr(Ops.NOT_EQUAL, "*k1", "*k2"),
       Block.hashPartition("*k2")
            .compoundAgg("$$p1", CompoundAgg.map("*k2", Agg.count()))
            .compoundAgg("$$p2", CompoundAgg.map("*k2", CompoundAgg.map("*k1", Agg.count()))));

    s.source("*depot2").out("*v")
     .each(Ops.CURRENT_TASK_ID).out("*taskId")
     .each(Ops.PRINTLN, "From *depot2:", "*taskId", "*v");
  }

  // main method omitted for brevity
}
----

Running this code with sample appends produces:
[source,text]
----
a count: 2
b count: 2
c count: 1
b subcounts: [["a" 1] ["c" 1]]
From *depot2: 3 X
From *depot2: 0 Y
----

.Key Concepts from Usage
* **No Batch Blocks**: Stream topologies cannot use batch blocks, which prevents them from using xref:aggregators.adoc#_high_performance_two_phase_aggregation_with_combiners[two-phase aggregation]. For efficient global aggregation, prefer xref:microbatch.adoc[microbatch topologies].
* **Depot Integration**: Depot appends can be configured to block until colocated stream topologies have finished processing the data. See xref:depots.adoc[depots] for different "ack levels".

== Operation ==

When a depot partition receives a record, it pushes it to all subscribed stream topologies. The processing of a single record can trigger a dynamic "event tree" of downstream work across many tasks. The depot record is considered processed only when the entire event tree completes.

image::./diagrams/stream/event-tree.png[]

.Key Operational Details
* **Event-level Visibility**: PState writes from an event are made visible externally only after the event completes and its writes are replicated. This differs from microbatch topologies, where all writes for a microbatch become visible at once.
* **Auto-Batching**: To optimize performance, Rama batches multiple pending stream topology events on a task. All PState writes within a batch are buffered and become visible externally at the same time, providing a balance of low latency and high throughput.
* **Progress Tracking**: Stream topologies track their progress per depot partition in an internal PState (e.g., `"\$$\__streaming-state-s"`). This state is checkpointed periodically, not after every record, which has implications for fault-tolerance.

== Fault-tolerance and Retry Modes ==

If an event fails for any reason (e.g., code exception, machine failure, timeout), the originating depot record is retried according to the source's configured "retry mode". This provides "at least once processing" semantics.

IMPORTANT: Because records can be processed more than once upon retry, non-idempotent operations like incrementing a counter can become inaccurate. For use cases requiring perfect accuracy with such operations, use a xref:microbatch.adoc[microbatch topology].

Retry modes are configured per source:
[source,java]
----
s.source("*depot", StreamSourceOptions.retryNone());
s.source("*depot2", StreamSourceOptions.retryAllAfter());
----

.Available Retry Modes
* `individual` (Default): Only the failed depot record is retried. Order is not guaranteed on retry.
* `none`: Failed records are never retried ("at most once" semantics).
* `all after`: The failed record and all subsequent records from the same depot partition are retried, preserving order.

.Failure Scenarios
* **Timeouts**: A record fails if its event tree does not complete within a configured timeout.
* **Checkpoint Lag**: If a leader fails after processing records but before checkpointing, those records will be retried by the new leader.
* **Batch Failure**: If any event in a processing batch fails, the entire batch of events is discarded and retried. All PState writes from that batch are rolled back.

== Initial Processing Position ==

The first time a stream topology runs on a depot partition, you can configure where it starts processing. By default, it starts from the end (processing only new records).

[source,java]
----
s.source("*depot", StreamSourceOptions.startFromBeginning());
s.source("*depot2", StreamSourceOptions.startFromOffsetAfterTimestamp(107740800000));
s.source("*depot3", StreamSourceOptions.startFromOffsetAgo(10000, OffsetAgo.RECORDS));
s.source("*depot4", StreamSourceOptions.startFromOffsetAgo(15, OffsetAgo.DAYS));

// Options can be chained
s.source("*depot", StreamSourceOptions.startFromBeginning().retryNone());
----

NOTE: "Start from" options only apply the very first time a topology processes a depot partition. On subsequent runs (after updates or leader switches), it resumes from its last checkpointed position.

== Ack Return Aggregation ==

When using `AckLevel.ACK`, depot appends can receive a value back from colocated stream topologies. This is achieved with the `ackReturn()` method. By default, if multiple `ackReturn()` calls occur for a single depot record, the last value is returned.

[source,java]
----
// Returns the new count after each append
s.source("*depot").out("*k")
 .compoundAgg("$$p", CompoundAgg.map("*k", Agg.count()))
 .localSelect("$$p", Path.key("*k")).out("*v")
 .ackReturn("*v");
----

The aggregation logic can be customized using xref:aggregators.adoc[aggregators].

[source,java]
----
// Aggregates all ackReturn values by summing them
s.source("*depot", StreamSourceOptions.ackReturnAgg(Agg::sum)).out("*v")
 .each(Ops.RANGE, 0, "*v")
 .shufflePartition()
 .ackReturn("*v");
----

A custom aggregator can be provided via `StreamSourceOptions.ackReturnAgg(new MyAggregator())`.

== Throttling ==

To prevent being overloaded, each task enforces a limit on the number of depot records that can be simultaneously processing for a stream topology. When this per-task limit is hit:
* The depot partition pauses sending new records to that task.
* Depot appends with `AckLevel.ACK` will throw an exception.
* Records for sources with `retryNone` will be skipped entirely.

You should scale your modules so this limit is not normally hit. It is a safeguard against unexpected traffic bursts.

== Tuning Options ==

Stream topology behavior can be fine-tuned via configurations and dynamic options.

=== Configurations (Fixed on Deploy) ===
* `topology.stream.periodic.checkpoint.seconds`: How often to checkpoint progress, ensuring progress is saved for low-throughput depots.

=== Dynamic Options (Editable in UI) ===
* `topology.stream.checkpoint.progress.threshold`: Checkpoint after a certain number of records have been processed.
* `topology.stream.max.events.per.batch`: Max number of events to process in a single execution batch on a task.
* `topology.stream.max.executing.per.task`: The per-task throttling limit for concurrently processing depot records.
* `topology.stream.timeout.seconds`: Timeout for an event tree to complete.
* `depot.cache.cardinality`: Size of the depot cache used for retries.
* `depot.cache.catchup.chunk.size`: Number of records to fetch at a time when a topology is catching up.
* `depot.max.pending.streaming.per.partition`: How many acked depot appends can be tracked per depot partition.
* `depot.ack.failure.on.any.streaming.failure`: If `true` (default), an acked append fails immediately on the first stream processing failure. If `false`, it waits for retries to potentially succeed.This page is a quick reference for core Rama terminology.

== Cluster ==
A collection of machines running a Rama application (a "module"). A cluster consists of three daemon types:
* *Conductor:* One per cluster.
* *Metastore:* One or more per cluster.
* *Supervisor:* One or more per cluster.

For testing, an `xref:testing.adoc[InProcessCluster]` can simulate a cluster within a single process using threads.

== Conductor ==
Orchestrates module deployment, updates, and scaling. It serves the Cluster UI for monitoring module status and telemetry. The Conductor does not handle data processing or PState queries.

== Metastore ==
Stores cluster metadata, such as worker assignments, replication info, and `xref:operating-rama.adoc#_worker_configurations_and_dynamic_options[dynamic options]`. It is backed by a link:https://zookeeper.apache.org/[Zookeeper cluster].

== Supervisor ==
A process running on each worker node. It watches the Metastore and manages the lifecycle (start/stop) of worker processes on its machine.

== Worker ==
A process launched by a Supervisor to execute part of a module. A worker runs many "tasks" within "task threads".

== Module ==
A Rama application, comprising depots, PStates, and topologies. Modules can be updated with new code or scaled to change resource allocation.

== Module instance ==
A specific deployment of a module with a particular version of code. During updates or scaling, multiple instances of the same module can exist temporarily.

== Task ==
A logical unit of a module that contains partitions for depots and PStates. All tasks run the same module code. The number of tasks in a module must be a power of two. See the xref:tutorial3.adoc#task-model[Task model].

== Task instance ==
A single replica of a task. Computation, called an "event", runs on a task instance.

== Task group / task thread ==
A "task thread" executes a "task group", which is a collection of task instances. Replication occurs at the task group level.

== Category queue ==
A per-task-thread queue system that balances CPU time between different event categories. The proportions are configurable via the `worker.event.category.proportions` dynamic option. Categories include:
* *isr.replication:* Critical replication events between replicas.
* *client:* Depot appends, PState queries, and query topologies.
* *low.latency:* Stream topology events.
* *flex.latency:* Microbatch topology events.
* *mirror.client:* Operations from other modules.

== Depot ==
A `xref:depots.adoc[depot]` is a distributed, replicated, unindexed log of data consumed by ETL topologies.

== PState ==
A `xref:pstates.adoc[PState]` is a distributed, durable, replicated datastore of arbitrary structure. It is manipulated using `xref:paths.adoc[Paths]`.

== Topology ==
A `xref:tutorial4.adoc[dataflow-based]` distributed computation.
* *ETL Topologies:* Consume data from depots to build PStates. Can be `xref:stream.adoc[stream]` or `xref:microbatch.adoc[microbatch]`.
* *Query Topologies:* On-demand, realtime computations that read from PStates to produce results. See `xref:query.adoc[query topologies]`.

== Partition ==
A subdivision of a depot or PState hosted on a task. Partitions can be:
* *Global:* A single partition on task 0.
* *Non-global:* A partition on every task in the module.

== Leaders and followers ==
Each task group has multiple replicas. The "leader" manages all computation (ETL, queries, appends). "Followers" stay in sync with the leader. If a leader fails, a follower is automatically promoted.

== Replication factor ==
A module configuration that sets the number of replicas for each task group. A production-recommended value is three (one leader, two followers).

== Replog ==
Short for "replication log". An internal log on each task thread containing all changes to depots and PStates. Leaders use it to keep followers synchronized.

== ISR ==
"In-sync replicas". Replicas that are fully caught up with their task group's leader. Changes are acknowledged to clients only after all ISR members have the data, ensuring durability.

== OSR ==
"Out-of-sync replicas". Replicas that have fallen behind the leader and are ineligible for promotion until they catch up.

== Min-ISR ==
A dynamic option (`replication.min.isr`) specifying the minimum number of replicas (including the leader) that must persist a change for it to succeed. This prevents data loss if too many replicas fail. See `xref:replication.adoc[replication]`.

== Mirror ==
A `xref:module-dependencies.adoc[mirror]` is a reference in one module to a depot, PState, or query topology belonging to another module.

== Foreign context ==
Work initiated from outside a module, such as a client appending to a depot or querying a PState.

== WORP ==
"Worker Rama Protocol". The internal messaging layer for all communication with and between task threads, used by both workers and external clients.

== WEFT ==
"Worker Efficient File Transfer". An internal, HTTP-based protocol for transferring files between machines. Used for bulk data synchronization, such as when a follower is catching up or during scaling.= Testing

Rama's integrated design simplifies testing. `InProcessCluster` simulates a full Rama cluster in a single process, allowing for end-to-end testing of modules without complex external dependencies.

This page covers:
* Using `InProcessCluster` for end-to-end module tests.
* Testing topologies with mirror depots and microbatches.
* Unit testing custom operations with `MockOutputCollector`.
* Unit testing dataflow logic with `TestPState`.

All examples are from the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== Using InProcessCluster

`InProcessCluster` provides the same API for launching, updating, and destroying modules as a real cluster, with identical execution semantics. The only difference is that modules on an `InProcessCluster` run with a replication factor of one.

The following example demonstrates testing a simple module with JUnit.

[source, java]
----
public class SimpleInProcessClusterExample {
  public static class SimpleModule implements RamaModule {
    @Override
    public void define(Setup setup, Topologies topologies) {
      setup.declareDepot("*depot", Depot.hashBy(Ops.IDENTITY));

      StreamTopology s = topologies.stream("counter");
      s.pstate("$$counts", PState.mapSchema(String.class, Long.class));
      s.source("*depot").out("*k")
       .compoundAgg("$$counts", CompoundAgg.map("*k", Agg.count()));
    }
  }

  @Test
  public void simpleTest() throws Exception {
    try(InProcessCluster cluster = InProcessCluster.create()) {
      cluster.launchModule(new SimpleModule(), new LaunchConfig(4, 2));

      String moduleName = SimpleModule.class.getName();
      Depot depot = cluster.clusterDepot(moduleName, "*depot");
      PState counts = cluster.clusterPState(moduleName, "$$counts");

      depot.append("cagney");
      depot.append("davis");
      depot.append("cagney");

      assertEquals(2, (Long) counts.selectOne(Path.key("cagney")));
      assertEquals(1, (Long) counts.selectOne(Path.key("davis")));
      assertNull(counts.selectOne(Path.key("garbo")));
    }
  }
}
----

Key points for using `InProcessCluster`:

*   **Resource Management:** An `InProcessCluster` starts multiple threads. Always use a `try-with-resources` block or manually call `close()` to ensure cleanup.
*   **Creation:** Use `InProcessCluster.create()`. An overload exists for registering xref:serialization.adoc[custom serializations].
*   **Launching Modules:** Use `launchModule` with a module instance and a `LaunchConfig`. `LaunchConfig` specifies the parallelism (e.g., tasks and threads). You can also specify the number of workers:
[source, java]
----
cluster.launchModule(new SimpleModule(), new LaunchConfig(4, 2).numWorkers(2));
----
Running multiple workers is useful for exercising custom serializations, as Rama only serializes data between workers.
*   **Testing Flow:** The typical test flow is to append data to depots and then query PStates to assert the expected state. For stream topologies, appends with `AckLevel.ACK` are synchronous, so assertions can be made immediately.

=== Testing Microbatch Topologies

Microbatch processing is asynchronous to depot appends. `InProcessCluster` provides helpers to manage this.

*   **`waitForMicrobatchProcessedCount`**: This method blocks until a topology has processed a specific total number of records.

[source, java]
----
public class MicrobatchTestingExample {
  // ... module definition ...
  @Test
  public void microbatchTest() throws Exception {
    try(InProcessCluster cluster = InProcessCluster.create()) {
      cluster.launchModule(new MicrobatchModule(), new LaunchConfig(4, 2));
      // ... get depot and PState ...

      depot.append("cagney");
      depot.append("davis");
      depot.append("cagney");

      // Wait for 3 total records to be processed
      cluster.waitForMicrobatchProcessedCount(moduleName, "counter", 3);
      assertEquals(2, (Long) counts.selectOne(Path.key("cagney")));

      depot.append("cagney");

      // Wait for 4 total records to be processed
      cluster.waitForMicrobatchProcessedCount(moduleName, "counter", 4);
      assertEquals(3, (Long) counts.selectOne(Path.key("cagney")));
    }
  }
}
----
The count provided to `waitForMicrobatchProcessedCount` is the *total* number of records processed by the topology, not an incremental amount.

*   **`pauseMicrobatchTopology` / `resumeMicrobatchTopology`**: To test logic that depends on the specific composition of a microbatch (e.g., xref:intermediate-dataflow.adoc#_batch_blocks[batch blocks]), you can pause processing, append a set of records, and then resume. This guarantees the appended records will be processed together in the next microbatch.

[source, java]
----
cluster.pauseMicrobatchTopology(moduleName, "counter");
depot.append("a");
depot.append("b");
cluster.resumeMicrobatchTopology(moduleName, "counter");
----

=== Testing Stream Topologies with Mirror Depots

Processing data from a mirror depot is asynchronous. When a module consumes a depot from another module, `AckLevel.ACK` on the append only confirms receipt by the source module, not completion by the downstream consumer.

The solution is to poll the downstream PState until the expected condition is met, using a generous timeout to prevent flaky tests.

[source, java]
----
// Helper function to poll a condition
public void assertValueAttained(Object expected, RamaFunction0 f) throws Exception {
  long start = System.nanoTime();
  while(true) {
    Object val = f.invoke();
    if(expected.equals(val)) return;
    if(System.nanoTime() - start > 30_000_000_000L) { // 30s timeout
      throw new RuntimeException("Condition failed to attain " + expected + ", was " + val);
    }
    Thread.sleep(50);
  }
}

@Test
public void streamTopologyWithMirrorTest() throws Exception {
  try(InProcessCluster cluster = InProcessCluster.create()) {
    cluster.launchModule(new DepotModule(), new LaunchConfig(8, 2));
    cluster.launchModule(new CounterModule(), new LaunchConfig(4, 2));

    Depot depot = cluster.clusterDepot(DepotModule.class.getName(), "*depot");
    PState counts = cluster.clusterPState(CounterModule.class.getName(), "$$counts");

    depot.append("cagney");
    depot.append("davis");
    depot.append("cagney");

    assertValueAttained(2L, () -> counts.selectOne(Path.key("cagney")));
    assertValueAttained(1L, () -> counts.selectOne(Path.key("davis")));
  }
}
----

=== Module Update

`InProcessCluster` supports module updates. To update a module, both the old and new versions must return the same name from `getModuleName()`.

[source, java]
----
public class CounterModule_v1 implements RamaModule {
  // ... increments by 2 (bug) ...
  @Override public String getModuleName() { return "CounterModule"; }
}

public class CounterModule_v2 implements RamaModule {
  // ... increments by 1 (fix) ...
  @Override public String getModuleName() { return "CounterModule"; }
}

@Test
public void updateTest() throws Exception {
  try(InProcessCluster cluster = InProcessCluster.create()) {
    cluster.launchModule(new CounterModule_v1(), new LaunchConfig(4, 2));
    Depot depot = cluster.clusterDepot("CounterModule", "*depot");
    PState counts = cluster.clusterPState("CounterModule", "$$counts");

    depot.append("cagney");
    assertEquals(2, (Long) counts.selectOne(Path.key("cagney")));

    cluster.updateModule(new CounterModule_v2());

    depot.append("cagney");
    assertEquals(3, (Long) counts.selectOne(Path.key("cagney")));
  }
}
----
Existing depot and PState clients continue to work after an update, automatically redirecting to the new module version.

To perform a destructive update (removing depots or PStates), you must explicitly list the objects to be deleted. The list must be exact.

[source, java]
----
cluster.updateModule(new MyModule_v2(), UpdateOptions.objectsToDelete("*depot", "$$p"));
----

=== Module Destroy

Modules can be destroyed by name. This will fail if another module has a dependency on it.

[source, java]
----
cluster.destroyModule("com.mycompany.MyModule");
----

== Testing Modules with Circular Dependencies

Modules with circular dependencies can be deployed via a sequence of module updates. See xref:module-dependencies.adoc#_circular_dependencies[Circular Dependencies] for details on this pattern.

== Unit Testing a Custom RamaOperation with MockOutputCollector

To unit test a `RamaOperation` in isolation, use `MockOutputCollector`. It captures all emitted tuples for assertion.

[source, java]
----
public class MyOperation implements RamaOperation1<Integer> {
  @Override
  public void invoke(Integer n, OutputCollector collector) {
    collector.emitStream("somestream", 1, 2);
    for(int i=0; i < n; i++) collector.emit(i);
  }
}

@Test
public void mockOutputCollectorTest() {
  MockOutputCollector collector = new MockOutputCollector();
  new MyOperation().invoke(2, collector);

  // getEmits() returns all emits in chronological order
  List<CapturedEmit> emits = collector.getEmits();
  assertEquals(3, emits.size());
  assertEquals("somestream", emits.get(0).getStreamName());
  assertEquals(Arrays.asList(1, 2), emits.get(0).getValues());
  assertNull(emits.get(1).getStreamName()); // stream name is null for emit()
  assertEquals(Arrays.asList(0), emits.get(1).getValues());

  // getEmitsByStream() groups emits by stream name
  Map<String, List<List<Object>>> byStream = collector.getEmitsByStream();
  assertEquals(Arrays.asList(Arrays.asList(1, 2)), byStream.get("somestream"));
  assertEquals(Arrays.asList(Arrays.asList(0), Arrays.asList(1)), byStream.get(null));
}
----
Use `getEmits()` when the order of emits across different streams matters. Otherwise, `getEmitsByStream()` can be more convenient.

== TestPState

`TestPState` is a concrete PState implementation for testing dataflow logic, such as macros, outside of a module. It can be used with `localTransform` and `localSelect` or manipulated directly.

[source, java]
----
public static Block incrementKeyA(String pstateVar) {
  return Block.localTransform(pstateVar, Path.key("a").term(Ops.INC));
}

@Test
public void testPStateExampleTest() throws Exception {
  try(TestPState tp = TestPState.create(PState.mapSchema(String.class, Integer.class))) {
    // Direct manipulation
    tp.transform(Path.key("a").termVal(10));
    assertEquals(10, (int) tp.selectOne(Path.key("a")));

    // Use in dataflow code
    Block.each(Ops.IDENTITY, tp).out("$$p")
         .macro(incrementKeyA("$$p"))
         .execute();
    assertEquals(11, (int) tp.selectOne(Path.key("a")));
  }
}
----

== Summary

Rama provides comprehensive tools for unit testing modules and their components. While this page focuses on unit and integration testing, remember to also perform performance analysis on a development cluster. Performance tests can leverage Rama's xref:operating-rama.adoc#_activating_self_monitoring[self-monitoring] to determine throughput and resource requirements for production.= Building your first module: Hello, World

Rama is a distributed-first, scalable programming platform for building an application's entire data layer. It integrates data ingestion, processing, indexing, and querying into a single, unified system.

This tutorial introduces the basics of writing and running a Rama application by building a simple "Hello, World" example.

== Setting up your sandbox

First, clone the `rama-examples` project.
[source,shell]
----
git clone https://github.com/redplanetlabs/rama-examples.git
cd rama-examples
----

The project uses Maven. To run the examples, launch an interactive Groovy shell. You must compile the project first.

[source,shell]
----
mvn compile
mvn gplus:shell
----
Starting the shell may take 30-60 seconds as it loads Rama classes.

== Hello, World Example

The following `HelloWorldModule` defines a minimal Rama application.

[source,java]
----
package rama.examples.tutorial;

import com.rpl.rama.*;
import com.rpl.rama.module.*;
import com.rpl.rama.ops.Ops;
import com.rpl.rama.test.*;

public class HelloWorldModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*depot", Depot.random());
    StreamTopology s = topologies.stream("s");
    s.source("*depot").out("*data")
     .each(Ops.PRINTLN, "*data");
  }

  public static void main(String[] args) throws Exception {
    try (InProcessCluster cluster = InProcessCluster.create()) {
      cluster.launchModule(new HelloWorldModule(), new LaunchConfig(1, 1));

      String moduleName = HelloWorldModule.class.getName();
      Depot depot = cluster.clusterDepot(moduleName, "*depot");
      depot.append("Hello, world!!");
    }
  }
}
----

Run this code from the Groovy shell:
[source,shell]
----
groovy:000> rama.examples.tutorial.HelloWorldModule.main(null)
----

After a brief pause on the first run, you will see `Hello, world!!` printed to the console.

== The Big Picture: Event Sourcing

Rama's programming model is based on _event sourcing_ and _materialized views_. Instead of mutating data in place (like a traditional database), you append immutable facts to a log. From this log, you compute indexed "views" of your data to serve queries.

This separates the _source of truth_ (the log) from the _query layer_ (the views).

*   **Source of Truth**: An append-only, normalized log ensures data consistency and provides a natural audit trail. If a bug corrupts a view, it can be recomputed from the log.
*   **Materialized Views**: Views can be denormalized and structured specifically for your application's query patterns, ensuring high performance.

Rama integrates this process, allowing client appends to be coordinated with view updates. This makes it suitable for both synchronous, interactive use cases and asynchronous ones.

== Core Concepts

=== Clusters

A _cluster_ is Rama's execution environment. It is a group of networked machines (_nodes_) running processes with three distinct roles:

*   **Conductor**: Orchestrates launching and updating modules.
*   **Supervisor**: Manages worker processes on a single machine.
*   **Worker**: Executes the data processing logic defined in your modules.

image::./diagrams/module-diagrams/cluster-overview.svg[]

For development and testing, Rama provides an **in-process cluster (IPC)**. An IPC is a lightweight, virtual cluster that runs within a single JVM process, enabling a much faster feedback loop. The example uses `InProcessCluster.create()`.

=== Modules

A _module_ is Rama's executable unit. It's where you define your data schemas, processing logic, and queries. To create a module, you implement the `RamaModule` interface and its `define` method.

A module contains several key components:
*   **Depots**: Entry points for data into the module.
*   **ETLs**: Logic for extracting, transforming, and loading (processing) data.
*   **PStates**: Indexed, partitioned data structures (materialized views).
*   **Query Topologies**: Pre-defined, distributed queries.

=== Anatomy of the Example

Let's break down the `HelloWorldModule` code.

**1. The `main` method (Client-side logic)**

The `main` method simulates a client interacting with the Rama cluster. In a real application, this logic would live in your application's backend services, not inside the module definition.

[source,java]
----
// 1. Create a lightweight cluster for development.
try (InProcessCluster cluster = InProcessCluster.create()) {
  // 2. Deploy the module definition to the cluster with a resource configuration.
  cluster.launchModule(new HelloWorldModule(), new LaunchConfig(1, 1));

  String moduleName = HelloWorldModule.class.getName();
  // 3. Get a handle to the module's depot.
  Depot depot = cluster.clusterDepot(moduleName, "*depot");
  // 4. Append data to the depot, triggering the module's ETL.
  depot.append("Hello, world!!");
}
----

[NOTE]
====
Launching modules on a real cluster is done via Rama's xref:operating-rama.adoc[command-line client], which offers more configuration options. The concepts remain the same.
====

**2. The `define` method (Module Definition)**

The `define` method describes the module's static topology. This code runs when the module is launched on the cluster.

[source,java]
----
@Override
public void define(Setup setup, Topologies topologies) {
  // 1. Declare a depot named "*depot". Data will enter the module here.
  //    Strings starting with * are variables in Rama's APIs.
  setup.declareDepot("*depot", Depot.random());

  // 2. Define a stream topology, which processes data as it arrives.
  StreamTopology s = topologies.stream("s");
  s.source("*depot").out("*data")      // 3. Source data from "*depot" and bind it to the *data variable.
   .each(Ops.PRINTLN, "*data"); // 4. For each incoming datum, execute the PRINTLN operation.
}
----

When `depot.append()` is called in `main`, the string `"Hello, world!!"` enters `*depot`. The stream topology, which is sourcing that depot, immediately receives the data and executes `Ops.PRINTLN` on it.

=== PStates and Queries

Typically, an ETL's purpose is not just to print data, but to build and maintain indexes to answer application queries. In Rama, these indexes are called xref:pstates.adoc[PStates] (partitioned states).

You can query PStates directly for simple lookups or define complex, distributed xref:query.adoc[query topologies] for more advanced needs. You will learn about PStates and queries in the next tutorial.

== Summary

*   Rama applications run on **clusters**. For development, you use an **in-process cluster**.
*   **Modules** are Rama's executables, containing your data processing logic.
*   You **launch** a module on a cluster to run it.
*   Data enters a module through **depots** when a client **appends** to them.
*   **ETLs** (like stream topologies) **source** data from depots and perform operations, often to build and update **PStates** (indexes).= Doing work in Rama: Depots, ETLs, and PStates

This document introduces the core Rama constructs for building applications: depots for ingesting data, PStates for storing it, and ETLs for processing it. The fundamental workflow involves gathering data in depots, transforming it with ETLs, and storing the results in PStates for efficient retrieval.

== Storing results in PStates

Rama's construct for data storage is the xref:pstates.adoc[partitioned state] (PState). While PStates are named data containers like tables in an RDBMS, they are far more flexible. A PState is an _arbitrarily compound data structure of arbitrary size_. This allows you to store data in the exact shape you need to use it, avoiding the impedance mismatches common with other databases.

The following example implements a simple word count to demonstrate the basic concepts.

[source,java]
----
package rama.examples.tutorial;

import com.rpl.rama.*;
import com.rpl.rama.module.*;
import com.rpl.rama.test.*;

public class SimpleWordCountModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*wordDepot", Depot.random());

    StreamTopology s = topologies.stream("wordCountStream");
    s.pstate("$$wordCounts", PState.mapSchema(String.class, Long.class));

    s.source("*wordDepot").out("*token")
     .hashPartition("*token")
     .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
  }

  public static void main(String[] args) throws Exception {
    try (InProcessCluster cluster = InProcessCluster.create()) {
      cluster.launchModule(new SimpleWordCountModule(), new LaunchConfig(1, 1));
      String moduleName = SimpleWordCountModule.class.getName();
      Depot depot = cluster.clusterDepot(moduleName, "*wordDepot");
      depot.append("one");
      depot.append("two");
      depot.append("two");
      depot.append("three");
      depot.append("three");
      depot.append("three");

      PState wc = cluster.clusterPState(moduleName, "$$wordCounts");
      System.out.println("one: " + wc.selectOne(Path.key("one")));
      System.out.println("two: " + wc.selectOne(Path.key("two")));
      System.out.println("three: " + wc.selectOne(Path.key("three")));
    }
  }
}
----

=== Creating a partitioned state
You create a PState on a topology instance with `.pstate()`:

[source,java]
----
s.pstate("$$wordCounts", PState.mapSchema(String.class, Long.class));
----

A PState requires a name (e.g., `$$wordCounts`) and a schema. The schema defines the PState's data structure, like `PState.mapSchema(String.class, Long.class)` for a key-value store. PStates belong to a topology but must have names unique within the module. They are materialized views of depots, and only the owning topology can write to them.

=== Updating PStates
PStates are updated via ETLs that create a data pipeline from a depot. As new data is appended to the depot, the ETL processes it and updates the PState.

[width="80%",options="header"]
|====
| Time| Event| `$$wordCounts`
| 1| `append("one")`| `{"one": 1}`
| 2| `append("two")`| `{"one": 1, "two": 1}`
| 3| `append("two")`| `{"one": 1, "two": 2}`
|====

This is achieved with the following code:
[source,java]
----
s.source("*wordDepot").out("*token")
 .hashPartition("*token")
 .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
----

. `.source("*wordDepot").out("*token")`: Reads from `*wordDepot` and binds each entry to the `*token` variable.
. `.hashPartition("*token")`: Relocates the subsequent computation to a specific partition based on the value of `*token`, enabling distribution and scalability.
. `.compoundAgg("$$wordCounts", ...)`: Updates the PState. xref:aggregators.adoc[Aggregators] like `CompoundAgg.map("*token", Agg.count())` specify how to navigate the PState's structure and what update to perform (e.g., increment a counter at the key specified by `*token`).

=== Querying PStates
To query a PState from a client, get a reference to it and use methods like `selectOne()`. Queries use xref:paths.adoc[Path] objects to navigate the PState's data structure. Paths are powerful selectors, similar to XPath or CSS, that can traverse nested structures, filter, and apply transformations.

[source,java]
----
PState wc = cluster.clusterPState(moduleName, "$$wordCounts");
// Returns the count for the word "one"
wc.selectOne(Path.key("one"));
----

If a PState contained nested data like `{"Frankenstein": {"rejoiced": 3}}`, a path could retrieve the nested value:
[source,java]
----
wc.selectOne(Path.key("Frankenstein", "rejoiced")) // returns 3
----

=== More PState capabilities
PStates also support advanced features like "subindexing" for huge inner data structures and "fine-grained reactive queries" for continuous updates. These are described on the xref:pstates.adoc[PStates page].

=== Rama programming
The word count example demonstrates the fundamental Rama workflow:
. Append data to depots.
. ETLs read from depots, process the data, and...
. ...store the results in PStates for later retrieval.

.The basic Rama workflow
image::./diagrams/module-diagrams/single-depot-etl.svg[]

A module can have multiple depots and ETLs, and ETLs can read from multiple depots to populate multiple PStates.

.A more complex module
image::./diagrams/module-diagrams/multiple-depots-etls.svg[]

Rama is designed to be a single, unified system for all your data needs. It integrates storage and computation, eliminating the need to glue together disparate systems like Kafka, Cassandra, and Redis.

== Rama as a Unified Data System
Traditional architectures often combine multiple systems (e.g., RDBMS, NoSQL databases, caches, message queues), leading to significant complexity in development, operation, and data consistency.

Rama's innovation is integrating all aspects of a data system into a single, cohesive platform:
* *PStates*:: Serve as any data model (key/value, document, graph) because they are arbitrary data structures. You can create as many as you need, each with its own optimal shape.
* *Topologies*:: Provide scalable, Turing-complete distributed computation to build and query PStates.
* *Depots*:: Act as the unified entry point for all incoming data.
* *Queries*:: A flexible API supports everything from simple lookups to complex, on-demand distributed computations.
* *Simplified Operations*:: Scalability, deployment, and monitoring are built-in.

== Theory in practice: PStates are views
A core part of Rama development is identifying useful views of your data and creating PStates to materialize them. You store data in the structure that is optimal for how it will be consumed.

For example, given raw web analytics events, you might create two PStates: one for page hit counts and another for user session histories.

.Raw Event Data
[source, json]
----
{"path": "/product/1", "duration": 3500, "sessionId": "abc"}
----

.Desired PState Views
[source, json]
----
// Page Hit Count PState
{"/product/1": 1}

// Session History PState
{"abc": [{"path": "/product/1", "duration": 3500}]}
----

=== Analytics app example
The following module implements this analytics application.
[source,java]
----
public class PageAnalyticsModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    setup.declareDepot("*depot", Depot.random());

    StreamTopology s = topologies.stream("s");
    // PState for page hit counts
    s.pstate("$$pageViewCount", PState.mapSchema(String.class, Long.class));
    // PState for session histories (map of string to list of maps)
    s.pstate("$$sessionHistory",
             PState.mapSchema(
               String.class,
               PState.listSchema(PState.mapSchema(String.class, Object.class))));

    s.source("*depot").out("*pageVisit")
     .each((Map<String, Object> visit) -> visit.get("sessionId"), "*pageVisit").out("*sessionId")
     .each((Map<String, Object> visit) -> visit.get("path"), "*pageVisit").out("*path")
     .compoundAgg("$$pageViewCount", CompoundAgg.map("*path", Agg.count()))
     .compoundAgg("$$sessionHistory", CompoundAgg.map("*sessionId", Agg.list("*pageVisit")));
  }
  // main method omitted for brevity
}
----

=== Schemas define PState structure
A PState's structure is defined by its schema. Schemas can be simple types (e.g., `Long.class`) or collections like `mapSchema`, `listSchema`, and `setSchema`. These can be nested to create complex structures, allowing you to model your data naturally.

Examples of schemas:
`PState.mapSchema(String.class, Long.class)`:: A map from Strings to Longs.
`PState.listSchema(Object.class)`:: A list of any type.
`PState.mapSchema(String.class, PState.setSchema(Long.class))`:: A map from a String to a set of Longs.

=== ETLs update PStates
ETLs define a distributed data pipeline using a chain of methods to process data.

The key concepts of the ETL programming model are:
* *Vars*:: String identifiers starting with `*` (e.g., `"*pageVisit"`) act as temporary variables within the topology.
* `.source(...).out("*var")`:: Binds data from a depot to a var.
* `.each(javaLambda, "*inputVar").out("*outputVar")`:: Applies a Java function to an input var and binds the result to an output var. This is how you use arbitrary Java logic.
* `.compoundAgg("$$pstate", ...)`:: Uses vars to update a PState.

The ETL API is an abstraction for distributed computation. It is implemented in Java, allowing you to use the full power of the language (like lambdas) to transform data within the pipeline.

=== Queries can be distributed computations, too
Queries are not limited to simple key lookups. Rama provides *query topologies* (`topologies.query(...)`) for defining complex, on-demand distributed computations over PStates.

ETLs are a type of topology called a *stream topology* (`topologies.stream(...)`). Both share the same fundamental API for defining computation graphs, unifying how you work with data whether it's streaming in or being queried on-demand.

== Summary
This document covered the core workflow for building applications in Rama.
* **Data Flow**: Data enters via depots, is processed by topologies (ETLs) into PStates, and clients query those PStates.
* **PStates**: Flexible, arbitrarily structured materialized views, shaped for optimal consumption.
* **Topologies**: Stream and query topologies are distributed computations defined with a unified API that allows arbitrary Java logic for transformations.

The result is a single, powerful system that integrates storage and computation, simplifying backend development by letting you use standard Java to work with data in its natural shape.= Distributed Programming in Rama

Rama's distributed programming model enables you to spread work across a cluster of machines. This guide covers the model's core concepts and its design implications. While distributed programming is a complex field, Rama provides abstractions that simplify building high-performance, scalable applications without requiring deep expertise.

== Why Distributed?

Distributed systems are built for several reasons, including separation of concerns, performance, and resilience. This guide focuses on *performance*: how Rama's design allows you to distribute work across multiple machines to handle greater load.

[#task-model]
== The Task Model for Performance

In Rama, a *module* contains all the storage (PStates, depots) and logic (topologies) for your backend. When you deploy a module, you define its parallelism using three parameters:

. *Tasks:* A task is a partition of a module. It contains one partition of every depot and PState, plus an event queue. The number of tasks must be a power of two.
. *Threads:* The number of OS threads that will execute the tasks.
. *Workers:* The number of JVM processes (workers) running across the cluster.

For example, a module with 64 tasks, 32 threads, and 8 workers will spawn 8 worker processes. Each worker will have 4 threads, and each thread will run 2 tasks. You scale a module by adjusting the number of threads and workers.

Tasks are like lightweight threads, identified by an index (e.g., "task 0"). Rama's dataflow API makes it seamless to move between tasks to access different data partitions.

== PStates and Depots are Partitioned

A key concept is that PStates and depots are *partitioned* across all tasks. A logical entity, like a `$$wordCounts` PState, is physically split into separate partitions, one for each task.

When an ETL runs on a specific task, any reference to `$$wordCounts` accesses the partition local to that task. This has a critical implication: if two tasks independently increment a count for the same word, each task's partition will have a count of 1, leading to an incorrect global total of 1 instead of 2.

To achieve correct results, you must ensure that all operations for a given piece of data (like the word "ostentation") are always routed to the same task.

== Controlling Distribution with Partitioners

You control where data is sent and where work is performed using *partitioners*. You can specify a partitioner in two places:

. *Depot Definition:* A depot partitioner determines which task receives a record when it's appended.
. *ETL Logic:* An ETL partitioner relocates the current ETL execution to a different task, transferring its current scope (e.g., variables bound with `.out()`).

Consider this word count example:

[source,java]
----
public class SimpleWordCountModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    // Appended records are sent to a random task's depot partition.
    setup.declareDepot("*wordDepot", Depot.random());

    StreamTopology s = topologies.stream("wordCountStream");
    s.pstate("$$wordCounts", PState.mapSchema(String.class, Long.class));

    s.source("*wordDepot").out("*token")
     // Relocate the ETL to a task determined by the hash of *token.
     .hashPartition("*token")
     .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
  }
}
----

In this flow:
1. A word is appended to a *random* depot partition (e.g., on task 0).
2. The ETL on task 0 reads the word.
3. `.hashPartition("*token")` calculates a target task based on the word's value and relocates the ETL to that task (e.g., task 1).
4. The `.compoundAgg` operation then updates the `$$wordCounts` partition on task 1.

Because `.hashPartition` is deterministic, all occurrences of the same word will always be processed on the same task, ensuring its count is aggregated correctly in a single PState partition.

== Designing Applications for Distribution

In Rama, partitioning is a primary design concern that directly impacts your application's performance and data consistency.

=== Performance

Your partitioning strategy affects efficiency. In the example above, using `Depot.random()` is inefficient because an append might go to one task, only to require a network hop to the correct task for processing.

A more performant design sends data directly to the correct task from the start:

[source,java]
----
public class SimpleWordCountModule implements RamaModule {
  @Override
  public void define(Setup setup, Topologies topologies) {
    // Depot partitioner hashes the record itself to pick a partition.
    setup.declareDepot("*wordDepot", Depot.hashBy(Ops.IDENTITY)); // <1>

    StreamTopology s = topologies.stream("wordCountStream");
    s.pstate("$$wordCounts", PState.mapSchema(Object.class, Object.class));

    s.source("*wordDepot").out("*token")
     // No .hashPartition needed; the data is already on the correct task.
     .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
  }
}
----
<1> The depot now deterministically routes words to the correct task partition.

The general principle is to *co-locate related data and work on the same partition* to minimize network hops.

=== Consistency

Partitioning also affects data consistency. Rama does not guarantee a global processing order for records appended to different depot partitions. This can create race conditions.

Consider a user profile module where edits are appended to a depot with a random partitioner.
1. User `bananaman` submits edit A, which lands on depot partition 0.
2. User `bananaman` submits edit B, which lands on depot partition 1.
3. The ETL for edit A starts on task 0 and partitions to task 5 (the correct task for `bananaman`).
4. The ETL for edit B starts on task 1, partitions to task 5, and updates the profile.
5. The ETL for edit A arrives on task 5 and processes its stale data, overwriting the newer update from edit B.

The solution is to use a depot partitioner (e.g., `Depot.hashBy(username)`). Rama *does* guarantee that records appended to the *same* depot partition are processed in the order they were received. This ensures that edits for a single user are processed sequentially, preventing race conditions.

== Summary

*   Rama's distributed model is built on *tasks*, which are concurrent, partitioned instances of a module.
*   PStates and depots are physically *partitioned* across all tasks. An ETL on a given task interacts with its local partitions.
*   *Partitioners* are used to control work distribution. Depot partitioners route incoming data, and ETL partitioners relocate execution between tasks.
*   Designing an effective partitioning scheme is crucial for both *performance* (by co-locating data) and *consistency* (by ensuring correct processing order).= Dataflow Programming

In Rama, you write ETL and query topologies using a _dataflow API_. This chapter explains the dataflow programming paradigm and the API's core elements.

== The Dataflow Programming Paradigm

Dataflow programming can be understood in contrast to the more common imperative paradigm.

*   **Imperative Programming:** A sequence of instructions for a worker to execute (e.g., call this method, assign this variable). Think of a robot following commands.
*   **Dataflow Programming:** An assembly line where each worker waits for an input, performs an operation, and sends outputs to other workers. This forms a directed graph of operations, which Rama calls a _topology_.

image::./diagrams/dataflow-diagrams/basic-dataflow-graph.png[]

While this model may feel different, it is highly expressive for distributed programming, which is why Rama uses it.

== Operations, Input, and Output

In an imperative program, methods coordinate through a callstack, passing arguments and waiting for a single return value.

image::./diagrams/dataflow-diagrams/basic-function-call.png[]

In a dataflow program, _operations_ wait for input, perform work, and emit output to other operations.

image::./diagrams/dataflow-diagrams/operations.png[]

Key differences from imperative programming include:

. **Decoupled Execution:** The mapping of operations to threads is an implementation detail, not dictated by a main execution thread.
. **Reactivity:** Operations execute only when they receive input.
. **Flexible Output:** An operation can _emit_ multiple values to multiple downstream operations, unlike a method which _returns_ a single value to its caller.

== Dataflow in Rama

In Rama, you define a dataflow graph's structure and its operations simultaneously using a builder-style API. The following examples can be run in a Java process or a Groovy shell.

[source, shell]
----
groovy:000> import com.rpl.rama.Block;
groovy:000> import com.rpl.rama.ops.Ops;
----

Here is a simple dataflow graph with a single node:

[source,java]
----
public static void singleNode() {
    Block.each(Ops.PRINTLN, 10).execute();
}
----

`Block.each()` appends a node to the dataflow graph. It means, "For each input, execute this operation with these arguments." The `.execute()` method sends a single, empty input to the graph, causing `Ops.PRINTLN` to run and print `10`.

image::./diagrams/dataflow-diagrams/one-node-example.png[]

=== Variable Scope

As you chain methods, you build a linear dataflow graph where the output of one node becomes the input for the next. Variables are managed using a _scope_.

[source,java]
----
Block.each(Ops.IDENTITY, "a").out("*x")
     .each(Ops.PRINTLN, "*x")
     .execute();
----
image::./diagrams/dataflow-diagrams/println-example.png[]

Here's how this works:
. `each(Ops.IDENTITY, "a")` emits the string `"a"`.
. `out("*x")` receives `"a"` and binds it to the var `*x` in a new scope. This scope is passed downstream.
. `each(Ops.PRINTLN, "*x")` receives the scope, looks up the value for `*x`, and prints `"a"`.

Strings prefixed with `*` or `$$` are treated as vars. To use a literal string with these prefixes, wrap it in a `Constant`, e.g., `new Constant("*x")`.

You can incrementally build up the scope:
[source,java]
----
Block.each(Ops.IDENTITY, "a").out("*x")
     .each(Ops.IDENTITY, "b").out("*y")
     .each(Ops.TO_STRING, "*x", "*y", 1, "!", 2).out("*z")
     .each(Ops.PRINTLN, "*z")
     .execute(); // Prints "ab1!2"
----

=== Performance

Rama compiles dataflow graphs into efficient bytecode. Vars and control flow constructs become optimized, low-level Java constructs.

== `execute()` vs. `source()`

The `execute()` method is used in these examples to synthetically trigger a dataflow graph. In production code, you will instead _source_ data from depots, which continuously read and emit records into the topology.

[source,java]
----
// In a real module
s.source("*wordDepot").out("*token")
 .hashPartition("$$wordCounts", "*token")
 .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
----

== Branching Dataflow Graphs

You can create graphs where one node emits to multiple branches, and multiple branches merge into one. This is achieved using a 1D sequence of API calls to build a 2D graph structure.

[source,java]
----
public static class Brancher implements RamaOperation0 {
    @Override
    public void invoke(OutputCollector collector) {
        collector.emitStream("pizzaOrders", 1);
        collector.emitStream("saladOrders", 10);
        collector.emitStream("saladOrders", 11);
    }
}

public static void unifyExample() {
    Block.each(new Brancher()).outStream("pizzaOrders", "pizzaAnchor1", "*pizzaOrderSize")
                              .outStream("saladOrders", "saladAnchor1", "*saladOrderSize")
         .each(Ops.DEC, "*saladOrderSize").out("*orderSize")
         .anchor("saladAnchor2")
         .hook("pizzaAnchor1")
         .each(Ops.INC, "*pizzaOrderSize").out("*orderSize")
         .anchor("pizzaAnchor2")
         .unify("pizzaAnchor2", "saladAnchor2")
         .each(Ops.PRINTLN, "*orderSize")
         .execute();
}
----

This constructs the following graph:

image::./diagrams/dataflow-diagrams/unify-example.png[]

=== Output Streams, Anchors, Hooks, and Unify

*   **Output Streams:** Operations emit values to named streams. By default, they emit to the _default output stream_. `emitStream()` in a custom operation allows emitting to specific streams.
*   **`outStream`:** Creates a node that consumes from a named stream and binds its output to vars. It also lets you name the node by creating an _anchor_.
*   **`anchor`:** Attaches a name (an anchor) to the preceding node.
*   **`hook`:** Switches the context to a previously defined anchor, allowing you to add subsequent nodes to a different branch.
*   **`unify`:** Merges multiple branches into a single stream. A var is in scope after a `unify` if it was:
    ** In scope before the branch point, OR
    ** Defined on *all* unified branches.

In the example, `*orderSize` is defined on both branches, so it is available after `unify`.

== Conditionals

You can add conditional branching to a dataflow graph.

[source,java]
----
Block.ifTrue(new Expr(Ops.EQUAL, 1, 2),
       Block.each(Ops.PRINTLN, "math is dead!"),
       Block.each(Ops.PRINTLN, "math is alive!"))
     .each(Ops.PRINTLN, "conditional complete")
     .execute();
----
image::./diagrams/dataflow-diagrams/full-conditional.png[]

`ifTrue` creates a conditional node. It takes a condition, a block for the `true` case, and an optional block for the `false` case. `Expr` is used here for a more concise inline expression.

The branches are implicitly unified. For a var to be available after the conditional, it must be defined in *all* branches. This code is valid because `*a` is defined in both the `true` and `false` blocks:

[source,java]
----
Block.ifTrue(new Expr(Ops.EQUAL, 1, 2),
       Block.each(Ops.IDENTITY, 1).out("*a"),
       Block.each(Ops.IDENTITY, 2).out("*a"))
     .each(Ops.PRINTLN, "*a")
     .execute();
----
Rama performs compile-time checks to prevent invalid var references.

== Writing Custom Operations

You can define custom operations by implementing `RamaFunction` or `RamaOperation`, or by using lambdas and method references.

*   **`RamaFunction`:** Simpler and more efficient. The `invoke` method takes arguments and returns a single value, which is emitted to the default output stream.
[source,java]
----
public static class AddTen implements RamaFunction1<Integer, Integer> {
    @Override
    public Integer invoke(Integer l) {
        return l + 10;
    }
}
Block.each(new AddTen(), 1).out("*numberPlusTen").execute();
----

*   **`RamaOperation`:** More general. The `invoke` method receives an `OutputCollector` to emit multiple values to multiple streams. When an operation emits, downstream code executes immediately, and control returns to the operation only after it finishes.
[source,java]
----
public static class MyOperation implements RamaOperation0 {
    @Override
    public void invoke(OutputCollector collector) {
        collector.emitStream("streamA", 1);
        collector.emit(2, 3, "s"); // Emits to the default stream
    }
}
----

*   **Lambdas and Method References:** A more concise way to define operations, and are generally preferred.
[source,java]
----
// Lambda
Block.each((Integer l) -> l + 10, 1).out("*numberPlusTen").execute();

// Method Reference
Block.each(Math::abs, -1.2).out("*absoluteValue").execute();
----

== Immutable Scope and Shadowing

Scope bindings in Rama are *immutable*. When a node emits, its scope cannot be changed by other nodes.

In the `shadowExample` below, the `"stream2"` branch rebinds `*person`. This creates a new, distinct scope and does not affect the `*person` binding in the `"stream1"` branch. The first branch will always print `"Megan"`.

[source,java]
----
// Defines a Person class and a MultiOut operation
public static void shadowExample() {
    Block.each(Ops.IDENTITY, new Person("Megan")).out("*person")
         .each(new MultiOut()).outStream("stream1", "stream1Anchor")
                              .outStream("stream2", "stream2Anchor")
         .each(Ops.IDENTITY, new Person("John")).out("*person") // Shadowing
         .hook("stream1Anchor")
         .each((Person person) -> person.name).out("*personName")
         .each(Ops.PRINTLN, "*personName") // Prints "Megan"
         .execute();
}
----

However, if you *mutate* a shared object within a scope, the change will be visible across branches, potentially leading to race conditions. In `shadowMutationExample`, the `Person` object itself is modified, which can affect the output of other branches depending on execution order.

== Looping

Rama's dataflow API is Turing-complete, featuring a looping construct.

[source,java]
----
public static void loopExample() {
    Block.loopWithVars(LoopVars.var("*i", 0),
            Block.ifTrue(new Expr(Ops.NOT_EQUAL, "*i", 5),
              Block.emitLoop("*i")
                   .each(Ops.PRINTLN, "Variable *i is not 5 yet")
                   .continueLoop(new Expr(Ops.INC, "*i")))).out("*loopValue")
         .each(Ops.PRINTLN, "Emitted:", "*loopValue")
         .execute();
}
----
image::./diagrams/dataflow-diagrams/looping.png[]

*   `loopWithVars`: Initializes loop variables (e.g., `*i` to `0`).
*   `emitLoop`: Emits a value *out* of the loop to the code that follows. This does not terminate the loop. Control flow immediately passes to the code after the loop, executes it, and then returns to the node after `emitLoop`.
*   `continueLoop`: Triggers the next iteration of the loop with new values for the loop variables. If `continueLoop` is not called, the loop terminates.

Unlike standard loops, code following a Rama loop can execute multiple times via `emitLoop` *before* the loop has finished.

=== Looping without Vars

The `loop` method works like `loopWithVars` but without explicit loop variables. State is typically managed by a mutable object bound before the loop.

[source,java]
----
Block.each((RamaFunction0) ArrayList::new).out("*list")
     .loop(
       Block.each(Ops.SIZE, "*list").out("*size")
           .ifTrue(new Expr(Ops.EQUAL, "*size", 3),
             Block.emitLoop(), // Emits when list size is 3
             Block.each((List l) -> l.add("iter"), "*list")
                  .continueLoop()))
  .each(Ops.PRINTLN, "Result:", "*list")
  .execute();
----

== Summary

Rama's dataflow API provides precise control over data processing, branching, and merging logic. More advanced features like batch computation and code reuse are covered in xref:intermediate-dataflow.adoc[Intermediate dataflow programming].= Types of ETLs

Rama provides two types of ETLs: stream topologies and microbatch topologies. Each is suited for different use cases. This document explains their characteristics and tradeoffs.

== Stream Topologies

Stream topologies process data as it arrives, record by record.

*   **Processing Order:** Data from a single depot partition is processed in append order. However, processing across different partitions is independent and concurrent. Due to partitioning, a record that starts processing first may not finish first.
*   **Fault Tolerance:** Rama tracks the completion status of each record. Failed or timed-out records are retried according to a configurable _retry policy_. This typically results in at-least-once or at-most-once processing semantics.
*   **Depot Integration:** A key feature is integration with depot appends. A `depot.append()` call can block until the data has been processed by colocated stream topologies, providing a simple way to coordinate client actions with data processing.

.A simple stream topology for word counting
[source, java]
----
public void define(Setup setup, Topologies topologies) {
  setup.declareDepot("*depot", Depot.random());
  StreamTopology s = topologies.stream("s");
  s.pstate("$$wordCounts", PState.mapSchema(Object.class, Object.class));

  s.source("*depot").out("*token")
   .hashPartition("*token")
   .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
}
----

An asynchronous version of `append` is also available:
[source, java]
----
// Blocks until processing is complete
depot.append("a");

// Returns a CompletableFuture for non-blocking coordination
depot.appendAsync("a");
----

See xref:stream.adoc[Stream Topologies] and xref:depots.adoc[Depots] for more details.

== Microbatch Topologies

Microbatch topologies process data in coordinated batches across all partitions rather than one record at a time. Each microbatch processes all data accumulated since the previous one.

*   **Processing Model:** Processing begins with `source(...)`, which emits a single microbatch object. The `explodeMicrobatch` operation then unpacks this batch, emitting each individual record for parallel processing.
*   **Performance:** While microbatch latency is higher (hundreds of milliseconds to seconds), the batched nature results in lower per-record overhead and significantly higher overall throughput compared to stream topologies.
*   **Fault Tolerance:** Microbatch topologies provide a strong *exactly-once* processing guarantee for PState updates, even with failures and retries.
*   **Depot Integration:** Unlike stream topologies, microbatch topologies do not integrate with depot appends due to their higher processing latency.

.A microbatch topology for word counting
[source, java]
----
public void define(Setup setup, Topologies topologies) {
  setup.declareDepot("*depot", Depot.random());
  MicrobatchTopology mb = topologies.microbatch("mb");
  mb.pstate("$$wordCounts", PState.mapSchema(Object.class, Object.class));

  mb.source("*depot").out("*microbatch")
    .explodeMicrobatch("*microbatch").out("*token") // <-- Key difference
    .hashPartition("*token")
    .compoundAgg("$$wordCounts", CompoundAgg.map("*token", Agg.count()));
}
----

Microbatch topologies also support advanced features like sub-batching and two-phase aggregation. See xref:microbatch.adoc[Microbatch Topologies] for details.

== Key Differences and Tradeoffs

[width="90%",options="header"]
|===
| Feature | Stream Topology | Microbatch Topology

| Latency
| Low (milliseconds)
| High (100s of ms to seconds)

| Throughput
| Medium
| High

| Fault-Tolerance Semantics
| At-least-once / At-most-once
| Exactly-once

| Depot Append Integration
| Yes
| No
|===


== Building a Social Network with Rama

This tutorial demonstrates how to build "RamaSpace", a scalable social network, in only 180 lines of code. It covers bidirectional relationships, profiles, user walls, and simple analytics, introducing key Rama concepts and tools.

The complete code is available in the link:https://github.com/redplanetlabs/rama-examples[rama-examples] project.

== Application Design

Rama development starts with identifying the application's required tasks, which then informs the design of the PStates, depots, and ETLs needed to support them.

=== Tasks

Our application will support the following tasks:

*Users*
* Register a new user.
* Update a user's profile.
* Fetch a user's password hash for login.
* Post on a user's wall.
* View and count posts on a user's wall (paginated).

*Friendships*
* Send, view, and cancel friendship requests.
* Accept a friendship request.
* Unfriend a user.
* Check friendship status.
* View and count friends for a user (paginated).

*Analytics*
* Query the number of profile views for a user over a range of hours.

=== PState Schemas

These tasks are supported by the following PStates:

*$$profiles*
[source,json]
----
{userId<String>:
  {"displayName": <String>, "email": <String>, "profilePic": <String>,
   "bio": <String>, "location": <String>, "pwdHash": <Integer>,
   "joinedAtMillis": <Long>, "registrationUUID": <String>}}
----

*$$outgoingFriendRequests* & *$$incomingFriendRequests*
[source,json]
----
{userId<String>: Set<userId<String>>}
----

*$$friends*
[source,json]
----
{userId<String>: Set<userId<String>>}
----

*$$posts*
[source,json]
----
{userId<String>: {postId<Long>: <Post>}}
----

*$$postId*
[source,json]
----
<Long>
----

*$$profileViews*
[source,json]
----
{userId<String>: {hourBucket<Long>: count<Long>}}
----

== Application Queries

Queries are defined in a `RamaSpaceClient` class that wraps a cluster manager and provides an application-specific API.

[source,java]
----
// In RamaSpaceClient constructor
String moduleName = RamaSpaceModule.class.getName();
_profiles = cluster.clusterPState(moduleName, "$$profiles");
_friends = cluster.clusterPState(moduleName, "$$friends");
// ... other PStates
----

=== Subindexing

To efficiently query large nested collections (like a user's friends or posts), Rama uses **subindexing**. This indexes each element of a nested collection individually, enabling fast queries on collections that can grow larger than memory.

To enable it, add the `.subindexed()` flag to a schema definition:
[source,java]
----
friends.pstate(
  "$$friends",
  PState.mapSchema(
    String.class,
    PState.setSchema(String.class).subindexed()));
----

=== Query Examples

*Fetch a password hash from a fixed-keys map:*
[source,java]
----
public Integer getPwdHash(String userId) {
  return _profiles.selectOne(Path.key(userId, "pwdHash"));
}
----

*Fetch a subset of profile fields:*
[source,java]
----
public Profile getProfile(String userId) {
  Map profile = _profiles.selectOne(
                  Path.key(userId)
                      .subMap("displayName", "location", "bio", "email",
                              "profilePic", "joinedAtMillis"));
  // ... package map into a Profile object
}
----

*Get the size of a subindexed collection:*
This query executes entirely on the server, returning only the final count. Rama automatically indexes the size of subindexed structures for constant-time lookups.
[source,java]
----
public long getFriendsCount(String userId) {
  return _friends.selectOne(Path.key(userId).view(Ops.SIZE));
}
----

*Paginate through a subindexed set:*
Subindexed collections are sorted. `sortedSetRangeFrom` navigates a sorted set, making pagination straightforward.
[source,java]
----
public Set<String> getFriendsPage(String userId, String start) {
  return _friends.selectOne(Path.key(userId).sortedSetRangeFrom(start,
    SortedRangeFromOptions.maxAmt(20).excludeStart()));
}
----
To get the first page, `start` is an empty string. For subsequent pages, `start` is the last user ID from the previous page.

*Perform a complex aggregation on the server:*
This query sums profile views across an hourly range, demonstrating the power of Rama's path-based API.
[source,java]
----
public long getNumProfileViews(String userId, long startHourBucket, long endHourBucket) {
  return _profileViews.selectOne(
           Path.key(userId)
               .sortedMapRange(startHourBucket, endHourBucket)
               .subselect(Path.mapVals())
               .view(Ops.SUM));
}
----
1.  `key(userId)`: Navigates to the user's map of views.
2.  `sortedMapRange(...)`: Selects the submap for the given hour range.
3.  `subselect(Path.mapVals())`: Collects all values (counts) from the submap into a list.
4.  `view(Ops.SUM)`: Sums the list of counts.

== Application Data

Data appended to depots or returned from queries should be defined as strongly-typed objects. Rama serializes these objects when they are sent over the network. By implementing the `RamaSerializable` interface, you can use Java's built-in serialization.

Here are a few example data types:
[source,java]
----
// For depot appends
public class UserRegistration implements RamaSerializable {
  public String userId;
  public String email;
  public String displayName;
  public int pwdHash;
  // ... constructor
}

public class Post implements RamaSerializable {
  public String userId;
  public String toUserId;
  public String content;
  // ... constructor
}

// For query results
public class ResolvedPost implements RamaSerializable {
  public String userId;
  public String content;
  public String displayName;
  public String profilePic;
  // ... constructor
}
----

== `RamaSpaceModule` Implementation

All depots, PStates, and topologies are defined in the `define` method of a `RamaModule`.

[source,java]
----
@Override
public void define(Setup setup, Topologies topologies) {
  // 1. Declare depots
  setup.declareDepot("*userRegistrationsDepot", Depot.hashBy(UserIdExtract.class));
  // ... other depots

  // 2. Define ETL topologies
  declareUsersTopology(topologies);
  declareFriendsTopology(topologies);
  declarePostsTopology(topologies);
  declareProfileViewsTopology(topologies);

  // 3. Define query topologies
  topologies.query("resolvePosts", /* ... */);
}
----

=== Depots

Depots are partitioned to ensure related data is processed in order and on the correct task. For example, `*friendRequestsDepot` receives both `FriendRequest` and `CancelFriendRequest` objects. By partitioning with `Depot.hashBy(UserIdExtract.class)`, we guarantee that all requests from the same user land on the same partition and are processed sequentially, preventing race conditions.

=== Users Topology (Streaming)

This topology handles user registrations and profile updates with low latency.

*PState:* A map from `userId` to a `fixedKeysSchema` containing profile information.
[source,java]
----
users.pstate(
  "$$profiles",
  PState.mapSchema(
    String.class,
    PState.fixedKeysSchema(/* ... profile fields ... */)));
----

*Registration Logic:* To handle registration races atomically, the ETL checks for the user's existence before writing. A client-provided UUID confirms success.
[source,java]
----
users.source("*userRegistrationsDepot").out("*registration")
     .macro(extractJavaFields("*registration", "*userId", /* ... */, "*registrationUUID"))
     .each(System::currentTimeMillis).out("*joinedAtMillis")
     .localTransform("$$profiles",
       Path.key("*userId")
           .filterPred(Ops.IS_NULL) // Only continue if user does not exist
           .multiPath(/* ... paths to set all profile fields ... */));
----
The `extractJavaFields` macro simplifies extracting fields from the `*registration` object. The client then verifies registration by querying for the UUID it sent.

=== Friends Topology (Streaming)

This topology manages friend requests and maintains the bidirectional social graph.

*PStates:* Three subindexed maps for `$$outgoingFriendRequests`, `$$incomingFriendRequests`, and `$$friends`.

*Logic:* It uses `subSource` to branch logic based on the data type (`FriendRequest` vs. `CancelFriendRequest`). When a friendship is formed, a single `FriendshipAdd` event triggers logic that both removes the pending requests and adds the bidirectional friendship records to `$$friends`. This is done by branching the dataflow with `anchor` and `hook`.
[source,java]
----
friends.source("*friendshipChangesDepot").out("*change")
       // ... extract fields
       .anchor("start")
       // Branch 1: Clear friend requests
       .compoundAgg("$$incomingFriendRequests", CompoundAgg.map("*userId1", Agg.setRemove("*userId2")))
       // ... clear other direction
       .hook("start")
       // Branch 2: Add friendship
       .subSource("*change",
         SubSource.create(FriendshipAdd.class)
                  .compoundAgg("$$friends", CompoundAgg.map("*userId1", Agg.set("*userId2")))
                  // ... add other direction
         // ... logic for FriendshipRemove
       );
----

=== Posts Topology (Microbatch)

This topology handles new posts with high throughput, suitable for use cases where latency of a few hundred milliseconds is acceptable.

*PState:* `$$posts` is a map from `userId` to a subindexed map of `postId` to `Post`.

*ID Generation:* Post IDs are generated in descending order using the `TaskUniqueIdPState` helper. This stores posts in reverse-chronological order automatically, simplifying queries for the latest posts.
[source,java]
----
// In setup
TaskUniqueIdPState id = new TaskUniqueIdPState("$$postId").descending();
id.declarePState(posts);

// In topology
posts.source("*postsDepot").out("*microbatch")
     .explodeMicrobatch("*microbatch").out("*post")
     .macro(extractJavaFields("*post", "*toUserId"))
     .macro(id.genId("*id")) // Generate a new descending ID
     .localTransform("$$posts", Path.key("*toUserId", "*id").termVal("*post"));
----

=== Profile Views Topology (Microbatch)

This analytics ETL aggregates profile view counts into hourly buckets. Microbatching is ideal here, providing high throughput for a non-latency-sensitive task.

*PState:* `$$profileViews` is a map from `userId` to a subindexed map of `hourBucket` to `count`.

*Logic:* It converts a timestamp to an hour bucket and uses `compoundAgg` with `Agg.count()` to increment the view counter. `compoundAgg` concisely handles the initialization and update of nested map values.
[source,java]
----
profileViews.source("*profileViewsDepot").out("*microbatch")
            .explodeMicrobatch("*microbatch").out("*profileView")
            .macro(extractJavaFields("*profileView", "*toUserId", "*timestamp"))
            .each((Long ts) -> ts / (1000 * 60 * 60), "*timestamp").out("*bucket")
            .compoundAgg("$$profileViews",
              CompoundAgg.map("*toUserId", CompoundAgg.map("*bucket", Agg.count())));
----

=== `resolvePosts` Query Topology

A query topology executes a distributed computation to fetch and combine data from multiple PStates in a single round trip. This is far more efficient than multiple client-side queries.

*Purpose:* Fetches a page of posts and "resolves" them by also fetching the display name and profile picture for each poster.

*Logic:*
1.  Starts on the partition for `*forUserId`.
2.  Selects a page of posts from the `$$posts` PState.
3.  For each post, partitions to the poster's `*userId`.
4.  Selects the user's `displayName` and `profilePic` from the `$$profiles` PState.
5.  Constructs a `ResolvedPost` object.
6.  Returns to the original partition and aggregates the results into a map.
[source,java]
----
topologies.query("resolvePosts", "*forUserId", "*startPostId").out("*resultMap")
          .hashPartition("*forUserId")
          .localSelect("$$posts", Path.key("*forUserId").sortedMapRangeFrom("*startPostId", 20)).out("*submap")
          .each(Ops.EXPLODE_MAP, "*submap").out("*i", "*post")
          .macro(extractJavaFields("*post", "*userId", "*content"))
          .hashPartition("*userId")
          .localSelect("$$profiles", Path.key("*userId", "displayName")).out("*displayName")
          .localSelect("$$profiles", Path.key("*userId", "profilePic")).out("*profilePic")
          .each(ResolvedPost::new, /* ... */).out("*resolvedPost")
          .originPartition()
          .compoundAgg(CompoundAgg.map("*i", Agg.last("*resolvedPost"))).out("*resultMap");
----

== Unit Testing

`InProcessCluster` provides an ideal environment for unit testing modules. The typical pattern is to append data to depots and then query PStates to verify the expected state changes.

*   For **stream topologies**, depot appends (with default `AckLevel.ACK`) block until processing is complete, so assertions can be made immediately.
*   For **microbatch topologies**, processing is asynchronous. Use `ipc.waitForMicrobatchProcessedCount` to ensure data has been processed before making assertions.

[source,java]
----
@Test
public void basicTest() throws Exception {
  try(InProcessCluster ipc = InProcessCluster.create()) {
    // 1. Launch the module
    ipc.launchModule(new RamaSpaceModule(), new LaunchConfig(4, 4));
    RamaSpaceClient client = new RamaSpaceClient(ipc);

    // 2. Test a streaming topology
    assertTrue(client.appendUserRegistration("alice", "alice@gmail.com", "Alice", 1));
    assertEquals(1, client.getPwdHash("alice")); // Assert immediately

    // 3. Test a microbatch topology
    client.appendPost("bob", "alice", "a post");
    ipc.waitForMicrobatchProcessedCount("...RamaSpaceModule", "posts", 1); // Wait
    assertEquals(1, client.getPostsCount("alice")); // Now assert
  }
}
----

== Summary

You now have the core knowledge to build robust and scalable applications with Rama. This tutorial has demonstrated how to combine depots, PStates, and topologies to solve real-world problems elegantly.

To continue your journey, you can:
*   Build your own application using `InProcessCluster` for rapid feedback.
*   Explore more complex applications like our xref:downloads-maven-local-dev.adoc[Twitter-scale Mastodon implementation].
*   Browse the link:https://github.com/redplanetlabs/rama-demo-gallery[rama-demo-gallery] for short, focused examples.
*   Read deep-dive documentation on advanced features as you need them.= Why use Rama?

Software development suffers from a major disconnect: describing a large-scale application is far easier than building it. Too much time is spent on low-level "plumbing"ingesting, processing, storing, and querying datainstead of building product features. This involves gluing components, fixing impedance mismatches, and managing complex configurations.

Rama is a paradigm shift that offers a cohesive, end-to-end model for building scalable backends. It enables teams to:

* Build backends with up to 100x less code.
* xref:integrating.adoc[Easily integrate] with existing tools.
* Iterate on product ideas with extreme speed.
* Dramatically reduce operational burden and infrastructure.
* Lower overall complexity.

== When to Use Rama

Rama is ideal for applications requiring:

* Realtime, transactional, or reactive capabilities
* Realtime analytics
* Complex and varied indexing
* xref:acid.adoc[Strong fault-tolerance and data consistency]
* Massive scale for reads and writes
* High performance

== Complexity Rama Eliminates

Rama removes vast amounts of backend complexity by replacing:

* The need for multiple, disparate databases
* Complex toolchains and ad-hoc deployment scripts
* Low-level glue code for serialization, routing, and data transforms
* Impedance mismatches between data and application models
* Multiple narrow APIs, often in different languages

== Get Started

To start learning Rama and see these benefits for yourself, begin with our xref:tutorial1.adoc[tutorial].
